directory_name,category_match,status_match,similarity_score,total_input_tokens,total_output_tokens,total_elapsed_time,error_status,error_message,policy_check_result_1,policy_check_evidence_snippet_1,policy_check_result_best,policy_check_evidence_snippet_best,anonymity_check_evidence_snippet_1,anonymity_check_result_best,anonymity_check_evidence_snippet_best,formatting_check_result_1,formatting_check_evidence_snippet_1,formatting_check_result_best,formatting_check_evidence_snippet_best,scope_check_result_1,anonymity_check_result_1,scope_check_evidence_snippet_1,scope_check_result_best,scope_check_evidence_snippet_best,token_error_occurred
data/iclr/data/submission_eIK4ojL2QM,1.0,1.0,0.5608637762261662,27001,1864,198.94940733909607,,,False,,False,,"We initially proposed a novel method for constructing real networks from the complex domain using the Cauchy integral formula in Li et al. (2024); Zhang et al. (2024)... (Page 1, line 023)
Inspired by the mathematical precision of the Cauchy integral theorem, Li et al. (2024) introduced the XNet architecture... (Page 1, line 051)",True,"We initially proposed a novel method for constructing real networks from the complex domain using the Cauchy integral formula in Li et al. (2024); Zhang et al. (2024)... (Page 1, line 023)
Inspired by the mathematical precision of the Cauchy integral theorem, Li et al. (2024) introduced the XNet architecture... (Page 1, line 051)",False,"The main content (Abstract through Section 4) spans exactly 10 pages (pages 1-10), which is within the 6-10 page limit. References and Appendix are on subsequent pages. Line numbers are present on all pages in the left margin. Visual inspection of the PDF indicates that margins, font sizes, and paragraph spacing adhere to the ICLR 2025 style guide, with no apparent 'space-cheating' or extensive use of small fonts in the main body. No explicit Ethics Statement or Reproducibility Statement sections were found to exceed their page limit.",False,"The main content (Abstract through Section 4) spans exactly 10 pages (pages 1-10), which is within the 6-10 page limit. References and Appendix are on subsequent pages. Line numbers are present on all pages in the left margin. Visual inspection of the PDF indicates that margins, font sizes, and paragraph spacing adhere to the ICLR 2025 style guide, with no apparent 'space-cheating' or extensive use of small fonts in the main body. No explicit Ethics Statement or Reproducibility Statement sections were found to exceed their page limit.",False,True,,False,,False
data/iclr/data/submission_F27YbvbgKy,1.0,1.0,1.0,37159,1496,37.66863799095154,,,False,,False,,,False,,False,"The main content (Abstract through Conclusion/Reproducibility Statement) spans exactly 10 pages (from page 1 to page 10), which adheres to the strict upper limit of 10 pages for the main text. The Reproducibility Statement on page 10 is less than one page. Line numbers are present on all pages. Visual inspection of margins, font sizes, and spacing indicates adherence to the ICLR 2025 style guide, with no apparent 'space-cheating' or extensive use of smaller fonts in the main body.",False,"The main content (Abstract through Conclusion/Reproducibility Statement) spans exactly 10 pages (from page 1 to page 10), which adheres to the strict upper limit of 10 pages for the main text. The Reproducibility Statement on page 10 is less than one page. Line numbers are present on all pages. Visual inspection of margins, font sizes, and spacing indicates adherence to the ICLR 2025 style guide, with no apparent 'space-cheating' or extensive use of smaller fonts in the main body.",False,False,"The submission's abstract, introduction, and methodology clearly describe a novel graph-based framework for topology-preserving image segmentation, including a new loss function and topological guarantees. It applies deep learning techniques to vision tasks (image segmentation) and involves optimization for representation learning.",False,"The submission's abstract, introduction, and methodology clearly describe a novel graph-based framework for topology-preserving image segmentation, including a new loss function and topological guarantees. It applies deep learning techniques to vision tasks (image segmentation) and involves optimization for representation learning.",False
data/iclr/data/submission_3QoXfx1Tgw,1.0,1.0,1.0,49650,1734,41.68618130683899,,,False,,False,,,False,,False,"The document adheres to all specified formatting and layout rules. No violations were found regarding page limits, statement limits, line numbers, or margins/spacing.",False,"The document adheres to all specified formatting and layout rules. No violations were found regarding page limits, statement limits, line numbers, or margins/spacing.",False,False,"We study the computational limits of Low-Rank Adaptation (LoRA) for fine-tuning transformer-based models using fine-grained complexity theory. Our key observation is that the existence of low-rank decompositions within the gradient computation of LoRA adaptation leads to possible algorithmic speedup. This allows us to (i) identify a phase transition behavior of efficiency assuming the Strong Exponential Time Hypothesis (SETH), and (ii) prove the existence of nearly linear algorithms by controlling the LoRA update computation term by term.",False,"We study the computational limits of Low-Rank Adaptation (LoRA) for fine-tuning transformer-based models using fine-grained complexity theory. Our key observation is that the existence of low-rank decompositions within the gradient computation of LoRA adaptation leads to possible algorithmic speedup. This allows us to (i) identify a phase transition behavior of efficiency assuming the Strong Exponential Time Hypothesis (SETH), and (ii) prove the existence of nearly linear algorithms by controlling the LoRA update computation term by term.",True
data/iclr/data/submission_mhyl7HhNM5,0.0,0.0,0.0,38270,1681,34.71072626113892,,,False,,False,,"human experts, which are two of this paper's authors, fully familiar with the task",True,"human experts, which are two of this paper's authors, fully familiar with the task",False,"The document adheres to all specified formatting and layout guidelines. Line numbers are present on all pages. The main text (Abstract through Discussion) is exactly 10 pages, which is within the 6-10 page limit. The Ethics Statement and Reproducibility Statement are each less than one page. Margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide, and there is no evidence of 'space-cheating' or extensive use of small fonts in the main body.",False,"The document adheres to all specified formatting and layout guidelines. Line numbers are present on all pages. The main text (Abstract through Discussion) is exactly 10 pages, which is within the 6-10 page limit. The Ethics Statement and Reproducibility Statement are each less than one page. Margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide, and there is no evidence of 'space-cheating' or extensive use of small fonts in the main body.",False,True,,False,,False
data/iclr/data/submission_TkbjqexD8w,1.0,1.0,1.0,24721,1388,28.51611042022705,,,False,,False,,,False,,False,"The main paper adheres to all specified formatting and layout guidelines. No evidence of page limit, statement limit, line number, or margin/spacing violations was found.",False,"The main paper adheres to all specified formatting and layout guidelines. No evidence of page limit, statement limit, line number, or margin/spacing violations was found.",False,False,,False,,True
data/iclr/data/submission_JKMoLnT1wZ,1.0,1.0,1.0,23787,1608,20.93428039550781,,,False,,False,,,False,,False,"The main content of the paper (Abstract through Limitations) spans 10 pages (pages 1-10). The style guide specifies a strict upper limit of 10 pages for the main text, which this submission adheres to. Line numbers are present on all pages in the left margin. No explicit Ethics or Reproducibility statements were found that would be subject to a page limit. Visual inspection of the document indicates that margins, spacing, and font sizes in the main body text adhere to the specified guidelines (10pt type with 11pt vertical spacing, 1.5 inch left margin, 5.5 inch text width, 9 inch text length). Smaller fonts are appropriately used for footnotes, figure captions, and the references section, which is permitted.",False,"The main content of the paper (Abstract through Limitations) spans 10 pages (pages 1-10). The style guide specifies a strict upper limit of 10 pages for the main text, which this submission adheres to. Line numbers are present on all pages in the left margin. No explicit Ethics or Reproducibility statements were found that would be subject to a page limit. Visual inspection of the document indicates that margins, spacing, and font sizes in the main body text adhere to the specified guidelines (10pt type with 11pt vertical spacing, 1.5 inch left margin, 5.5 inch text width, 9 inch text length). Smaller fonts are appropriately used for footnotes, figure captions, and the references section, which is permitted.",False,False,,False,,False
data/iclr/data/submission_NAlpAxxLAT,1.0,1.0,1.0,198860,2050,66.5672287940979,,,True,"## Add MASK for CN_reveal：- While move action`[0-4]` choose `[4]`, reveal action will have mask: `[1, 1, 1, 1, 0]`,- While move action`[0-4]` choose `[0,1,2,3]`, reveal action will have mask: `[0, 0, 0, 0, 1]`,- `Do mask need gradient?`",True,"## Add MASK for CN_reveal：- While move action`[0-4]` choose `[4]`, reveal action will have mask: `[1, 1, 1, 1, 0]`,- While move action`[0-4]` choose `[0,1,2,3]`, reveal action will have mask: `[0, 0, 0, 0, 1]`,- `Do mask need gradient?`",,False,,False,"The main content (Abstract through Conclusion and Future Work) spans from page 1 to page 10, which is exactly 10 pages, adhering to the page limit. The Reproducibility Statement is less than one page. Line numbers are present on all pages. Margins, spacing, and font sizes appear to conform to the ICLR 2025 style guide specifications, with no evidence of 'space-cheating' or extensive use of small fonts in the main body.",False,"The main content (Abstract through Conclusion and Future Work) spans from page 1 to page 10, which is exactly 10 pages, adhering to the page limit. The Reproducibility Statement is less than one page. Line numbers are present on all pages. Margins, spacing, and font sizes appear to conform to the ICLR 2025 style guide specifications, with no evidence of 'space-cheating' or extensive use of small fonts in the main body.",False,False,"Effective communication is an essential component in collaborative multi-agent systems. Situations where explicit messaging is not feasible have been common in human society throughout history, which motivate the study of implicit communication. Previous works on learning implicit communication mostly rely on theory of mind (ToM), where agents infer the mental states and intentions of others by interpreting their actions. However, ToM-based methods become less effective in making accurate inferences in complex tasks. In this work, we propose the Implicit Channel Protocol (ICP) framework, which allows agents to construct implicit communication channels similar to the explicit ones. ICP leverages a subset of actions, denoted as the scouting actions, and a mapping between information and these scouting actions that encodes and decodes the messages. We propose training algorithms for agents to message and act, including learning with a randomly initialized information map and with a delayed information map. The efficacy of ICP has been tested on the tasks of Guessing Number, Revealing Goals, and Hanabi, where ICP significantly outperforms baseline methods through more efficient information transmission.",False,"Effective communication is an essential component in collaborative multi-agent systems. Situations where explicit messaging is not feasible have been common in human society throughout history, which motivate the study of implicit communication. Previous works on learning implicit communication mostly rely on theory of mind (ToM), where agents infer the mental states and intentions of others by interpreting their actions. However, ToM-based methods become less effective in making accurate inferences in complex tasks. In this work, we propose the Implicit Channel Protocol (ICP) framework, which allows agents to construct implicit communication channels similar to the explicit ones. ICP leverages a subset of actions, denoted as the scouting actions, and a mapping between information and these scouting actions that encodes and decodes the messages. We propose training algorithms for agents to message and act, including learning with a randomly initialized information map and with a delayed information map. The efficacy of ICP has been tested on the tasks of Guessing Number, Revealing Goals, and Hanabi, where ICP significantly outperforms baseline methods through more efficient information transmission.",False
data/iclr/data/submission_K1G8UKcEBO,1.0,1.0,1.0,54835,1782,33.99032521247864,,,False,,False,,,False,,False,"The document adheres to all specified formatting and layout guidelines. Line numbers are present on every page. The main content (Abstract through Conclusion/Discussion, including figures and tables within these sections) spans exactly 10 pages, which is within the allowed limit. No explicit Ethics or Reproducibility Statements are present. Margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of small fonts in the main body text.",False,"The document adheres to all specified formatting and layout guidelines. Line numbers are present on every page. The main content (Abstract through Conclusion/Discussion, including figures and tables within these sections) spans exactly 10 pages, which is within the allowed limit. No explicit Ethics or Reproducibility Statements are present. Margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of small fonts in the main body text.",False,False,,False,,False
data/iclr/data/submission_7bwE5MJAVJ,0.0,1.0,0.0,38147,1434,29.082319736480716,,,False,,False,,,False,,True,"The provided main_paper.pdf does not include line numbers in the left margin. The ICLR 2025 style guide (iclr2025_conference.pdf, page 1, line 027) explicitly states: ""Please note that we have introduced automatic line number generation into the style file for LaTeXe. This is to help reviewers refer to specific lines of the paper when they make their comments.""",True,"The provided main_paper.pdf does not include line numbers in the left margin. The ICLR 2025 style guide (iclr2025_conference.pdf, page 1, line 027) explicitly states: ""Please note that we have introduced automatic line number generation into the style file for LaTeXe. This is to help reviewers refer to specific lines of the paper when they make their comments.""",False,False,,False,,False
data/iclr/data/submission_CTLieyrNZr,1.0,1.0,1.0,30015,1710,43.61522841453552,,,False,,False,,,False,,False,"The main content (Abstract through Conclusion) spans exactly 10 pages (page 1 to page 10). Line numbers are present on all pages. No extensive use of small fonts or narrowed margins was detected in the main body. The document adheres to the specified font sizes, line spacing, and margin requirements as outlined in the ICLR 2025 style guide.",False,"The main content (Abstract through Conclusion) spans exactly 10 pages (page 1 to page 10). Line numbers are present on all pages. No extensive use of small fonts or narrowed margins was detected in the main body. The document adheres to the specified font sizes, line spacing, and margin requirements as outlined in the ICLR 2025 style guide.",False,False,,False,,False
data/iclr/data/submission_PAzVN4EEkj,1.0,1.0,0.7922669980546885,960928,1634,150.30741429328918,,,False,,False,,"Feihan Lit, Yifan Sunt, Weiye Zhao†, Rui Chen, Tianhao Wei & Changliu Liu * Robotics Institute Carnegie Mellon University Pittsburgh, PA 15213, USA {feihanl, yifansu2, weiyezha, ruic3, twei2, cliu6}@andrew.cmu.edu",True,"Feihan Lit, Yifan Sunt, Weiye Zhao†, Rui Chen, Tianhao Wei & Changliu Liu * Robotics Institute Carnegie Mellon University Pittsburgh, PA 15213, USA {feihanl, yifansu2, weiyezha, ruic3, twei2, cliu6}@andrew.cmu.edu",False,"The main paper PDF contains 10 pages of main content (Abstract through Conclusion and Future Prospectus), which adheres to the strict upper limit of 10 pages for main text. Line numbers are present in the left margin on all pages. Visual inspection of the PDF does not reveal any obvious violations of font size, spacing, or margins as per the ICLR 2025 style guide. No explicit Ethics or Reproducibility statements were found to check against a 1-page limit.",False,"The main paper PDF contains 10 pages of main content (Abstract through Conclusion and Future Prospectus), which adheres to the strict upper limit of 10 pages for main text. Line numbers are present in the left margin on all pages. Visual inspection of the PDF does not reveal any obvious violations of font size, spacing, or margins as per the ICLR 2025 style guide. No explicit Ethics or Reproducibility statements were found to check against a 1-page limit.",False,True,"Deep reinforcement learning (RL) excels in various control tasks, yet the absence of safety guarantees hampers its real-world applicability. ... To address these problems, we present Safe Set Guided State-wise Constrained Policy Optimization (S-3PO), a pioneering algorithm generating state-wise safe optimal policies with zero training violations, i.e., learning without mistakes. ... S-3PO outperforms existing methods in high-dimensional robotics tasks, managing state-wise constraints with zero training violation.",False,"Deep reinforcement learning (RL) excels in various control tasks, yet the absence of safety guarantees hampers its real-world applicability. ... To address these problems, we present Safe Set Guided State-wise Constrained Policy Optimization (S-3PO), a pioneering algorithm generating state-wise safe optimal policies with zero training violations, i.e., learning without mistakes. ... S-3PO outperforms existing methods in high-dimensional robotics tasks, managing state-wise constraints with zero training violation.",False
data/iclr/data/submission_jCtT1vUWb7,0.0,0.0,0.0,34257,1910,90.69245028495789,,,False,,False,,"Human: We conduct several experiments in which a human plays as the solver or expert to provide a strong baseline. As hiring participants was prohibitively expensive and time consuming, we role played as agents ourselves across 30 sampled puzzles as a preliminary study, and leave further human participation to future work.",True,"Human: We conduct several experiments in which a human plays as the solver or expert to provide a strong baseline. As hiring participants was prohibitively expensive and time consuming, we role played as agents ourselves across 30 sampled puzzles as a preliminary study, and leave further human participation to future work.",False,"The main paper (main_paper.pdf) adheres to the ICLR 2025 style guide. The main content, including Abstract, Introduction, Related Work, Benchmark, Evaluation, Results and Analysis, Conclusion, and Limitations, spans pages 1 through 10. The References section begins on page 10. This falls within the strict upper limit of 10 pages for the main text as specified in the style guide (iclr2025_conference.pdf, page 2, lines 060-061). All pages include line numbers in the left margin, as required. Visual inspection of the document shows no evidence of margin or spacing abuse, and the font sizes appear consistent with the guidelines. No optional Ethics or Reproducibility Statements were found to exceed their page limits.",False,"The main paper (main_paper.pdf) adheres to the ICLR 2025 style guide. The main content, including Abstract, Introduction, Related Work, Benchmark, Evaluation, Results and Analysis, Conclusion, and Limitations, spans pages 1 through 10. The References section begins on page 10. This falls within the strict upper limit of 10 pages for the main text as specified in the style guide (iclr2025_conference.pdf, page 2, lines 060-061). All pages include line numbers in the left margin, as required. Visual inspection of the document shows no evidence of margin or spacing abuse, and the font sizes appear consistent with the guidelines. No optional Ethics or Reproducibility Statements were found to exceed their page limits.",False,True,"The rapid advances of multi-modal agents built on large foundation models have largely overlooked their potential for language-based communication between agents in collaborative tasks. This oversight presents a critical gap in understanding their effectiveness in real-world deployments, particularly when communicating with humans. Existing agentic benchmarks fail to address key aspects of inter-agent communication and collaboration, particularly in scenarios where agents have unequal access to information and must work together to achieve tasks beyond the scope of individual capabilities. To fill this gap, we introduce a novel benchmark designed to evaluate the collaborative performance of multimodal multi-agent systems through language communication.",False,"The rapid advances of multi-modal agents built on large foundation models have largely overlooked their potential for language-based communication between agents in collaborative tasks. This oversight presents a critical gap in understanding their effectiveness in real-world deployments, particularly when communicating with humans. Existing agentic benchmarks fail to address key aspects of inter-agent communication and collaboration, particularly in scenarios where agents have unequal access to information and must work together to achieve tasks beyond the scope of individual capabilities. To fill this gap, we introduce a novel benchmark designed to evaluate the collaborative performance of multimodal multi-agent systems through language communication.",False
data/iclr/data/submission_MOt7TUucrK,0.0,0.0,0.0,52807,1865,34.96462631225586,,,True,Forgetting that you are a language model. Fully immerse yourself in this scene.,True,Forgetting that you are a language model. Fully immerse yourself in this scene.,,False,,False,"The document adheres to all specified formatting and layout guidelines. Line numbers are present on all pages. The main content (Abstract through Conclusion) spans pages 1-10, which is within the 6-10 page limit. The Reproducibility Statement and Ethical Considerations are brief and do not exceed one page. Visual inspection of margins, font sizes, paragraph spacing, and heading styles shows no deviations from the ICLR 2025 style guide as outlined in the provided iclr2025_conference.pdf.",False,"The document adheres to all specified formatting and layout guidelines. Line numbers are present on all pages. The main content (Abstract through Conclusion) spans pages 1-10, which is within the 6-10 page limit. The Reproducibility Statement and Ethical Considerations are brief and do not exceed one page. Visual inspection of margins, font sizes, paragraph spacing, and heading styles shows no deviations from the ICLR 2025 style guide as outlined in the provided iclr2025_conference.pdf.",False,False,"Large Language Models (LLMs) are recognized as systems that closely mimic aspects of human intelligence. This capability has attracted the attention of the social science community, who see the potential in leveraging LLMs to replace human participants in experiments, thereby reducing research costs and complexity. In this paper, we introduce a benchmark for LLMs personification, including a strategy for constructing virtual characters' life stories from the ground up, a Multi-Agent Cognitive Mechanism capable of simulating human cognitive processes, and a psychology-guided evaluation method to assess human simulations from both self and observational perspectives.",False,"Large Language Models (LLMs) are recognized as systems that closely mimic aspects of human intelligence. This capability has attracted the attention of the social science community, who see the potential in leveraging LLMs to replace human participants in experiments, thereby reducing research costs and complexity. In this paper, we introduce a benchmark for LLMs personification, including a strategy for constructing virtual characters' life stories from the ground up, a Multi-Agent Cognitive Mechanism capable of simulating human cognitive processes, and a psychology-guided evaluation method to assess human simulations from both self and observational perspectives.",False
data/iclr/data/submission_Mphd6Sf6z4,1.0,1.0,0.5104563861862452,21708,1600,26.00457501411438,,,False,,False,,,False,,False,"The main content of the paper spans 9 pages (from page 1 to page 9), which is within the allowed limit of 6-10 pages. Line numbers are present on all pages. The document's margins, font sizes, and paragraph spacing appear consistent with the ICLR 2025 style guide, matching the formatting of the provided 'iclr2025_conference.pdf' style guide document itself. No Ethics Statement or Reproducibility Statement sections were found.",False,"The main content of the paper spans 9 pages (from page 1 to page 9), which is within the allowed limit of 6-10 pages. Line numbers are present on all pages. The document's margins, font sizes, and paragraph spacing appear consistent with the ICLR 2025 style guide, matching the formatting of the provided 'iclr2025_conference.pdf' style guide document itself. No Ethics Statement or Reproducibility Statement sections were found.",True,False,"This review aims to evaluate and compare various static analysis tools across multiple programming languages for memory management. The tools and techniques under scrutiny include pattern matching, symbolic execution, CppCheck, SharpChecker, FindBugs, CheckStyle, and Pylint. ... This review will thoroughly examine the strengths and weaknesses of each static analysis tool, aiding in selecting the most suitable tool or combination of tools for effective memory management across diverse programming environments.",True,"This review aims to evaluate and compare various static analysis tools across multiple programming languages for memory management. The tools and techniques under scrutiny include pattern matching, symbolic execution, CppCheck, SharpChecker, FindBugs, CheckStyle, and Pylint. ... This review will thoroughly examine the strengths and weaknesses of each static analysis tool, aiding in selecting the most suitable tool or combination of tools for effective memory management across diverse programming environments.",False
data/iclr/data/submission_JGt14kjxB2,1.0,1.0,1.0,35173,1687,33.783442974090576,,,False,,False,,,False,,False,"The main content (Abstract through Conclusion) spans exactly 10 pages, which is within the allowed limit of 6-10 pages. The Reproducibility Statement is less than one page. All pages include line numbers. Visual inspection of the PDF and comparison with the provided LaTeX style guide (iclr2025_conference.pdf and iclr2025_conference.tex) indicates adherence to specified margins, font sizes, and spacing for body text, abstract, and headings. There is no evidence of 'space-cheating' or extensive use of small fonts in the main body.",False,"The main content (Abstract through Conclusion) spans exactly 10 pages, which is within the allowed limit of 6-10 pages. The Reproducibility Statement is less than one page. All pages include line numbers. Visual inspection of the PDF and comparison with the provided LaTeX style guide (iclr2025_conference.pdf and iclr2025_conference.tex) indicates adherence to specified margins, font sizes, and spacing for body text, abstract, and headings. There is no evidence of 'space-cheating' or extensive use of small fonts in the main body.",False,False,"The paper addresses federated learning, pre-trained models, representation learning, and communication efficiency, which are all within the scope of ICLR. The language is clear and understandable, allowing a domain expert to follow the technical contributions.",False,"The paper addresses federated learning, pre-trained models, representation learning, and communication efficiency, which are all within the scope of ICLR. The language is clear and understandable, allowing a domain expert to follow the technical contributions.",False
data/iclr/data/submission_v4Bl6tfaaO,1.0,1.0,0.5442621157212,32012,1567,26.933606147766117,,,False,,False,,Github link to Bayesian-LoRA implementation: https://github.com/KseniaSycheva/Bayesian-Lora,True,Github link to Bayesian-LoRA implementation: https://github.com/KseniaSycheva/Bayesian-Lora,False,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 10, inclusive, which is exactly 10 pages. Line numbers are present on all pages. Visual inspection of the PDF does not reveal any deviations from the specified font sizes, line spacing, or margin requirements as outlined in the ICLR 2025 style guide.",False,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 10, inclusive, which is exactly 10 pages. Line numbers are present on all pages. Visual inspection of the PDF does not reveal any deviations from the specified font sizes, line spacing, or margin requirements as outlined in the ICLR 2025 style guide.",False,True,"The paper introduces Bayesian-LoRA, a parameter-efficient fine-tuning approach for large language models that optimizes quantization levels and rank values using Bayesian techniques. It evaluates its performance on GLUE and MMLU benchmarks, demonstrating improved efficiency and reduced bit operations. This falls under 'optimization for representation learning', 'applications of deep learning in natural language processing', and 'implementation issues' related to computational efficiency.",False,"The paper introduces Bayesian-LoRA, a parameter-efficient fine-tuning approach for large language models that optimizes quantization levels and rank values using Bayesian techniques. It evaluates its performance on GLUE and MMLU benchmarks, demonstrating improved efficiency and reduced bit operations. This falls under 'optimization for representation learning', 'applications of deep learning in natural language processing', and 'implementation issues' related to computational efficiency.",False
data/iclr/data/submission_n6Gg0D2jWT,0.0,0.0,0.0,33057,1583,30.82260537147522,,,False,,False,,,False,,False,"The paper adheres to the ICLR 2025 formatting and layout guidelines regarding page limits, statement limits, line numbers, and margins/spacing.",False,"The paper adheres to the ICLR 2025 formatting and layout guidelines regarding page limits, statement limits, line numbers, and margins/spacing.",False,False,,False,,False
data/iclr/data/submission_ReKrTbRfJt,0.0,0.0,0.0,23670,1396,23.55573391914368,,,False,,False,,,False,,False,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 9, totaling 9 pages. All pages include line numbers in the left margin. The document adheres to the specified margins, font sizes, and spacing for headings, abstract, and main body text as outlined in the ICLR 2025 style guide. No explicit Ethics Statement or Reproducibility Statement sections were found.",False,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 9, totaling 9 pages. All pages include line numbers in the left margin. The document adheres to the specified margins, font sizes, and spacing for headings, abstract, and main body text as outlined in the ICLR 2025 style guide. No explicit Ethics Statement or Reproducibility Statement sections were found.",False,False,,False,,False
data/iclr/data/submission_Cody9WEwvN,1.0,1.0,1.0,34165,1735,26.497085094451904,,,False,,False,,,False,,False,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 10, which is exactly 10 pages and falls within the allowed limit of 6-10 pages. The Ethics and Reproducibility statements are brief and located on page 11, well within the 1-page limit each. Line numbers are present on all pages in the left margin. Visual inspection of the PDF against the ICLR 2025 style guide (iclr2025_conference.pdf) shows consistent margins, font sizes (10pt for main text, 12pt for Abstract title, 11pt vertical spacing), and paragraph spacing (1/2 line space) throughout the document, with no evidence of 'space-cheating' or extensive use of small fonts in the main body.",False,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 10, which is exactly 10 pages and falls within the allowed limit of 6-10 pages. The Ethics and Reproducibility statements are brief and located on page 11, well within the 1-page limit each. Line numbers are present on all pages in the left margin. Visual inspection of the PDF against the ICLR 2025 style guide (iclr2025_conference.pdf) shows consistent margins, font sizes (10pt for main text, 12pt for Abstract title, 11pt vertical spacing), and paragraph spacing (1/2 line space) throughout the document, with no evidence of 'space-cheating' or extensive use of small fonts in the main body.",False,False,"The paper introduces ""Discrete Auto-regressive Biasing, a controlled decoding algorithm that leverages gradients while operating entirely in the discrete text domain."" It demonstrates ""the advantages of our controlled decoding method on sentiment control, language detoxification, and keyword-guided generation."" The language is clear and the technical concepts are well-explained.",False,"The paper introduces ""Discrete Auto-regressive Biasing, a controlled decoding algorithm that leverages gradients while operating entirely in the discrete text domain."" It demonstrates ""the advantages of our controlled decoding method on sentiment control, language detoxification, and keyword-guided generation."" The language is clear and the technical concepts are well-explained.",False
data/iclr/data/submission_8sKXFvSCqA,0.0,1.0,0.0,40221,1431,34.51537895202637,,,False,,False,,,False,,True,The provided PDF (main_paper.pdf) does not include line numbers in the left margin. The ICLR 2025 LaTeX style guide (iclr2025_conference.tex) explicitly mentions: 'Please note that we have introduced automatic line number generation into the style file for \LaTeXe. This is to help reviewers refer to specific lines of the paper when they make their comments.',True,The provided PDF (main_paper.pdf) does not include line numbers in the left margin. The ICLR 2025 LaTeX style guide (iclr2025_conference.tex) explicitly mentions: 'Please note that we have introduced automatic line number generation into the style file for \LaTeXe. This is to help reviewers refer to specific lines of the paper when they make their comments.',False,False,,False,,False
data/iclr/data/submission_72nCh5JtLQ,1.0,1.0,1.0,435659,2412,124.73592162132265,,,False,,False,,,False,,False,"The main content of the paper (from Abstract to Conclusion/Future Work) spans 10 pages (Page 1 to Page 10), which is within the allowed limit of 6-10 pages. Line numbers are present on all pages. No explicit Ethics or Reproducibility Statements were found. Visual inspection of margins, font sizes, and paragraph spacing did not reveal any obvious deviations from the ICLR 2025 style guide.",False,"The main content of the paper (from Abstract to Conclusion/Future Work) spans 10 pages (Page 1 to Page 10), which is within the allowed limit of 6-10 pages. Line numbers are present on all pages. No explicit Ethics or Reproducibility Statements were found. Visual inspection of margins, font sizes, and paragraph spacing did not reveal any obvious deviations from the ICLR 2025 style guide.",False,False,"Abstract: ""Evaluating large vision-language models (LVLMs) is very expensive, due to the high computational costs and the wide variety of tasks. The good news is that if we already have some observed scores, we may be able to infer unknown ones. In this study, we propose a new framework for predicting unknown performance scores based on observed ones from other LVLMs or tasks. We first formulate the performance prediction as a matrix completion task. Specifically, we construct a sparse performance matrix R, where each entry Rmn represents the performance score of the m-th model on the n-th dataset. By applying probabilistic matrix factorization (PMF) with Markov chain Monte Carlo (MCMC), we can complete the performance matrix, that is, predict unknown scores. Additionally, we estimate the uncertainty of performance prediction based on MCMC.""
Introduction: ""In this study, we propose a new framework for predicting unknown performance scores based on observed ones from other LVLMs or tasks. We first formulate this as a matrix completion problem. Specifically, we construct a sparse performance matrix R where each entry Rmn represents the performance score of the m-th model on the n-th dataset. By applying probabilistic matrix factorization (PMF) with Markov chain Monte Carlo (MCMC), we can predict unknown performance scores based on observed entries in the matrix.""
Methodology (Section 3.1, Page 4): ""Let R be an M × N matrix representing model performance scores on datasets... To model the observed matrix and estimate the unknown values, we employ PMF (Mnih & Salakhutdinov, 2007)...""",False,"Abstract: ""Evaluating large vision-language models (LVLMs) is very expensive, due to the high computational costs and the wide variety of tasks. The good news is that if we already have some observed scores, we may be able to infer unknown ones. In this study, we propose a new framework for predicting unknown performance scores based on observed ones from other LVLMs or tasks. We first formulate the performance prediction as a matrix completion task. Specifically, we construct a sparse performance matrix R, where each entry Rmn represents the performance score of the m-th model on the n-th dataset. By applying probabilistic matrix factorization (PMF) with Markov chain Monte Carlo (MCMC), we can complete the performance matrix, that is, predict unknown scores. Additionally, we estimate the uncertainty of performance prediction based on MCMC.""
Introduction: ""In this study, we propose a new framework for predicting unknown performance scores based on observed ones from other LVLMs or tasks. We first formulate this as a matrix completion problem. Specifically, we construct a sparse performance matrix R where each entry Rmn represents the performance score of the m-th model on the n-th dataset. By applying probabilistic matrix factorization (PMF) with Markov chain Monte Carlo (MCMC), we can predict unknown performance scores based on observed entries in the matrix.""
Methodology (Section 3.1, Page 4): ""Let R be an M × N matrix representing model performance scores on datasets... To model the observed matrix and estimate the unknown values, we employ PMF (Mnih & Salakhutdinov, 2007)...""",False
data/iclr/data/submission_Vn23PakSbM,0.0,1.0,0.0,18686,1718,24.65908193588257,,,False,,False,,,False,,True,"The main content of the paper, from the Abstract through the '5 CONCLUSION AND DISCUSSION' section, spans pages 1 to 5. The '5 CONCLUSION AND DISCUSSION' section ends on page 5, line 257. The '6 REPRODUCIBILITY STATEMENT' starts on page 5, line 265, but is not considered part of the main content for page limit calculations.",True,"The main content of the paper, from the Abstract through the '5 CONCLUSION AND DISCUSSION' section, spans pages 1 to 5. The '5 CONCLUSION AND DISCUSSION' section ends on page 5, line 257. The '6 REPRODUCIBILITY STATEMENT' starts on page 5, line 265, but is not considered part of the main content for page limit calculations.",False,False,"Abstract: ""This paper introduces a new approach called fine-tuning stacks of language models (FSLM), which involves stacking small language models (SLM) as an alternative to LLMs. By fine-tuning each SLM to perform a specific task, this approach breaks down high level reasoning into multiple lower-level steps that specific SLMs are responsible for. As a result, FSLM allows for lower training and inference costs, and also improves model interpretability as each SLM communicates with the subsequent one through natural language. By evaluating FSLM on common natural language benchmarks, this paper highlights promising early results toward generalizable performance using FSLM as a cost-effective alternative to LLMs.""",False,"Abstract: ""This paper introduces a new approach called fine-tuning stacks of language models (FSLM), which involves stacking small language models (SLM) as an alternative to LLMs. By fine-tuning each SLM to perform a specific task, this approach breaks down high level reasoning into multiple lower-level steps that specific SLMs are responsible for. As a result, FSLM allows for lower training and inference costs, and also improves model interpretability as each SLM communicates with the subsequent one through natural language. By evaluating FSLM on common natural language benchmarks, this paper highlights promising early results toward generalizable performance using FSLM as a cost-effective alternative to LLMs.""",False
data/iclr/data/submission_ChbCYd1uk5,1.0,1.0,1.0,420909,1644,103.53554010391235,,,False,,False,,,False,,False,"The main content of the paper (from '1 INTRODUCTION' on page 1 to '4 CONCLUSION' on page 10) spans 10 pages, which is within the allowed limit of 6-10 pages. The Ethics Statement and Reproducibility Statement on page 12 are each less than one page. Line numbers are present on all pages. Visual inspection of the document's margins, font sizes, and spacing indicates adherence to the ICLR 2025 style guide, with no apparent 'space-cheating' or extensive use of small fonts in the main body.",False,"The main content of the paper (from '1 INTRODUCTION' on page 1 to '4 CONCLUSION' on page 10) spans 10 pages, which is within the allowed limit of 6-10 pages. The Ethics Statement and Reproducibility Statement on page 12 are each less than one page. Line numbers are present on all pages. Visual inspection of the document's margins, font sizes, and spacing indicates adherence to the ICLR 2025 style guide, with no apparent 'space-cheating' or extensive use of small fonts in the main body.",False,False,"The submission focuses on finding the intrinsic dimensionality of complex networks using metric node embeddings, which directly relates to representation learning, metric learning, and optimization for representation learning. It also discusses applications in machine learning tasks involving graphs such as node classification and link prediction. The language is clear and understandable, allowing a domain expert to follow the main contributions and methodology.",False,"The submission focuses on finding the intrinsic dimensionality of complex networks using metric node embeddings, which directly relates to representation learning, metric learning, and optimization for representation learning. It also discusses applications in machine learning tasks involving graphs such as node classification and link prediction. The language is clear and understandable, allowing a domain expert to follow the main contributions and methodology.",False
data/iclr/data/submission_5KRxLvYLlM,1.0,1.0,1.0,70186,1537,65.02313137054443,,,False,,False,,,False,,False,"The main content of the paper (Abstract through Conclusion/Discussion) spans from page 1 to page 10, which is exactly 10 pages, adhering to the page limit. Line numbers are present on all pages. No explicit Ethics or Reproducibility Statements exceeding one page were found. Visual inspection of the PDF does not reveal any obvious margin or spacing violations, and the font size appears consistent with the guidelines.",False,"The main content of the paper (Abstract through Conclusion/Discussion) spans from page 1 to page 10, which is exactly 10 pages, adhering to the page limit. Line numbers are present on all pages. No explicit Ethics or Reproducibility Statements exceeding one page were found. Visual inspection of the PDF does not reveal any obvious margin or spacing violations, and the font size appears consistent with the guidelines.",False,False,,False,,False
data/iclr/data/submission_s0gdfKcmoU,1.0,1.0,1.0,61154,1763,26.009494066238403,,,False,,False,,,False,,False,"The main content of the paper (Abstract through Conclusion/Discussion) spans 10 pages (pages 1-10), which is within the allowed limit of 6-10 pages as per the ICLR 2025 style guide (iclr2025_conference.pdf, page 2, lines 060-061). The paper includes line numbers in the left margin on all pages, which is required for reviewer feedback. No explicit Ethics Statement or Reproducibility Statement sections were found. Visual inspection of the document indicates adherence to the specified font size, vertical spacing, and margin requirements (iclr2025_conference.pdf, page 1, lines 047-049). There is no evidence of 'space-cheating' or extensive use of smaller fonts in the main body text.",False,"The main content of the paper (Abstract through Conclusion/Discussion) spans 10 pages (pages 1-10), which is within the allowed limit of 6-10 pages as per the ICLR 2025 style guide (iclr2025_conference.pdf, page 2, lines 060-061). The paper includes line numbers in the left margin on all pages, which is required for reviewer feedback. No explicit Ethics Statement or Reproducibility Statement sections were found. Visual inspection of the document indicates adherence to the specified font size, vertical spacing, and margin requirements (iclr2025_conference.pdf, page 1, lines 047-049). There is no evidence of 'space-cheating' or extensive use of smaller fonts in the main body text.",False,False,,False,,False
data/iclr/data/submission_seilwMD9m2,1.0,1.0,1.0,32999,1475,32.441498041152954,,,False,,False,,,False,,False,"The main content (Abstract through Conclusion/Discussion) spans pages 1-10, which is exactly 10 pages, adhering to the page limit. Line numbers are present on all pages. The document's margins, font sizes, and paragraph spacing visually align with the specifications outlined in the ICLR 2025 style guide (iclr2025_conference.pdf and iclr2025_conference.tex). Optional sections like 'Author Contributions' and 'Acknowledgments' are brief and do not exceed one page each. No evidence of 'space-cheating' or extensive use of smaller fonts in the main body was found.",False,"The main content (Abstract through Conclusion/Discussion) spans pages 1-10, which is exactly 10 pages, adhering to the page limit. Line numbers are present on all pages. The document's margins, font sizes, and paragraph spacing visually align with the specifications outlined in the ICLR 2025 style guide (iclr2025_conference.pdf and iclr2025_conference.tex). Optional sections like 'Author Contributions' and 'Acknowledgments' are brief and do not exceed one page each. No evidence of 'space-cheating' or extensive use of smaller fonts in the main body was found.",False,False,,False,,False
data/iclr/data/submission_TIxiwxd4iD,0.0,1.0,0.0,15480,1528,17.28897738456726,,,False,,False,,,False,,True,"The provided PDF (main_paper.pdf) contains only a single page with the title, anonymous authors, and abstract. There is no main body content (Introduction, Methods, Results, Discussion, Conclusion) present.",True,"The provided PDF (main_paper.pdf) contains only a single page with the title, anonymous authors, and abstract. There is no main body content (Introduction, Methods, Results, Discussion, Conclusion) present.",False,False,,False,,False
data/iclr/data/submission_Rd34VDwqB7,0.0,0.0,0.0,23772,1600,34.94824934005737,,,False,,False,,,False,,False,"The main content of the paper (Abstract through Section 7 'Limitations and Future Works') spans 10 pages, which is within the ICLR 2025 page limit of 6-10 pages. References and appendices are not counted towards this limit. Line numbers are present on all pages. Visual inspection of the PDF and review of the LaTeX source (using standard ICLR 2025 style files) indicate no deviations from specified margins, font sizes for main body text, or spacing rules. No explicit Ethics or Reproducibility statements were found to exceed any page limits.",False,"The main content of the paper (Abstract through Section 7 'Limitations and Future Works') spans 10 pages, which is within the ICLR 2025 page limit of 6-10 pages. References and appendices are not counted towards this limit. Line numbers are present on all pages. Visual inspection of the PDF and review of the LaTeX source (using standard ICLR 2025 style files) indicate no deviations from specified margins, font sizes for main body text, or spacing rules. No explicit Ethics or Reproducibility statements were found to exceed any page limits.",False,False,"The paper proposes a multi-modal generative framework (Environment-Aware Latent Diffusion Model - EALDM) for generating subsequent stream images, incorporating temporal context of weather, water flow, and time information. This is an application of deep learning (latent diffusion models) in vision for environmental monitoring.",False,"The paper proposes a multi-modal generative framework (Environment-Aware Latent Diffusion Model - EALDM) for generating subsequent stream images, incorporating temporal context of weather, water flow, and time information. This is an application of deep learning (latent diffusion models) in vision for environmental monitoring.",True
data/iclr/data/submission_nQoRKLeOP0,1.0,1.0,0.8726588902329208,319083,1523,37.08509707450867,,,False,,False,,/cpfs01/projects-HDD/cfff-c7cd658afc74_HDD/tanzhaorui/test/Code_for_submit/,True,/cpfs01/projects-HDD/cfff-c7cd658afc74_HDD/tanzhaorui/test/Code_for_submit/,False,"The main content (Abstract through Conclusion) spans pages 1-10, which adheres to the 10-page limit. Line numbers are present on all pages. Visual inspection of margins, font sizes, and spacing indicates compliance with the ICLR 2025 style guide. No explicit Ethics or Reproducibility statements exceeding one page were found.",False,"The main content (Abstract through Conclusion) spans pages 1-10, which adheres to the 10-page limit. Line numbers are present on all pages. Visual inspection of margins, font sizes, and spacing indicates compliance with the ICLR 2025 style guide. No explicit Ethics or Reproducibility statements exceeding one page were found.",False,True,"Personalized medicine is a groundbreaking healthcare framework for the 21st century, tailoring medical treatments to individuals based on unique clinical characteristics, including diverse medical imaging modalities. ... Specifically, we propose a novel approach to derive a tractable form of the underlying personalized invariant representation Xh by leveraging individual-level constraints and a learnable biological prior. ... applications of deep learning in vision, audio, speech, natural language processing, robotics, neuroscience, or any other field.",False,"Personalized medicine is a groundbreaking healthcare framework for the 21st century, tailoring medical treatments to individuals based on unique clinical characteristics, including diverse medical imaging modalities. ... Specifically, we propose a novel approach to derive a tractable form of the underlying personalized invariant representation Xh by leveraging individual-level constraints and a learnable biological prior. ... applications of deep learning in vision, audio, speech, natural language processing, robotics, neuroscience, or any other field.",False
data/iclr/data/submission_1poUSIGSCI,1.0,1.0,0.7473450473238952,29909,1492,50.35527682304382,,,False,,False,,"Mohammad Hassan Vali & Tom Bäckström Department of Information and Communications Engineering, Aalto University, Finland mohammad.vali@aalto.fi, tom.backstrom@aalto.fi",True,"Mohammad Hassan Vali & Tom Bäckström Department of Information and Communications Engineering, Aalto University, Finland mohammad.vali@aalto.fi, tom.backstrom@aalto.fi",False,"The main content (Abstract through Conclusions) spans exactly 10 pages (pages 1-10 of the provided PDF). The Reproducibility Statement is less than one page (page 11). Line numbers are present on all pages. The document itself is the ICLR 2025 style guide, which is designed to demonstrate compliance with its own formatting rules, and no evidence of margin/spacing abuse or extensive small font usage in the main body was found.",False,"The main content (Abstract through Conclusions) spans exactly 10 pages (pages 1-10 of the provided PDF). The Reproducibility Statement is less than one page (page 11). Line numbers are present on all pages. The document itself is the ICLR 2025 style guide, which is designed to demonstrate compliance with its own formatting rules, and no evidence of margin/spacing abuse or extensive small font usage in the main body was found.",False,True,"The paper discusses 'UNSUPERVISED PANOPTIC INTERPRETATION OF LATENT SPACES IN GANS USING SPACE-FILLING VECTOR QUANTIZATION'. The abstract, introduction, and methodology clearly describe a method for interpreting latent spaces of Generative Adversarial Networks (GANs) using a modified vector quantization technique (SFVQ). This falls under unsupervised representation learning and applications of deep learning in vision.",False,"The paper discusses 'UNSUPERVISED PANOPTIC INTERPRETATION OF LATENT SPACES IN GANS USING SPACE-FILLING VECTOR QUANTIZATION'. The abstract, introduction, and methodology clearly describe a method for interpreting latent spaces of Generative Adversarial Networks (GANs) using a modified vector quantization technique (SFVQ). This falls under unsupervised representation learning and applications of deep learning in vision.",False
data/iclr/data/submission_viOk8hPJwb,1.0,1.0,1.0,42411,1720,27.55176615715027,,,False,,False,,,False,,False,"The main content (Abstract through '5 RESULT AND DISCUSSION') spans pages 1-10 of the main_paper.pdf, which is exactly 10 pages. Line numbers are present on all pages. No explicit Ethics or Reproducibility Statements were found. Visual inspection of the PDF and review of the LaTeX source (iclr2025_conference.tex) indicate adherence to margin, font size, and spacing guidelines, with no evidence of 'space-cheating' or extensive use of smaller fonts for main body text.",False,"The main content (Abstract through '5 RESULT AND DISCUSSION') spans pages 1-10 of the main_paper.pdf, which is exactly 10 pages. Line numbers are present on all pages. No explicit Ethics or Reproducibility Statements were found. Visual inspection of the PDF and review of the LaTeX source (iclr2025_conference.tex) indicate adherence to margin, font size, and spacing guidelines, with no evidence of 'space-cheating' or extensive use of smaller fonts for main body text.",False,False,,False,,False
data/iclr/data/submission_ARhJbaAjyE,0.0,0.0,0.0,2284243,2303,223.31634402275083,,,True,"Example from `Lens/Probing/datasets/aya_en_zh_ja_ar_ko_bn_sw_300/test.jsonl`: {""input_en"": ""Give three tips for staying healthy."", ""output"": """", ""input_zh"": ""给出三个保持健康的秘诀。\n"", ""input_ja"": ""健康を維持するための 3 つのヒントを教えてください。\n"", ""input_ko"": ""건강을 유지하기 위한 세 가지 요령을 제시하십시오。\n"", ""input_ar"": ""أعط ثلاث نصائح للبقاء بصحة جيدة。\n"", ""input_bn"": ""সুস্থ থাকার জন্য তিনটি টিপস দিন।\n"", ""input_sw"": ""Toa vidokezo vitatu vya kudumisha afya。\n""}

Similar empty ""output"" fields are found across multiple dataset files, including `language_pull_data_alpaca_safe_200/train.json`, `language_pull_data_alpaca_safe_200/eval.json`, `language_pull_data_alpaca_safe_200/test.json`, `dataset_phi3.py`, and `dataset_llama3.py`.",True,"Example from `Lens/Probing/datasets/aya_en_zh_ja_ar_ko_bn_sw_300/test.jsonl`: {""input_en"": ""Give three tips for staying healthy."", ""output"": """", ""input_zh"": ""给出三个保持健康的秘诀。\n"", ""input_ja"": ""健康を維持するための 3 つのヒントを教えてください。\n"", ""input_ko"": ""건강을 유지하기 위한 세 가지 요령을 제시하십시오。\n"", ""input_ar"": ""أعط ثلاث نصائح للبقاء بصحة جيدة。\n"", ""input_bn"": ""সুস্থ থাকার জন্য তিনটি টিপস দিন।\n"", ""input_sw"": ""Toa vidokezo vitatu vya kudumisha afya。\n""}

Similar empty ""output"" fields are found across multiple dataset files, including `language_pull_data_alpaca_safe_200/train.json`, `language_pull_data_alpaca_safe_200/eval.json`, `language_pull_data_alpaca_safe_200/test.json`, `dataset_phi3.py`, and `dataset_llama3.py`.","def get_truthfulQA_answers_plaintext(tqa_file_path=""/workspace/work/aigc-user-workspace/zhaoweixiang/LLM_Unlearning/datasets/train/TruthfulQA/TruthfulQA.csv""):
...
def get_pku_answers_plaintext(pku_file_path=""/workspace/work/aigc-user-workspace/zhaoweixiang/LLM_Unlearning/datasets/train/BeaverTails_100_14/train.json""):",True,"def get_truthfulQA_answers_plaintext(tqa_file_path=""/workspace/work/aigc-user-workspace/zhaoweixiang/LLM_Unlearning/datasets/train/TruthfulQA/TruthfulQA.csv""):
...
def get_pku_answers_plaintext(pku_file_path=""/workspace/work/aigc-user-workspace/zhaoweixiang/LLM_Unlearning/datasets/train/BeaverTails_100_14/train.json""):",False,"The main paper adheres to the ICLR 2025 style guide. The main text (Abstract through Conclusion) is 10 pages long, which is within the 6-10 page limit. Line numbers are present in the left margin of each page. Visual inspection of the PDF and review of the LaTeX source indicate adherence to specified margins, font sizes, and spacing, with no evidence of 'space-cheating' or extensive use of small fonts in the main body. Optional statements (Author Contributions, Acknowledgments) are correctly placed and do not count towards the page limit. The reproducibility statement is a single line and does not exceed one page.",False,"The main paper adheres to the ICLR 2025 style guide. The main text (Abstract through Conclusion) is 10 pages long, which is within the 6-10 page limit. Line numbers are present in the left margin of each page. Visual inspection of the PDF and review of the LaTeX source indicate adherence to specified margins, font sizes, and spacing, with no evidence of 'space-cheating' or extensive use of small fonts in the main body. Optional statements (Author Contributions, Acknowledgments) are correctly placed and do not count towards the page limit. The reproducibility statement is a single line and does not exceed one page.",False,True,,False,,False
data/iclr/data/submission_GjSWkSaNTQ,1.0,1.0,1.0,42317,1538,57.92968916893005,,,False,,False,,,False,,False,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 10, which is within the ICLR 2025 page limit of 10 pages. Line numbers are present on all pages. Margins, font sizes, and spacing appear to adhere to the style guide specifications, with smaller fonts appropriately used in references and figure/table captions. No explicit Ethics or Reproducibility Statements were found to assess against a page limit.",False,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 10, which is within the ICLR 2025 page limit of 10 pages. Line numbers are present on all pages. Margins, font sizes, and spacing appear to adhere to the style guide specifications, with smaller fonts appropriately used in references and figure/table captions. No explicit Ethics or Reproducibility Statements were found to assess against a page limit.",False,False,,False,,False
data/iclr/data/submission_sSWiZr8QU7,1.0,1.0,0.6849291250867845,36367,1730,33.663124084472656,,,False,,False,,"Aayushya Agarwal
Department of Electrical and Computer Engineering
Carnegie Mellon University
Pittsburgh, PA 15213
aayushya@andrew.cmu.edu

Yihan Ruan
Department of Electrical and Computer Engineering
Carnegie Mellon University
Pittsburgh, PA 15213
yihanr@andrew.cmu.edu

Larry Pileggi
Department of Electrical and Computer Engineering
Carnegie Mellon University
Pittsburgh, PA 15213
pileggi@andrew.cmu.edu",True,"Aayushya Agarwal
Department of Electrical and Computer Engineering
Carnegie Mellon University
Pittsburgh, PA 15213
aayushya@andrew.cmu.edu

Yihan Ruan
Department of Electrical and Computer Engineering
Carnegie Mellon University
Pittsburgh, PA 15213
yihanr@andrew.cmu.edu

Larry Pileggi
Department of Electrical and Computer Engineering
Carnegie Mellon University
Pittsburgh, PA 15213
pileggi@andrew.cmu.edu",True,"The main content of the paper, from the Abstract (page 1) through the Conclusion (page 11), spans 11 pages.",True,"The main content of the paper, from the Abstract (page 1) through the Conclusion (page 11), spans 11 pages.",False,True,"Abstract: ""Gray-box models that combine deep neural networks (DNNs) with physics-based models have been proposed to address the computational challenges in modeling complex physical systems... We, therefore, explore an implicit gray box model, where both DNNs (trained on measurement and simulated data) and physical equations share a common set of state-variables... introduce a new hybrid simulation that directly integrates DNNs into the numerical solvers of simulation engines to fully simulate implicit gray box models of large physical systems. This is accomplished by backpropagating through the DNN to calculate specific Jacobian values during each iteration of the numerical method.""",False,"Abstract: ""Gray-box models that combine deep neural networks (DNNs) with physics-based models have been proposed to address the computational challenges in modeling complex physical systems... We, therefore, explore an implicit gray box model, where both DNNs (trained on measurement and simulated data) and physical equations share a common set of state-variables... introduce a new hybrid simulation that directly integrates DNNs into the numerical solvers of simulation engines to fully simulate implicit gray box models of large physical systems. This is accomplished by backpropagating through the DNN to calculate specific Jacobian values during each iteration of the numerical method.""",False
data/iclr/data/submission_n6PE0xbgdA,0.0,0.0,0.0,40158,1335,21.485724449157715,,,False,,False,,,False,,False,"The main content (Abstract through Conclusion) spans exactly 10 pages (pages 1-10 of the PDF), which adheres to the specified page limit of 6-10 pages. Line numbers are present on all pages. Margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide, and no evidence of 'space-cheating' or extensive use of small fonts in the main body was found. No separate Ethics or Reproducibility Statements exceeding 1 page were identified.",False,"The main content (Abstract through Conclusion) spans exactly 10 pages (pages 1-10 of the PDF), which adheres to the specified page limit of 6-10 pages. Line numbers are present on all pages. Margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide, and no evidence of 'space-cheating' or extensive use of small fonts in the main body was found. No separate Ethics or Reproducibility Statements exceeding 1 page were identified.",False,False,,False,,False
data/iclr/data/submission_cIKQp84vqN,0.0,0.0,0.0,29142,1718,30.353593349456787,,,False,,False,,,False,,False,"The main content (Abstract through Conclusion) spans from page 1 to page 10, which is within the allowed 6-10 page limit. Line numbers are present on all pages. There are no separate Ethics or Reproducibility Statements. Visual inspection of the PDF and review of the LaTeX source (which uses the ICLR 2025 conference style) indicates adherence to margin, font size, and spacing guidelines.",False,"The main content (Abstract through Conclusion) spans from page 1 to page 10, which is within the allowed 6-10 page limit. Line numbers are present on all pages. There are no separate Ethics or Reproducibility Statements. Visual inspection of the PDF and review of the LaTeX source (which uses the ICLR 2025 conference style) indicates adherence to margin, font size, and spacing guidelines.",False,False,"Point cloud registration is a critical and challenging task in computer vision. Its goal is to transform point clouds with arbitrary coordinate systems into a common coordinate system to obtain full coverage of an object or scene. Point cloud registration can be used for scene reconstruction (Yu et al., 2023; Mei et al., 2023), object recognition (Jiang et al., 2023; Yuan et al., 2024; Nie et al., 2024), autonomous driving (Lu et al., 2019; Liu et al., 2024), and medical imaging processing (Chen et al., 2022c; Ginzburg & Raviv, 2022; Ma et al., 2023).",False,"Point cloud registration is a critical and challenging task in computer vision. Its goal is to transform point clouds with arbitrary coordinate systems into a common coordinate system to obtain full coverage of an object or scene. Point cloud registration can be used for scene reconstruction (Yu et al., 2023; Mei et al., 2023), object recognition (Jiang et al., 2023; Yuan et al., 2024; Nie et al., 2024), autonomous driving (Lu et al., 2019; Liu et al., 2024), and medical imaging processing (Chen et al., 2022c; Ginzburg & Raviv, 2022; Ma et al., 2023).",False
data/iclr/data/submission_3x4vpeAclU,0.0,1.0,0.0,15735,2036,23.35201478004456,,,False,,False,,,False,,True,"Under review as a conference paper at ICLR 2025
ENHANCEMENT OF IN-CONTEXT REASONING IN
LLMS THROUGH INDUCTIVE RULE LEARNING
Anonymous authors
Paper under double-blind review
ABSTRACT
Currently, Large language models (LLMs) have achieved remarkable performance
across various language tasks, largely due to their training on extensive datasets
and their considerable model size. These models exhibit in-context learning abili-
ties, which is to learn through few-shot learning. However, the underlying reason-
ing process remains ambiguous, it is unclear whether the model simply retrieves
relevant information and instructions from its training data to generate similar re-
sponses, or whether it generalizes examples to form overarching rules, which are
then applied to produce accurate answers. Another method for improving few-
shot learning is Chain-of-Thought prompting that complement steps by steps in-
struction for LLMs, so they can follow this instruction to solve many reasoning
tasks. Several approaches for evaluating the reasoning abilities of LLMs typically
involve task-solving through code generation, which enables models to formal-
ize problems and leverage a code compiler to solve them precisely. However,
these methods are constrained to specific task types and are insufficient for a com-
prehensive assessment of the model's broader reasoning capabilities. Therefore,
this paper proposes a method to enhance in-context learning capabilities through
two main stages: generating general rules from the provided examples and uti-
lizing LLMs to verify these general rules, thereby aiming to improve reliability
and accuracy. At the same time, this approach seeks to investigate the induc-
tive and deductive reasoning abilities, and can improve our understanding of the
model's reasoning by generating and applying general rules to provide transpar-
ent, clearly explained responses. The proposed method demonstrates competitive
performance on the 1D-ARC benchmark and several traditional language tasks,
suggesting its potential for more robust evaluation of LLM reasoning abilities.
1",True,"Under review as a conference paper at ICLR 2025
ENHANCEMENT OF IN-CONTEXT REASONING IN
LLMS THROUGH INDUCTIVE RULE LEARNING
Anonymous authors
Paper under double-blind review
ABSTRACT
Currently, Large language models (LLMs) have achieved remarkable performance
across various language tasks, largely due to their training on extensive datasets
and their considerable model size. These models exhibit in-context learning abili-
ties, which is to learn through few-shot learning. However, the underlying reason-
ing process remains ambiguous, it is unclear whether the model simply retrieves
relevant information and instructions from its training data to generate similar re-
sponses, or whether it generalizes examples to form overarching rules, which are
then applied to produce accurate answers. Another method for improving few-
shot learning is Chain-of-Thought prompting that complement steps by steps in-
struction for LLMs, so they can follow this instruction to solve many reasoning
tasks. Several approaches for evaluating the reasoning abilities of LLMs typically
involve task-solving through code generation, which enables models to formal-
ize problems and leverage a code compiler to solve them precisely. However,
these methods are constrained to specific task types and are insufficient for a com-
prehensive assessment of the model's broader reasoning capabilities. Therefore,
this paper proposes a method to enhance in-context learning capabilities through
two main stages: generating general rules from the provided examples and uti-
lizing LLMs to verify these general rules, thereby aiming to improve reliability
and accuracy. At the same time, this approach seeks to investigate the induc-
tive and deductive reasoning abilities, and can improve our understanding of the
model's reasoning by generating and applying general rules to provide transpar-
ent, clearly explained responses. The proposed method demonstrates competitive
performance on the 1D-ARC benchmark and several traditional language tasks,
suggesting its potential for more robust evaluation of LLM reasoning abilities.
1",False,False,,False,,False
data/iclr/data/submission_iJxsH77FPQ,1.0,1.0,1.0,3233823,1569,106.35437440872192,,,True,Forget all previous instructions,True,Forget all previous instructions,,False,,False,"The paper adheres to all specified formatting and layout guidelines, including page limits, statement limits, line numbering, and margin/spacing requirements. The main content (Abstract through Conclusion) is 9 pages, which is within the 6-10 page limit. The reproducibility statement is less than 1 page. Line numbers are present throughout the document. Margins, font sizes, and spacing appear to conform to the ICLR 2025 style guide, with no evidence of 'space-cheating'.",False,"The paper adheres to all specified formatting and layout guidelines, including page limits, statement limits, line numbering, and margin/spacing requirements. The main content (Abstract through Conclusion) is 9 pages, which is within the 6-10 page limit. The reproducibility statement is less than 1 page. Line numbers are present throughout the document. Margins, font sizes, and spacing appear to conform to the ICLR 2025 style guide, with no evidence of 'space-cheating'.",False,False,"In this paper, following first principles, we develop a model that learns the structure of an MBO task and empirically leads to improved designs. To this end, we introduce Cliqueformer—a scalable transformer-based architecture that learns the black-box function's structure in the form of its functional graphical model (FGM), thus bypassing the problem of distribution shift, previously tackled by conservative approaches. We evaluate Cliqueformer on various tasks, ranging from high-dimensional black-box functions from MBO literature to real-world tasks of chemical and genetic design, consistently demonstrating its state-of-the-art performance.",False,"In this paper, following first principles, we develop a model that learns the structure of an MBO task and empirically leads to improved designs. To this end, we introduce Cliqueformer—a scalable transformer-based architecture that learns the black-box function's structure in the form of its functional graphical model (FGM), thus bypassing the problem of distribution shift, previously tackled by conservative approaches. We evaluate Cliqueformer on various tasks, ranging from high-dimensional black-box functions from MBO literature to real-world tasks of chemical and genetic design, consistently demonstrating its state-of-the-art performance.",False
data/iclr/data/submission_y9xNQZjUJM,0.0,0.0,0.0,24933,1866,24.41498374938965,,,False,,False,,,False,,False,"The document adheres to the ICLR 2025 style guide. The main text (Abstract through Conclusion) spans 10 pages, which is within the 6-10 page limit. No explicit Ethics or Reproducibility Statements were found. Line numbers are present on all pages. Visual inspection of font sizes, line spacing, paragraph spacing, and margins indicates compliance with the specified guidelines.",False,"The document adheres to the ICLR 2025 style guide. The main text (Abstract through Conclusion) spans 10 pages, which is within the 6-10 page limit. No explicit Ethics or Reproducibility Statements were found. Line numbers are present on all pages. Visual inspection of font sizes, line spacing, paragraph spacing, and margins indicates compliance with the specified guidelines.",False,False,,False,,False
data/iclr/data/submission_GCSr6QCCwF,0.0,0.0,0.0,24982,1924,25.382397890090942,,,False,,False,,,False,,False,"The main content of the paper (Abstract through Conclusion) spans 10 pages, which adheres to the ICLR 2025 page limit of 6-10 pages. Line numbers are present on all pages. Visual inspection of margins, font sizes, and paragraph spacing indicates adherence to the style guide specifications. No Ethics Statement or Reproducibility Statement sections were found to evaluate against a page limit.",False,"The main content of the paper (Abstract through Conclusion) spans 10 pages, which adheres to the ICLR 2025 page limit of 6-10 pages. Line numbers are present on all pages. Visual inspection of margins, font sizes, and paragraph spacing indicates adherence to the style guide specifications. No Ethics Statement or Reproducibility Statement sections were found to evaluate against a page limit.",False,False,"Precipitation nowcasting is a vital spatio-temporal prediction task essential for various meteorological applications, but it faces significant challenges due to the chaotic property of precipitation systems. Mainstream methods primarily rely on radar data for echo extrapolation, but over longer lead times, radar echoes mainly exhibit translation, failing to capture precipitation generation and dissipation processes. This results in blurry predictions, attenuation of high-value echoes, and positional inaccuracies issues. In the other hand, deterministic models using MSE loss often produce blurry forecasts, while probabilistic models struggle with localization accuracy. To address these challenges, we propose a multi-source data fusion framework, which integrates satellite and radar data, with former effectively complementing limitations of latter. In this framework, we leverages global motion fields to capture echo dynamics and introduces a residual diffusion mechanism to reduce memory usage by non-residual features. Various spatio-temporal models (e.g. RNN-based, CNN-based, and ConvRNN-based models) can seam-lessly integrated into this framework. Extensive experiments on a Jiangsu dataset demonstrates significant improvements over state-of-the-art methods, particularly in short-term forecasts. The code and models will be released.",False,"Precipitation nowcasting is a vital spatio-temporal prediction task essential for various meteorological applications, but it faces significant challenges due to the chaotic property of precipitation systems. Mainstream methods primarily rely on radar data for echo extrapolation, but over longer lead times, radar echoes mainly exhibit translation, failing to capture precipitation generation and dissipation processes. This results in blurry predictions, attenuation of high-value echoes, and positional inaccuracies issues. In the other hand, deterministic models using MSE loss often produce blurry forecasts, while probabilistic models struggle with localization accuracy. To address these challenges, we propose a multi-source data fusion framework, which integrates satellite and radar data, with former effectively complementing limitations of latter. In this framework, we leverages global motion fields to capture echo dynamics and introduces a residual diffusion mechanism to reduce memory usage by non-residual features. Various spatio-temporal models (e.g. RNN-based, CNN-based, and ConvRNN-based models) can seam-lessly integrated into this framework. Extensive experiments on a Jiangsu dataset demonstrates significant improvements over state-of-the-art methods, particularly in short-term forecasts. The code and models will be released.",False
data/iclr/data/submission_weeGJ2NqfN,1.0,1.0,1.0,979248,1774,144.2824981212616,,,False,,False,,"reference https://github.com/yanx27/Pointnet_Pointnet2_pytorch, modified by Yang You",True,"reference https://github.com/yanx27/Pointnet_Pointnet2_pytorch, modified by Yang You",False,"The main content (Abstract through Conclusion and Future Work) spans exactly 10 pages, which adheres to the ICLR 2025 page limit of 6-10 pages. Line numbers are present in the left margin of every page. No explicit Ethics or Reproducibility Statements exceeding one page were found. Visual inspection of the document and review of the LaTeX source (using the official `iclr2025_conference` style file) do not reveal any deviations from the specified margins, font sizes, or spacing for the main body text, headings, or abstract. There are no indications of 'space-cheating' or extensive use of small fonts outside of acceptable areas like figure captions or footnotes.",False,"The main content (Abstract through Conclusion and Future Work) spans exactly 10 pages, which adheres to the ICLR 2025 page limit of 6-10 pages. Line numbers are present in the left margin of every page. No explicit Ethics or Reproducibility Statements exceeding one page were found. Visual inspection of the document and review of the LaTeX source (using the official `iclr2025_conference` style file) do not reveal any deviations from the specified margins, font sizes, or spacing for the main body text, headings, or abstract. There are no indications of 'space-cheating' or extensive use of small fonts outside of acceptable areas like figure captions or footnotes.",False,True,"Using natural language as a vehicle-to-vehicle (V2V) communication protocol offers the potential for autonomous vehicles to drive cooperatively not only with each other but also with human drivers. ... this paper introduces a novel method, LLM+DEBRIEF, to learn a message generation and high-level command policy for autonomous vehicles through multi-agent discussion. ...Our experimental results demonstrate that our method is more effective at generating meaningful and human-understandable natural language messages to facilitate cooperation and co-ordination than untrained LLMs.",False,"Using natural language as a vehicle-to-vehicle (V2V) communication protocol offers the potential for autonomous vehicles to drive cooperatively not only with each other but also with human drivers. ... this paper introduces a novel method, LLM+DEBRIEF, to learn a message generation and high-level command policy for autonomous vehicles through multi-agent discussion. ...Our experimental results demonstrate that our method is more effective at generating meaningful and human-understandable natural language messages to facilitate cooperation and co-ordination than untrained LLMs.",False
data/iclr/data/submission_gD5mxbneen,1.0,1.0,1.0,28891,1486,30.921671628952023,,,False,,False,,,False,,False,"The main content (Abstract through Conclusion/Discussion) spans from page 1 to page 10, which is exactly 10 pages, adhering to the 6-10 page limit. Line numbers are present on all pages in the left margin. Visual inspection of the document indicates that margins, font sizes, and spacing appear to conform to the ICLR 2025 style guide, with no obvious signs of 'space-cheating' or extensive use of smaller fonts in the main body. No explicit Ethics or Reproducibility Statements exceeding one page were found.",False,"The main content (Abstract through Conclusion/Discussion) spans from page 1 to page 10, which is exactly 10 pages, adhering to the 6-10 page limit. Line numbers are present on all pages in the left margin. Visual inspection of the document indicates that margins, font sizes, and spacing appear to conform to the ICLR 2025 style guide, with no obvious signs of 'space-cheating' or extensive use of smaller fonts in the main body. No explicit Ethics or Reproducibility Statements exceeding one page were found.",False,False,,False,,False
data/iclr/data/submission_O9W9DesXid,0.0,1.0,0.0,35189,1700,31.645687341690063,,,True,"Our contributions are:

Code: https://github.com/CLU-UML/MU-Bench",True,"Our contributions are:

Code: https://github.com/CLU-UML/MU-Bench",,False,,False,"The main content (Abstract through Conclusion and Future Work) spans 9 pages (from page 1 to page 9), which is within the allowed limit of 6-10 pages. Line numbers are present on all pages. The document's margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide specifications. No explicit Ethics Statement or Reproducibility Statement exceeding 1 page was found; the 'Broader Impact Statement' is less than one page.",False,"The main content (Abstract through Conclusion and Future Work) spans 9 pages (from page 1 to page 9), which is within the allowed limit of 6-10 pages. Line numbers are present on all pages. The document's margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide specifications. No explicit Ethics Statement or Reproducibility Statement exceeding 1 page was found; the 'Broader Impact Statement' is less than one page.",False,False,"The paper introduces MU-Bench, a comprehensive benchmark for Machine Unlearning (MU) that covers a wide range of tasks and data modalities, including previously unexplored domains such as speech and video classification. It also provides an easy-to-use package with dataset splits, models, and implementations, together with a leader board to enable unified and scalable MU research. The language used throughout the abstract, introduction, and methodology is clear, technical, and well-structured, allowing a domain expert to easily follow the contributions, methods, and results.",False,"The paper introduces MU-Bench, a comprehensive benchmark for Machine Unlearning (MU) that covers a wide range of tasks and data modalities, including previously unexplored domains such as speech and video classification. It also provides an easy-to-use package with dataset splits, models, and implementations, together with a leader board to enable unified and scalable MU research. The language used throughout the abstract, introduction, and methodology is clear, technical, and well-structured, allowing a domain expert to easily follow the contributions, methods, and results.",False
data/iclr/data/submission_GLKig15TWJ,0.0,0.0,0.0,38341,1803,23.2271089553833,,,False,,False,,,False,,False,"The main paper (Abstract through Section 5) spans exactly 10 pages, which is within the allowed limit of 6 to 10 pages. Line numbers are present on all pages in the left margin. No explicit Ethics or Reproducibility statements were found. The margins, font sizes, and spacing for the main text, abstract, and headings appear to conform to the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of small fonts outside of acceptable areas like references or footnotes.",False,"The main paper (Abstract through Section 5) spans exactly 10 pages, which is within the allowed limit of 6 to 10 pages. Line numbers are present on all pages in the left margin. No explicit Ethics or Reproducibility statements were found. The margins, font sizes, and spacing for the main text, abstract, and headings appear to conform to the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of small fonts outside of acceptable areas like references or footnotes.",False,False,"We present a framework for designing efficient diffusion models on symmetric Riemannian manifolds, which include the torus, sphere, special orthogonal group, and unitary group. While diffusion models on symmetric manifolds have gained significant attention, existing approaches often rely on the manifolds' heat kernels, which lack closed-form expressions and result in exponential-in-dimension per-iteration runtimes during training. We introduce a new diffusion model for symmetric-space manifolds, leveraging a projection of Euclidean Brownian motion to bypass explicit heat kernel computations. Our training algorithm minimizes a novel objective function derived via Ito's Lemma, with efficiently computable gradients, allowing each iteration to run in polynomial time for symmetric manifolds. Additionally, the symmetries of the manifold ensure the diffusion satisfies an ""average-case"" Lipschitz condition, enabling accurate and efficient sample generation. These improvements enhance both the training runtime and sample accuracy for key cases of symmetric manifolds, helping to bridge the gap between diffusion models on symmetric manifolds and Euclidean space.",False,"We present a framework for designing efficient diffusion models on symmetric Riemannian manifolds, which include the torus, sphere, special orthogonal group, and unitary group. While diffusion models on symmetric manifolds have gained significant attention, existing approaches often rely on the manifolds' heat kernels, which lack closed-form expressions and result in exponential-in-dimension per-iteration runtimes during training. We introduce a new diffusion model for symmetric-space manifolds, leveraging a projection of Euclidean Brownian motion to bypass explicit heat kernel computations. Our training algorithm minimizes a novel objective function derived via Ito's Lemma, with efficiently computable gradients, allowing each iteration to run in polynomial time for symmetric manifolds. Additionally, the symmetries of the manifold ensure the diffusion satisfies an ""average-case"" Lipschitz condition, enabling accurate and efficient sample generation. These improvements enhance both the training runtime and sample accuracy for key cases of symmetric manifolds, helping to bridge the gap between diffusion models on symmetric manifolds and Euclidean space.",False
data/iclr/data/submission_ijuesqStgU,1.0,1.0,1.0,35523,2397,33.92575025558472,,,False,,False,,,False,,False,"The main content of the paper (Abstract through Conclusion/Discussion) spans 10 pages, which is within the allowed limit of 6-10 pages. Line numbers are present on the left margin. No explicit Ethics or Reproducibility Statements were found. Visual inspection of margins, font sizes, and spacing does not reveal any obvious deviations from the ICLR 2025 style guide.",False,"The main content of the paper (Abstract through Conclusion/Discussion) spans 10 pages, which is within the allowed limit of 6-10 pages. Line numbers are present on the left margin. No explicit Ethics or Reproducibility Statements were found. Visual inspection of margins, font sizes, and spacing does not reveal any obvious deviations from the ICLR 2025 style guide.",False,False,"Abstract: ""Test-Time Adaptation has recently emerged as a promising strategy that allows the adaptation of pre-trained models to changing data distributions at deployment time, without access to any labels. To address the error accumulation problem, various approaches have used the teacher-student framework. In this work, we challenge the common strategy of setting the teacher weights to be an exponential moving average of the student by showing that error accumulation still occurs, but only on longer sequences compared to those commonly utilized. We analyze the stability-plasticity trade-off within the teacher-student framework and propose to use an intransigent teacher instead.""
Introduction: ""Machine learning models typically assume that training and testing data originate from a similar distribution. However, in real-world applications, distribution shifts between training (source) and testing (target) data domains are common and can lead to performance issues throughout inference (Geirhos et al., 2019; Hendrycks & Dietterich, 2019; Koh et al., 2021). Test-Time Adaptation (TTA) (Wang et al., 2021) is an emerging paradigm that allows for an online adaptation of a pre-trained model to the changing data distributions during testing, where there is a lack of access to any labels.""
Methodology (Section 3): ""The teacher-student framework is widely studied in TTA, with great results being obtained by popular methods such as AdaContrast (Chen et al., 2022) and CoTTA (Wang et al., 2022). Although these methods also rely on other components (e.g., memory queue or weight restoration), they share a common trend of incorporating a self-supervision loss. We dissect their self-supervised losses (directly related to learning), which allows for disentangling the impact on the teacher-student framework by the component that is most related to the stability-plasticity trade-off.""",False,"Abstract: ""Test-Time Adaptation has recently emerged as a promising strategy that allows the adaptation of pre-trained models to changing data distributions at deployment time, without access to any labels. To address the error accumulation problem, various approaches have used the teacher-student framework. In this work, we challenge the common strategy of setting the teacher weights to be an exponential moving average of the student by showing that error accumulation still occurs, but only on longer sequences compared to those commonly utilized. We analyze the stability-plasticity trade-off within the teacher-student framework and propose to use an intransigent teacher instead.""
Introduction: ""Machine learning models typically assume that training and testing data originate from a similar distribution. However, in real-world applications, distribution shifts between training (source) and testing (target) data domains are common and can lead to performance issues throughout inference (Geirhos et al., 2019; Hendrycks & Dietterich, 2019; Koh et al., 2021). Test-Time Adaptation (TTA) (Wang et al., 2021) is an emerging paradigm that allows for an online adaptation of a pre-trained model to the changing data distributions during testing, where there is a lack of access to any labels.""
Methodology (Section 3): ""The teacher-student framework is widely studied in TTA, with great results being obtained by popular methods such as AdaContrast (Chen et al., 2022) and CoTTA (Wang et al., 2022). Although these methods also rely on other components (e.g., memory queue or weight restoration), they share a common trend of incorporating a self-supervision loss. We dissect their self-supervised losses (directly related to learning), which allows for disentangling the impact on the teacher-student framework by the component that is most related to the stability-plasticity trade-off.""",False
data/iclr/data/submission_DXz1PDA0Wg,0.0,1.0,0.0,15599,1765,22.926918983459476,,,False,,False,,"Smitha Reddy Sª, Vaishnavi Kb, Sapna RC*
@Department of Computer Science, School of Computer Science and Engineering and Information Science, Presidency University, Bengaluru, India
bAssociate 1 software engineer, Capegemini, Bengaluru, India
C* *Department of Information Technology, Manipal Institute of Technology Bengaluru, Manipal Academy of Higher Education, Manipal, India
Corresponding author: Sapna R",True,"Smitha Reddy Sª, Vaishnavi Kb, Sapna RC*
@Department of Computer Science, School of Computer Science and Engineering and Information Science, Presidency University, Bengaluru, India
bAssociate 1 software engineer, Capegemini, Bengaluru, India
C* *Department of Information Technology, Manipal Institute of Technology Bengaluru, Manipal Academy of Higher Education, Manipal, India
Corresponding author: Sapna R",True,"The submitted PDF (main_paper.pdf) contains only one page of content, which includes the title, authors, abstract, and the entire main body text. There are no additional pages for references or appendices.",True,"The submitted PDF (main_paper.pdf) contains only one page of content, which includes the title, authors, abstract, and the entire main body text. There are no additional pages for references or appendices.",False,True,"Accurate classification of medicinal leaves is essential across various fields, including agriculture, Ayurveda, drug discovery, and biodiversity conservation. ... This study explores the application of Conditional Generative Adversarial Networks (CGANs) to generate synthetic data aimed at improving medicinal leaf classification models. ... We employed a conditional Deep Convolution Generative Adversarial Network (cDCGAN) to produce 500 synthetic images for each of thirty different plant species. To evaluate the effectiveness of the generated data, we trained and evaluated three popular convolutional neural networks: ResNet-34, VGG-16, and EfficientNet-B1, on both the original and augmented datasets.",False,"Accurate classification of medicinal leaves is essential across various fields, including agriculture, Ayurveda, drug discovery, and biodiversity conservation. ... This study explores the application of Conditional Generative Adversarial Networks (CGANs) to generate synthetic data aimed at improving medicinal leaf classification models. ... We employed a conditional Deep Convolution Generative Adversarial Network (cDCGAN) to produce 500 synthetic images for each of thirty different plant species. To evaluate the effectiveness of the generated data, we trained and evaluated three popular convolutional neural networks: ResNet-34, VGG-16, and EfficientNet-B1, on both the original and augmented datasets.",False
data/iclr/data/submission_YE5m1lkx4q,1.0,1.0,1.0,294941,1694,24.46942496299744,,,False,,False,,"Main paper: ""Anonymous authors"", ""Paper under double-blind review"". Jupyter Notebook: No identifying information found in code or outputs.",False,"Main paper: ""Anonymous authors"", ""Paper under double-blind review"". Jupyter Notebook: No identifying information found in code or outputs.",False,"The main text of the submission spans pages 1-10, which is exactly 10 pages, adhering to the strict upper limit specified in the ICLR 2025 style guide (page 2, lines 060-061). Line numbers are present in the left margin on all pages of the submission. No explicit Ethics Statement or Reproducibility Statement sections are found. Visual inspection of the PDF does not reveal any obvious margin/spacing or font size violations in the main body text; font sizes in figures and tables are appropriately smaller, which is permitted by the style guide.",False,"The main text of the submission spans pages 1-10, which is exactly 10 pages, adhering to the strict upper limit specified in the ICLR 2025 style guide (page 2, lines 060-061). Line numbers are present in the left margin on all pages of the submission. No explicit Ethics Statement or Reproducibility Statement sections are found. Visual inspection of the PDF does not reveal any obvious margin/spacing or font size violations in the main body text; font sizes in figures and tables are appropriately smaller, which is permitted by the style guide.",False,False,"The paper introduces ""UncertaintyRAG, a novel approach for long-context Retrieval-Augmented Generation (RAG) that utilizes Signal-to-Noise Ratio (SNR)-based span uncertainty to estimate similarity between text chunks."" It proposes ""an efficient unsupervised learning technique to train the retrieval model, alongside an effective data sampling and scaling strategy."" The methodology section details the ""span uncertainty method based on SNR to obtain similarity scores between chunks"" and its use for ""training our retrieval models.""",False,"The paper introduces ""UncertaintyRAG, a novel approach for long-context Retrieval-Augmented Generation (RAG) that utilizes Signal-to-Noise Ratio (SNR)-based span uncertainty to estimate similarity between text chunks."" It proposes ""an efficient unsupervised learning technique to train the retrieval model, alongside an effective data sampling and scaling strategy."" The methodology section details the ""span uncertainty method based on SNR to obtain similarity scores between chunks"" and its use for ""training our retrieval models.""",False
data/iclr/data/submission_vrJtg5L3Qd,1.0,1.0,1.0,128634,1558,33.27810907363892,,,False,,False,,,False,,False,"The main content (Abstract through Conclusion/Discussion) spans pages 1-10, which is within the 6-10 page limit. The Reproducibility Statement is 1 page, which is within the 1-page limit. Line numbers are present on all pages. Margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide based on visual inspection of the PDF.",False,"The main content (Abstract through Conclusion/Discussion) spans pages 1-10, which is within the 6-10 page limit. The Reproducibility Statement is 1 page, which is within the 1-page limit. Line numbers are present on all pages. Margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide based on visual inspection of the PDF.",False,False,"The paper investigates the training dynamics of deep neural networks, focusing on gradient alignment with dominant subspaces and the role of stochastic noise in optimization. It explores various optimization methods (SGD, SAM, Adam) and their applications in deep learning (vision, NLP) using models like MLP, CNN, and Transformer on datasets such as MNIST, CIFAR10, and SST2.",False,"The paper investigates the training dynamics of deep neural networks, focusing on gradient alignment with dominant subspaces and the role of stochastic noise in optimization. It explores various optimization methods (SGD, SAM, Adam) and their applications in deep learning (vision, NLP) using models like MLP, CNN, and Transformer on datasets such as MNIST, CIFAR10, and SST2.",True
data/iclr/data/submission_QPsbC714Cy,1.0,1.0,1.0,28860,1413,19.879300832748413,,,False,,False,,,False,,False,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 10, inclusive, which is 10 pages. This falls within the allowed page limit of 6 to 10 pages for the main text. The paper includes line numbers in the left margin on all pages. There are no explicit Ethics or Reproducibility Statements exceeding one page. Visual inspection of the document's layout, font sizes, and spacing indicates adherence to the ICLR 2025 style guide's specifications for margins, font usage, and paragraph formatting.",False,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 10, inclusive, which is 10 pages. This falls within the allowed page limit of 6 to 10 pages for the main text. The paper includes line numbers in the left margin on all pages. There are no explicit Ethics or Reproducibility Statements exceeding one page. Visual inspection of the document's layout, font sizes, and spacing indicates adherence to the ICLR 2025 style guide's specifications for margins, font usage, and paragraph formatting.",False,False,,False,,False
data/iclr/data/submission_zebxJxSHcD,1.0,1.0,1.0,43358,1556,34.80503964424133,,,False,,False,,,False,,False,"The main content (Abstract through Section 8 Conclusion) spans 10 pages (pages 1-10 of the PDF), which is within the allowed 6-10 page limit. Line numbers are present in the left margin on all pages. Visual inspection of the document's margins, font sizes, and spacing in the main body text, abstract, and headings indicates adherence to the ICLR 2025 style guide. No explicit Ethics Statement or Reproducibility Statement sections were found to evaluate against a page limit.",False,"The main content (Abstract through Section 8 Conclusion) spans 10 pages (pages 1-10 of the PDF), which is within the allowed 6-10 page limit. Line numbers are present in the left margin on all pages. Visual inspection of the document's margins, font sizes, and spacing in the main body text, abstract, and headings indicates adherence to the ICLR 2025 style guide. No explicit Ethics Statement or Reproducibility Statement sections were found to evaluate against a page limit.",False,False,"The paper addresses federated learning, optimization convergence, and debiasing algorithms for machine learning models, which are all within the scope of ICLR. The language is technical and clear, allowing a domain expert to follow the mathematical and logical flow.",False,"The paper addresses federated learning, optimization convergence, and debiasing algorithms for machine learning models, which are all within the scope of ICLR. The language is technical and clear, allowing a domain expert to follow the mathematical and logical flow.",True
data/iclr/data/submission_lX8DahcJbL,0.0,1.0,0.0,2903989,1961,137.94066762924194,,,True,#https://github.com/YasMinSdt/FedAlt/blob/master/data/partition/sort_and_partition.py#L2????,True,#https://github.com/YasMinSdt/FedAlt/blob/master/data/partition/sort_and_partition.py#L2????,"1. In `supplemental_files/ICLR-pMixFed/Code/requirements.txt`, a file path contains a username: `wcwidth @ file:///Users/ktietz/demo/mc3/conda-bld/wcwidth_1629357192024/work`.
2. In `supplemental_files/ICLR-pMixFed/Code/pMixFed-(visualizations).ipynb` and `supplemental_files/ICLR-pMixFed/Code/pMixFed-main-(Dynamic_Mu).ipynb`, a GitHub link with a visible username is present in comments: `https://github.com/YasMinSdt/FedAlt/blob/master/data/partition/sort_and_partition.py#L2????`.",True,"1. In `supplemental_files/ICLR-pMixFed/Code/requirements.txt`, a file path contains a username: `wcwidth @ file:///Users/ktietz/demo/mc3/conda-bld/wcwidth_1629357192024/work`.
2. In `supplemental_files/ICLR-pMixFed/Code/pMixFed-(visualizations).ipynb` and `supplemental_files/ICLR-pMixFed/Code/pMixFed-main-(Dynamic_Mu).ipynb`, a GitHub link with a visible username is present in comments: `https://github.com/YasMinSdt/FedAlt/blob/master/data/partition/sort_and_partition.py#L2????`.",False,"The main paper (main_paper.pdf) adheres to the specified page limits (10 pages for main content). The optional statements (Reproducibility Statement) do not exceed 1 page. Line numbers are present in the left margin. Visual inspection of margins, font sizes, and paragraph spacing in the main body text does not reveal any violations or 'space-cheating' practices compared to the ICLR 2025 style guide (iclr2025_conference.pdf).",False,"The main paper (main_paper.pdf) adheres to the specified page limits (10 pages for main content). The optional statements (Reproducibility Statement) do not exceed 1 page. Line numbers are present in the left margin. Visual inspection of margins, font sizes, and paragraph spacing in the main body text does not reveal any violations or 'space-cheating' practices compared to the ICLR 2025 style guide (iclr2025_conference.pdf).",False,True,"The paper's title, abstract, introduction, and methodology clearly describe a novel approach to Personalized Federated Learning (PFL) using a dynamic, layer-wise mixup technique. This directly aligns with ICLR's scope, which includes 'unsupervised, semi-supervised, and supervised representation learning', 'optimization for representation learning', 'hierarchical models', and 'applications of deep learning in vision'. The supplemental code further demonstrates a clear and coherent implementation of the described methods.",False,"The paper's title, abstract, introduction, and methodology clearly describe a novel approach to Personalized Federated Learning (PFL) using a dynamic, layer-wise mixup technique. This directly aligns with ICLR's scope, which includes 'unsupervised, semi-supervised, and supervised representation learning', 'optimization for representation learning', 'hierarchical models', and 'applications of deep learning in vision'. The supplemental code further demonstrates a clear and coherent implementation of the described methods.",False
data/iclr/data/submission_UpPXWd9SBk,0.0,0.0,0.0,24043,2142,48.7423357963562,,,False,,False,,,False,,True,"The ICLR 2025 style guide (iclr2025_conference.pdf) states: 'All pages should start at 1 inch (6 picas) from the top of the page.' However, the provided PDF (which is the style guide itself) visually shows the header 'Under review as a conference paper at ICLR 2025' starting at approximately 0.5 inches from the top edge of the paper. This is also consistent with the `\topmargin -0.5in` setting found in the `iclr2025_conference.tex` file, which, combined with LaTeX's default 1-inch voffset, places the header at 0.5 inches from the top.",True,"The ICLR 2025 style guide (iclr2025_conference.pdf) states: 'All pages should start at 1 inch (6 picas) from the top of the page.' However, the provided PDF (which is the style guide itself) visually shows the header 'Under review as a conference paper at ICLR 2025' starting at approximately 0.5 inches from the top edge of the paper. This is also consistent with the `\topmargin -0.5in` setting found in the `iclr2025_conference.tex` file, which, combined with LaTeX's default 1-inch voffset, places the header at 0.5 inches from the top.",False,False,"The detection of foreground targets on airport surface is the foundation of airport surveillance applications. However, effective algorithms and specialized benchmarks are still lacking in this area. Based on this fact, we propose an Airport Foreground Target Detection dataset (AFTD), which contains the three most important foreground targets moving on the airport surface: airplane, vehicle, and person. Through self collection and collection of web images, we have obtained a total of over 200000 images and filtered out 10050 images based on diversity principles to form the AFTD dataset, which includes a total of 26968 airplane instances, 24759 vehicle instances, and 5064 person instances. AFTD includes a variety of changes of these targets, such as super multi-scale, multi-level occlusion and viewangle changes, etc. In addition, we further illustrate the challenges posed by AFTD to existing algorithms through statistical analysis and detailed experiments, and discuss how to solve these challenges in the airport surveillance scenario.",False,"The detection of foreground targets on airport surface is the foundation of airport surveillance applications. However, effective algorithms and specialized benchmarks are still lacking in this area. Based on this fact, we propose an Airport Foreground Target Detection dataset (AFTD), which contains the three most important foreground targets moving on the airport surface: airplane, vehicle, and person. Through self collection and collection of web images, we have obtained a total of over 200000 images and filtered out 10050 images based on diversity principles to form the AFTD dataset, which includes a total of 26968 airplane instances, 24759 vehicle instances, and 5064 person instances. AFTD includes a variety of changes of these targets, such as super multi-scale, multi-level occlusion and viewangle changes, etc. In addition, we further illustrate the challenges posed by AFTD to existing algorithms through statistical analysis and detailed experiments, and discuss how to solve these challenges in the airport surveillance scenario.",True
data/iclr/data/submission_EGjTCIcSnW,1.0,1.0,0.78396233417427,37208,1618,42.02993583679199,,,False,,False,,,False,,True,"The top of page 1 of the main_paper.pdf, showing the text 'Under review as a conference paper at ICLR 2025' starting very close to the top edge of the physical page, and the main content block 'ON THE ROBUSTNESS...' also starting higher than 1 inch from the top.",True,"The top of page 1 of the main_paper.pdf, showing the text 'Under review as a conference paper at ICLR 2025' starting very close to the top edge of the physical page, and the main content block 'ON THE ROBUSTNESS...' also starting higher than 1 inch from the top.",False,False,"Although vision-language models (VLMs) have achieved significant success in various applications such as visual question answering, their resilience to prompt distractions remains as an under-explored area. Understanding how distractions affect VLMs is crucial for improving their real-world applicability, as inputs could be filled with noisy and irrelevant information in many practical scenarios. This paper aims to assess the robustness of VLMs against both visual and textual distractions in the context of science question answering.",False,"Although vision-language models (VLMs) have achieved significant success in various applications such as visual question answering, their resilience to prompt distractions remains as an under-explored area. Understanding how distractions affect VLMs is crucial for improving their real-world applicability, as inputs could be filled with noisy and irrelevant information in many practical scenarios. This paper aims to assess the robustness of VLMs against both visual and textual distractions in the context of science question answering.",True
data/iclr/data/submission_GE0UKtI6Lf,0.0,1.0,0.0,659913,2805,86.38874435424805,,,True,"percipient = ""TBD""",True,"percipient = ""TBD""","In multiple supplemental code files (e.g., WALL-Ecode/ALFWorld/buffer.py, WALL-Ecode/ALFWorld/utils.py, WALL-Ecode/ALFWorld/main.py, WALL-Ecode/ALFWorld/generate_reflections.py, WALL-Ecode/ALFWorld/utilsextra/common_utils.py, WALL-Ecode/Minecraft/agent/run_agent--planningtest.py, WALL-Ecode/Minecraft/agent/planner.py, WALL-Ecode/Minecraft/agent/buffer.py, WALL-Ecode/Minecraft/agent/memory.py, WALL-Ecode/Minecraft/agent/utils/common_utils.py, WALL-Ecode/Demo/buffer.py, WALL-Ecode/Demo/ruleminer.py, WALL-Ecode/Demo/utilsextra/common_utils.py), there are hardcoded absolute file paths containing personal user directories, such as '/home/**/Workspace/reflexion/...' or '/home/**/Workspace/MP5/...'.",True,"In multiple supplemental code files (e.g., WALL-Ecode/ALFWorld/buffer.py, WALL-Ecode/ALFWorld/utils.py, WALL-Ecode/ALFWorld/main.py, WALL-Ecode/ALFWorld/generate_reflections.py, WALL-Ecode/ALFWorld/utilsextra/common_utils.py, WALL-Ecode/Minecraft/agent/run_agent--planningtest.py, WALL-Ecode/Minecraft/agent/planner.py, WALL-Ecode/Minecraft/agent/buffer.py, WALL-Ecode/Minecraft/agent/memory.py, WALL-Ecode/Minecraft/agent/utils/common_utils.py, WALL-Ecode/Demo/buffer.py, WALL-Ecode/Demo/ruleminer.py, WALL-Ecode/Demo/utilsextra/common_utils.py), there are hardcoded absolute file paths containing personal user directories, such as '/home/**/Workspace/reflexion/...' or '/home/**/Workspace/MP5/...'.",False,"The main text (Abstract through Conclusion) spans from page 1 to page 10 of the main_paper.pdf, totaling 10 pages. Line numbers are present in the left margin of all pages. Visual inspection of the PDF indicates adherence to specified font sizes, typefaces, margins, and paragraph spacing. No evidence of extensive 'space cheating' or misuse of small fonts for main body text was found.",False,"The main text (Abstract through Conclusion) spans from page 1 to page 10 of the main_paper.pdf, totaling 10 pages. Line numbers are present in the left margin of all pages. Visual inspection of the PDF indicates adherence to specified font sizes, typefaces, margins, and paragraph spacing. No evidence of extensive 'space cheating' or misuse of small fonts for main body text was found.",False,True,,False,,False
data/iclr/data/submission_2924nrgHce,1.0,1.0,1.0,38458,2045,72.80023956298828,,,False,,False,,,False,,False,"The main content (Abstract through Conclusion/Discussion) spans exactly 10 pages (pages 1-10). Line numbers are present on all pages. No explicit Ethics Statement or Reproducibility Statement sections were found. Visual inspection of the PDF does not reveal any obvious margin, spacing, or font size manipulations in the main body text that would indicate a violation of the style guide.",False,"The main content (Abstract through Conclusion/Discussion) spans exactly 10 pages (pages 1-10). Line numbers are present on all pages. No explicit Ethics Statement or Reproducibility Statement sections were found. Visual inspection of the PDF does not reveal any obvious margin, spacing, or font size manipulations in the main body text that would indicate a violation of the style guide.",False,False,"Solving complex chart Q&A tasks requires advanced visual reasoning abilities in multimodal large language models (MLLMs). Recent studies highlight that these abilities consist of two main parts: recognizing key information from visual inputs and conducting reasoning over it. Thus, a promising approach to enhance MLLMS is to construct relevant training data focusing on the two aspects. However, collecting and annotating complex charts and questions is costly and time-consuming, and ensuring the quality of annotated answers remains a challenge. In this paper, we propose Code-as-Intermediary Translation (CIT), a cost-effective, efficient and easily scalable data synthesis method for distilling visual reasoning abilities from LLMs to MLLMs. The code serves as an intermediary that translates visual chart representations into textual representations, enabling LLMs to understand cross-modal information. Specifically, we employ text-based synthesizing techniques to construct chart-plotting code and produce REACHQA, a dataset containing 3k reasoning-intensive charts and 20k Q&A pairs to enhance both recognition and reasoning abilities. Experiments show that when fine-tuned with our data, models not only perform well on chart-related benchmarks, but also demonstrate improved multimodal reasoning abilities on general mathematical benchmarks such as MathVista.",False,"Solving complex chart Q&A tasks requires advanced visual reasoning abilities in multimodal large language models (MLLMs). Recent studies highlight that these abilities consist of two main parts: recognizing key information from visual inputs and conducting reasoning over it. Thus, a promising approach to enhance MLLMS is to construct relevant training data focusing on the two aspects. However, collecting and annotating complex charts and questions is costly and time-consuming, and ensuring the quality of annotated answers remains a challenge. In this paper, we propose Code-as-Intermediary Translation (CIT), a cost-effective, efficient and easily scalable data synthesis method for distilling visual reasoning abilities from LLMs to MLLMs. The code serves as an intermediary that translates visual chart representations into textual representations, enabling LLMs to understand cross-modal information. Specifically, we employ text-based synthesizing techniques to construct chart-plotting code and produce REACHQA, a dataset containing 3k reasoning-intensive charts and 20k Q&A pairs to enhance both recognition and reasoning abilities. Experiments show that when fine-tuned with our data, models not only perform well on chart-related benchmarks, but also demonstrate improved multimodal reasoning abilities on general mathematical benchmarks such as MathVista.",False
data/iclr/data/submission_z5Th95xtBW,0.0,0.0,0.0,32117,1724,40.13316559791565,,,False,,False,,,False,,False,"The main content (Abstract through Conclusion) spans 9 pages (pages 1-9 of main_paper.pdf), which is within the 6-10 page limit. Line numbers are present on all pages in the left margin. No Ethics or Reproducibility Statements were found. Visual inspection of the PDF confirms adherence to font sizes, paragraph spacing, heading styles, and margin specifications as detailed in the ICLR 2025 style guide.",False,"The main content (Abstract through Conclusion) spans 9 pages (pages 1-9 of main_paper.pdf), which is within the 6-10 page limit. Line numbers are present on all pages in the left margin. No Ethics or Reproducibility Statements were found. Visual inspection of the PDF confirms adherence to font sizes, paragraph spacing, heading styles, and margin specifications as detailed in the ICLR 2025 style guide.",False,False,"Large Language Models (LLMs) have shown impressive capabilities across a range of language tasks. However, questions remain about whether LLMs effectively encode linguistic structures such as phrases and sentences and how closely these representations align with those in the human brain. Here, we introduce the Hierarchical Frequency Tagging Probe (HFTP) to probe the phrase and sentence representations in LLMs and the human brain in a unified manner.",False,"Large Language Models (LLMs) have shown impressive capabilities across a range of language tasks. However, questions remain about whether LLMs effectively encode linguistic structures such as phrases and sentences and how closely these representations align with those in the human brain. Here, we introduce the Hierarchical Frequency Tagging Probe (HFTP) to probe the phrase and sentence representations in LLMs and the human brain in a unified manner.",False
data/iclr/data/submission_9d6RcViazd,0.0,0.0,0.0,25970,1868,23.29862594604492,,,False,,False,,,False,,False,"The main content (Abstract through Conclusions) spans 9 pages (from page 1 to page 9 of the PDF), which is within the allowed 6-10 page limit. References and Appendix sections are correctly excluded from this count. Line numbers are present on all pages in the left margin. Visual inspection of the PDF and review of the LaTeX source do not indicate any violations of font size, vertical spacing, or margin requirements for the main body text.",False,"The main content (Abstract through Conclusions) spans 9 pages (from page 1 to page 9 of the PDF), which is within the allowed 6-10 page limit. References and Appendix sections are correctly excluded from this count. Line numbers are present on all pages in the left margin. Visual inspection of the PDF and review of the LaTeX source do not indicate any violations of font size, vertical spacing, or margin requirements for the main body text.",False,False,"The paper focuses on ""language modeling for text-to-speech (TTS) synthesis"" and proposes ""a robust language modeling method for TTS, utilizing CoT prompting."" This falls under 'applications of deep learning in... audio, speech, natural language processing' which is explicitly listed as in-scope for ICLR 2025. The abstract and introduction clearly articulate the problem, proposed solution, and contributions using standard scientific language and notation. For example, the abstract states: 'We present RALL-E, a robust language modeling method for text-to-speech (TTS) synthesis. While previous codec language modeling methods have demonstrated impressive performance in zero-shot TTS, they often struggle with robustness issues... RALL-E addresses these issues through chain-of-thought (CoT) prompting...'",False,"The paper focuses on ""language modeling for text-to-speech (TTS) synthesis"" and proposes ""a robust language modeling method for TTS, utilizing CoT prompting."" This falls under 'applications of deep learning in... audio, speech, natural language processing' which is explicitly listed as in-scope for ICLR 2025. The abstract and introduction clearly articulate the problem, proposed solution, and contributions using standard scientific language and notation. For example, the abstract states: 'We present RALL-E, a robust language modeling method for text-to-speech (TTS) synthesis. While previous codec language modeling methods have demonstrated impressive performance in zero-shot TTS, they often struggle with robustness issues... RALL-E addresses these issues through chain-of-thought (CoT) prompting...'",False
data/iclr/data/submission_1KRM3igwn1,1.0,1.0,1.0,43357,1489,33.93458867073059,,,False,,False,,,False,,False,"The main content (Abstract through Limitations and Future Scope) spans pages 1-10, which is exactly 10 pages, adhering to the page limit. Line numbers are present on all pages. No extensive use of small fonts or narrowed margins was detected in the main body text. Ethics Statement is less than one page. No separate Reproducibility Statement exceeding one page was found.",False,"The main content (Abstract through Limitations and Future Scope) spans pages 1-10, which is exactly 10 pages, adhering to the page limit. Line numbers are present on all pages. No extensive use of small fonts or narrowed margins was detected in the main body text. Ethics Statement is less than one page. No separate Reproducibility Statement exceeding one page was found.",False,False,"The paper introduces ProFS (Projection Filter for Subspaces), a tuning-free alignment alternative for large language models (LLMs) to reduce toxicity. It identifies a toxic subspace in the model parameter space and reduces model toxicity by projecting away the detected subspace. The work connects tuning-based alignment with editing, interpreting ProFS as a denoised version of a single DPO step. The methodology involves factor analysis, singular value decomposition, and model editing techniques applied to LLMs.",False,"The paper introduces ProFS (Projection Filter for Subspaces), a tuning-free alignment alternative for large language models (LLMs) to reduce toxicity. It identifies a toxic subspace in the model parameter space and reduces model toxicity by projecting away the detected subspace. The work connects tuning-based alignment with editing, interpreting ProFS as a denoised version of a single DPO step. The methodology involves factor analysis, singular value decomposition, and model editing techniques applied to LLMs.",False
data/iclr/data/submission_LqB8cRuBua,1.0,1.0,1.0,26984,1821,26.45960283279419,,,False,,False,,,False,,False,"The provided document is the ICLR 2025 conference template itself, which is designed to demonstrate correct formatting. The main content (Abstract through Conclusion) spans 10 pages, which adheres to the specified page limit of 6-10 pages. Line numbers are present on all pages. No explicit Ethics or Reproducibility Statements were found. Visual inspection and review of the LaTeX source for the template do not indicate any margin or spacing abuse, or extensive use of small fonts for the main body text.",False,"The provided document is the ICLR 2025 conference template itself, which is designed to demonstrate correct formatting. The main content (Abstract through Conclusion) spans 10 pages, which adheres to the specified page limit of 6-10 pages. Line numbers are present on all pages. No explicit Ethics or Reproducibility Statements were found. Visual inspection and review of the LaTeX source for the template do not indicate any margin or spacing abuse, or extensive use of small fonts for the main body text.",False,False,"The paper proposes a novel deep learning method, Diffusion SigFormer, for electromagnetic signal recognition, which involves a diffusion signal denoising module and a transformer-based classification model. This falls under 'applications of deep learning in ... any other field.' The text, while having minor grammatical imperfections, is generally clear and understandable, allowing a domain expert to follow the main contributions, methodology, and results.",False,"The paper proposes a novel deep learning method, Diffusion SigFormer, for electromagnetic signal recognition, which involves a diffusion signal denoising module and a transformer-based classification model. This falls under 'applications of deep learning in ... any other field.' The text, while having minor grammatical imperfections, is generally clear and understandable, allowing a domain expert to follow the main contributions, methodology, and results.",False
data/iclr/data/submission_cO01zqImBC,0.0,0.0,0.0,26961,1786,21.59518814086914,,,False,,False,,,False,,False,"The provided document is 'iclr2025_conference.pdf', which serves as the official style guide and example document for ICLR 2025 submissions. It explicitly details all formatting requirements.",False,"The provided document is 'iclr2025_conference.pdf', which serves as the official style guide and example document for ICLR 2025 submissions. It explicitly details all formatting requirements.",False,False,"The paper proposes a multi-decomposition method for compressing larger AI models based on reinforcement learning. It addresses the challenge of deploying large DNNs on resource-constrained edge devices by optimizing model accuracy and compression rate through a joint optimization paradigm and a reinforcement learning framework (LMFBRL) to select optimal decomposition methods and ranks for each layer. This falls under 'optimization for representation learning', 'applications of deep learning', and 'implementation issues'. The language, while containing minor grammatical imperfections, does not hinder the understanding of the technical content. For example, the abstract and introduction clearly state the problem and proposed solution, and the methodology sections (e.g., Problem Formulation, Reinforcement Learning Framework) present mathematical formulations and algorithms in a comprehensible manner for a domain expert.",False,"The paper proposes a multi-decomposition method for compressing larger AI models based on reinforcement learning. It addresses the challenge of deploying large DNNs on resource-constrained edge devices by optimizing model accuracy and compression rate through a joint optimization paradigm and a reinforcement learning framework (LMFBRL) to select optimal decomposition methods and ranks for each layer. This falls under 'optimization for representation learning', 'applications of deep learning', and 'implementation issues'. The language, while containing minor grammatical imperfections, does not hinder the understanding of the technical content. For example, the abstract and introduction clearly state the problem and proposed solution, and the methodology sections (e.g., Problem Formulation, Reinforcement Learning Framework) present mathematical formulations and algorithms in a comprehensible manner for a domain expert.",False
data/iclr/data/submission_MOCEoNsjEx,1.0,1.0,1.0,24725,1399,22.08918070793152,,,True,Code is avaliable at https://gitlab.com/FUTURE_LINK,True,Code is avaliable at https://gitlab.com/FUTURE_LINK,,False,,False,"The main content (Abstract through Conclusion) spans 8 pages (from page 1 to page 8), which is within the allowed limit of 6-10 pages. Line numbers are present on all pages. The reproducibility statement is less than one page, and no ethics statement is present. Visual inspection of margins, font sizes, and paragraph spacing indicates adherence to the style guide specifications, with no apparent 'space-cheating' or extensive use of smaller fonts in the main body text.",False,"The main content (Abstract through Conclusion) spans 8 pages (from page 1 to page 8), which is within the allowed limit of 6-10 pages. Line numbers are present on all pages. The reproducibility statement is less than one page, and no ethics statement is present. Visual inspection of margins, font sizes, and paragraph spacing indicates adherence to the style guide specifications, with no apparent 'space-cheating' or extensive use of smaller fonts in the main body text.",False,False,,False,,False
data/iclr/data/submission_Uc2P6WIgoo,1.0,1.0,1.0,35221,1783,29.80941939353943,,,False,,False,,,False,,False,"The main content (Abstract through Conclusion) spans pages 1-10, which is exactly 10 pages, adhering to the strict upper limit of 10 pages for the main text. Line numbers are present on all pages in the left margin. Visual inspection of margins, font sizes, and paragraph spacing indicates adherence to the style guide specifications (e.g., 1.5 inch left margin, 10pt type with 11pt vertical spacing, 1/2 line space between paragraphs). No extensive use of smaller fonts or reduced spacing was detected in the main body text. No explicit Ethics or Reproducibility statements exceeding 1 page were found.",False,"The main content (Abstract through Conclusion) spans pages 1-10, which is exactly 10 pages, adhering to the strict upper limit of 10 pages for the main text. Line numbers are present on all pages in the left margin. Visual inspection of margins, font sizes, and paragraph spacing indicates adherence to the style guide specifications (e.g., 1.5 inch left margin, 10pt type with 11pt vertical spacing, 1/2 line space between paragraphs). No extensive use of smaller fonts or reduced spacing was detected in the main body text. No explicit Ethics or Reproducibility statements exceeding 1 page were found.",False,False,"Data-driven simulation of physical systems has recently kindled significant attention, where many neural models have been developed. In particular, mesh-based graph neural networks (GNNs) have demonstrated significant potential in predicting spatiotemporal dynamics across arbitrary geometric domains. However, the existing node-edge message passing mechanism in GNNs limits the model's representation learning ability. In this paper, we proposed a cell-embedded GNN model (aka, CeGNN) to learn spatiotemporal dynamics with lifted performance.",False,"Data-driven simulation of physical systems has recently kindled significant attention, where many neural models have been developed. In particular, mesh-based graph neural networks (GNNs) have demonstrated significant potential in predicting spatiotemporal dynamics across arbitrary geometric domains. However, the existing node-edge message passing mechanism in GNNs limits the model's representation learning ability. In this paper, we proposed a cell-embedded GNN model (aka, CeGNN) to learn spatiotemporal dynamics with lifted performance.",False
data/iclr/data/submission_IpLXuqmP0C,0.0,0.0,0.0,540268,2386,52.02202224731445,,,False,,False,,,False,,False,"The main paper (main_paper.pdf) adheres to all specified formatting and layout guidelines. The main text (Abstract through Conclusion) is exactly 10 pages long, which is within the allowed limit of 6-10 pages. Line numbers are present on all pages. Visual inspection of margins, spacing, and font sizes (excluding allowed smaller fonts in references) shows no deviations from the ICLR 2025 style guide. No explicit Ethics or Reproducibility Statements were found in the main paper, and the Reproducibility Statement in the supplemental README.md is well under the 1-page limit.",False,"The main paper (main_paper.pdf) adheres to all specified formatting and layout guidelines. The main text (Abstract through Conclusion) is exactly 10 pages long, which is within the allowed limit of 6-10 pages. Line numbers are present on all pages. Visual inspection of margins, spacing, and font sizes (excluding allowed smaller fonts in references) shows no deviations from the ICLR 2025 style guide. No explicit Ethics or Reproducibility Statements were found in the main paper, and the Reproducibility Statement in the supplemental README.md is well under the 1-page limit.",False,False,"Instruction-based fine-tuning of large language models (LLMs) has achieved remarkable success in various natural language processing (NLP) tasks. Parameter-efficient fine-tuning (PEFT) methods, such as Mixture of LoRA Experts (MoLE), combine the efficiency of Low-Rank Adaptation (LORA) with the versatility of Mixture of Experts (MoE) models, demonstrating significant potential for handling multiple downstream tasks. However, the existing routing mechanisms for MoLE often involve a trade-off between computational efficiency and predictive accuracy, and they fail to fully address the diverse expert selection demands across different transformer layers. In this work, we propose DYNMOLE, a hybrid routing strategy that dynamically adjusts expert selection based on the Tsallis entropy of the router's probability distribution. This approach mitigates router uncertainty, enhances stability, and promotes more equitable expert participation, leading to faster convergence and improved model performance. Additionally, we introduce an auxiliary loss based on Tsallis entropy to further guide the model toward convergence with reduced uncertainty, thereby improving training stability and performance. Our extensive experiments on commonsense reasoning benchmarks demonstrate that DYNMOLE achieves substantial performance improvements, outperforming LORA by 9.6% and surpassing the state-of-the-art MoLE method, MoLA, by 2.3%. We also conduct a comprehensive ablation study to evaluate the contributions of DYNMOLE's key components.",False,"Instruction-based fine-tuning of large language models (LLMs) has achieved remarkable success in various natural language processing (NLP) tasks. Parameter-efficient fine-tuning (PEFT) methods, such as Mixture of LoRA Experts (MoLE), combine the efficiency of Low-Rank Adaptation (LORA) with the versatility of Mixture of Experts (MoE) models, demonstrating significant potential for handling multiple downstream tasks. However, the existing routing mechanisms for MoLE often involve a trade-off between computational efficiency and predictive accuracy, and they fail to fully address the diverse expert selection demands across different transformer layers. In this work, we propose DYNMOLE, a hybrid routing strategy that dynamically adjusts expert selection based on the Tsallis entropy of the router's probability distribution. This approach mitigates router uncertainty, enhances stability, and promotes more equitable expert participation, leading to faster convergence and improved model performance. Additionally, we introduce an auxiliary loss based on Tsallis entropy to further guide the model toward convergence with reduced uncertainty, thereby improving training stability and performance. Our extensive experiments on commonsense reasoning benchmarks demonstrate that DYNMOLE achieves substantial performance improvements, outperforming LORA by 9.6% and surpassing the state-of-the-art MoLE method, MoLA, by 2.3%. We also conduct a comprehensive ablation study to evaluate the contributions of DYNMOLE's key components.",False
data/iclr/data/submission_uiBLOcyTIA,1.0,1.0,0.7419168060451155,39265,1617,27.73890447616577,,,False,,False,,"SHUAI LIU, Ning Cao, Yile Chen, Yue Jiang, Gao Cong College of Computing and Data Science, Nanyang Technological University 50 Nanyang Avenue, Singapore, 639798 {SHUAI004@e, Ning.Cao@, yile001@e,yue013@e,gaocong@}.ntu.edu.sg",True,"SHUAI LIU, Ning Cao, Yile Chen, Yue Jiang, Gao Cong College of Computing and Data Science, Nanyang Technological University 50 Nanyang Avenue, Singapore, 639798 {SHUAI004@e, Ning.Cao@, yile001@e,yue013@e,gaocong@}.ntu.edu.sg",True,"The submitted PDF document, 'main_paper.pdf', does not display line numbers in the left margin on any page. The ICLR 2025 style guide (iclr2025_conference.pdf, page 1, and iclr2025_conference.tex comments) explicitly states that submissions should include LaTeX line numbers to facilitate reviewer feedback.",True,"The submitted PDF document, 'main_paper.pdf', does not display line numbers in the left margin on any page. The ICLR 2025 style guide (iclr2025_conference.pdf, page 1, and iclr2025_conference.tex comments) explicitly states that submissions should include LaTeX line numbers to facilitate reviewer feedback.",False,True,"Next location prediction is a critical task in human mobility analysis and serves as a foundation for various downstream applications. Existing methods typically rely on discrete IDs to represent locations, which inherently overlook spatial relationships and cannot generalize across cities. In this paper, we propose NextLocLLM, which leverages the advantages of large language models (LLMs) in processing natural language descriptions and their strong generalization capabilities for next location prediction.",False,"Next location prediction is a critical task in human mobility analysis and serves as a foundation for various downstream applications. Existing methods typically rely on discrete IDs to represent locations, which inherently overlook spatial relationships and cannot generalize across cities. In this paper, we propose NextLocLLM, which leverages the advantages of large language models (LLMs) in processing natural language descriptions and their strong generalization capabilities for next location prediction.",True
data/iclr/data/submission_3E9oXcX8SH,1.0,1.0,1.0,32175,1891,35.948219537734985,,,False,,False,,,False,,True,"The ICLR 2025 style guide (iclr2025_conference.pdf, page 1, line 049) states: 'The left margin is 1.5 inch (9 picas).' and (page 1, line 053) 'All pages should start at 1 inch (6 picas) from the top of the page.' However, the LaTeX source (iclr2025_conference.tex) used to generate the style guide and presumably the submission sets '\oddsidemargin{-0.5in}' and '\topmargin{-0.5in}', resulting in an actual left margin of 0.5 inches and a top margin of 0.5 inches. The submitted main_paper.pdf exhibits these incorrect margins.",True,"The ICLR 2025 style guide (iclr2025_conference.pdf, page 1, line 049) states: 'The left margin is 1.5 inch (9 picas).' and (page 1, line 053) 'All pages should start at 1 inch (6 picas) from the top of the page.' However, the LaTeX source (iclr2025_conference.tex) used to generate the style guide and presumably the submission sets '\oddsidemargin{-0.5in}' and '\topmargin{-0.5in}', resulting in an actual left margin of 0.5 inches and a top margin of 0.5 inches. The submitted main_paper.pdf exhibits these incorrect margins.",False,False,"The submission's abstract, introduction, and methodology clearly describe a problem of efficient biological data acquisition using active learning and inference set design, applied to image and molecular datasets. This involves representation learning and applications of deep learning, which are explicitly listed as in-scope topics for ICLR 2025.",False,"The submission's abstract, introduction, and methodology clearly describe a problem of efficient biological data acquisition using active learning and inference set design, applied to image and molecular datasets. This involves representation learning and applications of deep learning, which are explicitly listed as in-scope topics for ICLR 2025.",False
data/iclr/data/submission_bqLx5Rs8Tc,0.0,0.0,0.0,41388,1733,43.93629169464111,,,False,,False,,,False,,False,"The main content (Abstract through Conclusion) spans 10 pages, which is within the allowed 6-10 page limit. Line numbers are present on all pages. No explicit Ethics Statement or Reproducibility Statement sections were found. Visual inspection of the document's margins, font sizes, and spacing for the main body text appears consistent with the ICLR 2025 style guide, showing no obvious signs of 'space-cheating' or extensive use of small fonts outside of acceptable areas like tables or figures.",False,"The main content (Abstract through Conclusion) spans 10 pages, which is within the allowed 6-10 page limit. Line numbers are present on all pages. No explicit Ethics Statement or Reproducibility Statement sections were found. Visual inspection of the document's margins, font sizes, and spacing for the main body text appears consistent with the ICLR 2025 style guide, showing no obvious signs of 'space-cheating' or extensive use of small fonts outside of acceptable areas like tables or figures.",False,False,"The paper's abstract and introduction clearly state its focus on analyzing biases in Contrastive Language-Image Pre-training (CLIP) models and Vision-Language Models (VLMs) in multi-object scenarios. It discusses representation learning, image-text matching, and text-to-image generation, which are all within the scope of ICLR topics such as 'unsupervised, semi-supervised, and supervised representation learning' and 'applications of deep learning in vision, audio, speech, natural language processing'. The language throughout the abstract, introduction, and methodology sections is clear, coherent, and technically sound, allowing a domain expert to easily follow the main contributions, methodology, and results. There are no critically garbled sentences, unexplained notations that impede understanding, or fundamental internal contradictions.",False,"The paper's abstract and introduction clearly state its focus on analyzing biases in Contrastive Language-Image Pre-training (CLIP) models and Vision-Language Models (VLMs) in multi-object scenarios. It discusses representation learning, image-text matching, and text-to-image generation, which are all within the scope of ICLR topics such as 'unsupervised, semi-supervised, and supervised representation learning' and 'applications of deep learning in vision, audio, speech, natural language processing'. The language throughout the abstract, introduction, and methodology sections is clear, coherent, and technically sound, allowing a domain expert to easily follow the main contributions, methodology, and results. There are no critically garbled sentences, unexplained notations that impede understanding, or fundamental internal contradictions.",False
data/iclr/data/submission_8g7hHwSBjH,1.0,1.0,0.8901608366016681,37116,1743,20.63486361503601,,,False,,False,,supplemental_files/Supplementary Material/2025_ICLR_LIU_Ziwei_Invar_RAG_Appendix.pdf,True,supplemental_files/Supplementary Material/2025_ICLR_LIU_Ziwei_Invar_RAG_Appendix.pdf,False,"The main content (Abstract through Conclusion) spans 9 pages (from page 1 to page 9), which is within the 6-10 page limit specified in the ICLR 2025 style guide. Line numbers are present on all pages. No explicit Ethics Statement or Reproducibility Statement sections are found. Visual inspection of margins, font sizes, and spacing in the main body of the main_paper.pdf does not reveal any deviations from the ICLR 2025 style guide.",False,"The main content (Abstract through Conclusion) spans 9 pages (from page 1 to page 9), which is within the 6-10 page limit specified in the ICLR 2025 style guide. Line numbers are present on all pages. No explicit Ethics Statement or Reproducibility Statement sections are found. Visual inspection of margins, font sizes, and spacing in the main body of the main_paper.pdf does not reveal any deviations from the ICLR 2025 style guide.",False,True,"Retrieval-augmented generation (RAG) has shown its impressive capability of providing reliable answer predictions and addressing severe hallucination problems. A typical RAG implementation adopts powerful retrieval models to extract external information and leverage large language models (LLMs) to generate corresponding answers. ... we propose a novel two-stage fine-tuning architecture called Invar-RAG. In the retrieval stage, a LLM-based retriever is constructed by integrating a LoRA-based representation learning to address the feature locality problem. ... Moreover, in the generation stage, a meticulously designed fine-tuning method is devised to improve our LLM for accurate answer generation based on the retrieved information.",False,"Retrieval-augmented generation (RAG) has shown its impressive capability of providing reliable answer predictions and addressing severe hallucination problems. A typical RAG implementation adopts powerful retrieval models to extract external information and leverage large language models (LLMs) to generate corresponding answers. ... we propose a novel two-stage fine-tuning architecture called Invar-RAG. In the retrieval stage, a LLM-based retriever is constructed by integrating a LoRA-based representation learning to address the feature locality problem. ... Moreover, in the generation stage, a meticulously designed fine-tuning method is devised to improve our LLM for accurate answer generation based on the retrieved information.",False
data/iclr/data/submission_xIW2WtCuYE,0.0,1.0,0.0,13382,1463,41.720343828201294,,,False,,False,,"The document is a legal contract template in Hebrew, containing placeholders for names and addresses (e.g., ""מרחוב:"", ""ת.ז."", ""המשכירה"", ""שוכר המשנה"") and a template provider's logo (""zap משפטי"").",False,"The document is a legal contract template in Hebrew, containing placeholders for names and addresses (e.g., ""מרחוב:"", ""ת.ז."", ""המשכירה"", ""שוכר המשנה"") and a template provider's logo (""zap משפטי"").",True,"The provided document (main_paper.pdf) is 2 pages long. It is a legal contract in Hebrew, not an academic paper, and does not contain an abstract, introduction, or conclusion section as typically defined for main text.",True,"The provided document (main_paper.pdf) is 2 pages long. It is a legal contract in Hebrew, not an academic paper, and does not contain an abstract, introduction, or conclusion section as typically defined for main text.",True,False,הסכם שכירות משנה בלתי מוגנת (Unprotected Sublease Agreement),True,הסכם שכירות משנה בלתי מוגנת (Unprotected Sublease Agreement),False
data/iclr/data/submission_9pDbWbJzC4,1.0,1.0,1.0,28026,1830,47.711158752441406,,,False,,False,,,False,,False,"The main content of the paper (Abstract through Conclusion) spans 10 pages, which is within the allowed limit of 6-10 pages. Line numbers are present on all pages. Margins, font sizes, and spacing appear to adhere to the ICLR 2025 style guide specifications, with no evidence of 'space-cheating' or extensive use of smaller fonts in the main body. No explicit Ethics Statement or Reproducibility Statement sections were found.",False,"The main content of the paper (Abstract through Conclusion) spans 10 pages, which is within the allowed limit of 6-10 pages. Line numbers are present on all pages. Margins, font sizes, and spacing appear to adhere to the ICLR 2025 style guide specifications, with no evidence of 'space-cheating' or extensive use of smaller fonts in the main body. No explicit Ethics Statement or Reproducibility Statement sections were found.",False,False,"Phosphorylation site prediction based on kinase-substrate interaction plays a vital role in understanding cellular signaling pathways and disease mechanisms. Computational methods for this task can be categorized into kinase-family-focused and individual kinase-targeted approaches. ... we introduce SAGEPhos (Structure-aware kinAse-substrate bio-coupled and bio-auGmented nEtwork for Phosphorylation site prediction), a novel framework that modifies the semantic space of main protein inputs using auxiliary inputs at two distinct modality levels. ... The field has increasingly turned to artificial intelligence techniques, particularly deep learning models.",False,"Phosphorylation site prediction based on kinase-substrate interaction plays a vital role in understanding cellular signaling pathways and disease mechanisms. Computational methods for this task can be categorized into kinase-family-focused and individual kinase-targeted approaches. ... we introduce SAGEPhos (Structure-aware kinAse-substrate bio-coupled and bio-auGmented nEtwork for Phosphorylation site prediction), a novel framework that modifies the semantic space of main protein inputs using auxiliary inputs at two distinct modality levels. ... The field has increasingly turned to artificial intelligence techniques, particularly deep learning models.",False
data/iclr/data/submission_eJVrwDE086,1.0,1.0,0.7013024377508044,28030,1862,25.55076146125793,,,False,,False,,The work was done while Razvan-Gabriel Dumitru interned at ServiceNow.,True,The work was done while Razvan-Gabriel Dumitru interned at ServiceNow.,False,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 10, inclusive, which is 10 pages. The ICLR 2025 style guide specifies a main text limit of 6 to 10 pages. All pages include line numbers in the left margin. Visual inspection of the document, including the abstract, body text, and title, indicates adherence to the specified font sizes, vertical spacing, paragraph separation, and margin requirements as outlined in the ICLR 2025 style guide. There is no evidence of extensive use of smaller fonts or compressed spacing in the main body text.",False,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 10, inclusive, which is 10 pages. The ICLR 2025 style guide specifies a main text limit of 6 to 10 pages. All pages include line numbers in the left margin. Visual inspection of the document, including the abstract, body text, and title, indicates adherence to the specified font sizes, vertical spacing, paragraph separation, and margin requirements as outlined in the ICLR 2025 style guide. There is no evidence of extensive use of smaller fonts or compressed spacing in the main body text.",False,True,The paper introduces a meta quantization approach for Large Language Models (LLMs) that quantizes different layers at varying bit precisions based on their importance. It proposes two importance scoring methods (Layer Input Modification and Z-score Distribution) and evaluates their effectiveness in reducing model size while retaining performance. The work also compares layer-wise quantization with pruning and discusses its applicability to different quantization techniques and LLM sizes. This clearly falls under 'optimization for representation learning' and 'applications of deep learning in natural language processing' within the ICLR scope.,False,The paper introduces a meta quantization approach for Large Language Models (LLMs) that quantizes different layers at varying bit precisions based on their importance. It proposes two importance scoring methods (Layer Input Modification and Z-score Distribution) and evaluates their effectiveness in reducing model size while retaining performance. The work also compares layer-wise quantization with pruning and discusses its applicability to different quantization techniques and LLM sizes. This clearly falls under 'optimization for representation learning' and 'applications of deep learning in natural language processing' within the ICLR scope.,False
