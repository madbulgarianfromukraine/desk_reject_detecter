directory_name,category_match,status_match,similarity_score,total_input_tokens,total_output_tokens,total_elapsed_time,error_status,error_message,formatting_check_result_1,formatting_check_evidence_snippet_1,formatting_check_result_2,formatting_check_evidence_snippet_2,formatting_check_result_3,formatting_check_evidence_snippet_3,formatting_check_result_best,formatting_check_evidence_snippet_best,anonymity_check_result_1,anonymity_check_evidence_snippet_1,anonymity_check_result_2,anonymity_check_evidence_snippet_2,anonymity_check_result_3,anonymity_check_evidence_snippet_3,anonymity_check_result_best,anonymity_check_evidence_snippet_best,policy_check_result_1,policy_check_evidence_snippet_1,policy_check_result_2,policy_check_evidence_snippet_2,policy_check_result_3,policy_check_evidence_snippet_3,policy_check_result_best,policy_check_evidence_snippet_best,scope_check_result_1,scope_check_evidence_snippet_1,scope_check_result_2,scope_check_evidence_snippet_2,scope_check_result_3,scope_check_evidence_snippet_3,scope_check_result_best,scope_check_evidence_snippet_best,token_error_occurred
data/iclr/data/submission_eJVrwDE086,1.0,1.0,0.7013024377508044,50754,1953,66.66050887107849,,,False,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 10, inclusive, which is exactly 10 pages. This falls within the allowed page limit of 6-10 pages. All pages include line numbers in the left margin. Visual inspection of the document's margins, font sizes, and spacing for main body text, headings, figures, and tables indicates adherence to the ICLR 2025 style guide specifications. No explicit Ethics Statement or Reproducibility Statement sections were found to assess against a page limit.",False,"The main content (Abstract through Conclusion) spans exactly 10 pages, which is within the allowed limit of 6-10 pages. Line numbers are present on all pages. Visual inspection of margins, font sizes, and spacing for the main body text and headings indicates adherence to the ICLR 2025 style guide. No extensive use of smaller fonts or reduced spacing was detected outside of acceptable areas like figure captions, footnotes, or the references section. No Ethics Statement or Reproducibility Statement sections were found to evaluate against a page limit.",False,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 10, which is exactly 10 pages. Line numbers are present on all pages. Visual inspection of margins, font sizes, and spacing throughout the document (including headings, abstract, figures, and tables) indicates adherence to the ICLR 2025 style guide. No explicit Ethics Statement or Reproducibility Statement sections were found.",False,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 10, which is exactly 10 pages. Line numbers are present on all pages. Visual inspection of margins, font sizes, and spacing throughout the document (including headings, abstract, figures, and tables) indicates adherence to the ICLR 2025 style guide. No explicit Ethics Statement or Reproducibility Statement sections were found.",True,The work was done while Razvan-Gabriel Dumitru interned at ServiceNow.,,,,,True,The work was done while Razvan-Gabriel Dumitru interned at ServiceNow.,False,,,,,,False,,False,,,,,,False,,False
data/iclr/data/submission_zebxJxSHcD,1.0,1.0,1.0,83057,2247,71.28948760032654,,,False,"The main paper (main_paper.pdf) adheres to all specified formatting and layout guidelines. Line numbers are present on all pages. The main content (Abstract through Conclusion/Discussion) spans 10 pages, which is within the 6-10 page limit. There are no Ethics or Reproducibility Statements exceeding 1 page. Visual inspection confirms correct margins, font sizes, paragraph spacing, and heading styles as per the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of small fonts in the main body text.",False,"The document adheres to all specified formatting and layout guidelines. The main content (Abstract through Conclusion) is exactly 10 pages, which falls within the 6-10 page limit. Line numbers are present on all pages. There are no explicit Ethics or Reproducibility statements that exceed page limits. Margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide, and no 'space-cheating' techniques (like extensive use of \small or \footnotesize for main text, or \vspace abuse) were detected in the provided PDF or LaTeX source.",False,"The main content of the paper (Abstract through Conclusion/Discussion) spans exactly 10 pages, which is within the allowed limit of 6 to 10 pages. Line numbers are present on all pages. Visual inspection of the document indicates that font sizes, line spacing, and margins adhere to the ICLR 2025 style guide specifications, with no apparent 'space-cheating' or extensive use of small fonts outside of acceptable areas like figures, tables, or footnotes. No explicit Ethics Statement or Reproducibility Statement sections were found to evaluate against a page limit.",False,"The main content of the paper (Abstract through Conclusion/Discussion) spans exactly 10 pages, which is within the allowed limit of 6 to 10 pages. Line numbers are present on all pages. Visual inspection of the document indicates that font sizes, line spacing, and margins adhere to the ICLR 2025 style guide specifications, with no apparent 'space-cheating' or extensive use of small fonts outside of acceptable areas like figures, tables, or footnotes. No explicit Ethics Statement or Reproducibility Statement sections were found to evaluate against a page limit.",False,,False,,,,False,,False,,,,,,False,,False,,,,,,False,,False
data/iclr/data/submission_GCSr6QCCwF,1.0,1.0,0.8773363024350193,39936,2053,58.86217331886292,,,False,,,,,,False,,False,,False,,,,False,,False,"The paper proposes MIMOUDIFF: A UNIFIED MULTI-SOURCE DATA FUSION FRAMEWORK VIA MIMO UNET AND REFINED DIFFUSION FOR PRECIPITATION NOWCASTING. It leverages deep learning techniques, including RNN-based, CNN-based, ConvRNN-based models, diffusion models, attention-guided multi-source fusion, and a spatio-temporal sequence prediction module with an RST-Unet architecture for precipitation nowcasting.",,,,,False,"The paper proposes MIMOUDIFF: A UNIFIED MULTI-SOURCE DATA FUSION FRAMEWORK VIA MIMO UNET AND REFINED DIFFUSION FOR PRECIPITATION NOWCASTING. It leverages deep learning techniques, including RNN-based, CNN-based, ConvRNN-based models, diffusion models, attention-guided multi-source fusion, and a spatio-temporal sequence prediction module with an RST-Unet architecture for precipitation nowcasting.",True,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 11. The 'Abstract' begins on page 1, and the '5 CONCLUSION' section ends on page 11, immediately followed by 'REFERENCES'.",True,"The main content of the paper (Abstract through Conclusion) spans 11 pages (pages 1-11). The '5 CONCLUSION' section ends on page 11, line 554, and the 'REFERENCES' section begins on page 11, line 555. The ICLR 2025 style guide (iclr2025_conference.pdf, page 2, line 060) specifies a strict upper limit of 10 pages for the main text.",,,True,"The main content of the paper (Abstract through Conclusion) spans 11 pages (pages 1-11). The '5 CONCLUSION' section ends on page 11, line 554, and the 'REFERENCES' section begins on page 11, line 555. The ICLR 2025 style guide (iclr2025_conference.pdf, page 2, line 060) specifies a strict upper limit of 10 pages for the main text.",True
data/iclr/data/submission_UpPXWd9SBk,0.0,0.0,0.0,45200,3335,94.22925591468812,,,False,,,,,,False,,False,,,,,,False,,False,"The detection of foreground targets on airport surface is the foundation of airport surveillance applications. However, effective algorithms and specialized benchmarks are still lacking in this area. Based on this fact, we propose an Airport Foreground Target Detection dataset (AFTD), which contains the three most important foreground targets moving on the airport surface: airplane, vehicle, and person. Through self collection and collection of web images, we have obtained a total of over 200000 images and filtered out 10050 images based on diversity principles to form the AFTD dataset, which includes a total of 26968 airplane instances, 24759 vehicle instances, and 5064 person instances. AFTD includes a variety of changes of these targets, such as super multi-scale, multi-level occlusion and viewangle changes, etc. In addition, we further illustrate the challenges posed by AFTD to existing algorithms through statistical analysis and detailed experiments, and discuss how to solve these challenges in the airport surveillance scenario.",,,,,False,"The detection of foreground targets on airport surface is the foundation of airport surveillance applications. However, effective algorithms and specialized benchmarks are still lacking in this area. Based on this fact, we propose an Airport Foreground Target Detection dataset (AFTD), which contains the three most important foreground targets moving on the airport surface: airplane, vehicle, and person. Through self collection and collection of web images, we have obtained a total of over 200000 images and filtered out 10050 images based on diversity principles to form the AFTD dataset, which includes a total of 26968 airplane instances, 24759 vehicle instances, and 5064 person instances. AFTD includes a variety of changes of these targets, such as super multi-scale, multi-level occlusion and viewangle changes, etc. In addition, we further illustrate the challenges posed by AFTD to existing algorithms through statistical analysis and detailed experiments, and discuss how to solve these challenges in the airport surveillance scenario.",False,"The main_paper.pdf submission adheres to all specified formatting and layout guidelines. No violations were detected regarding page limits, statement limits, line numbers, or margins/spacing.",False,"The document, which serves as the ICLR 2025 style guide, adheres to its own formatting requirements. The main content (Abstract through Conclusion) spans 10 pages (pages 1-10), which is within the 6-10 page limit. Line numbers are present on all pages. There are no explicit Ethics or Reproducibility statements. Visual inspection confirms that margins, font sizes, and paragraph spacing are consistent with the specifications detailed within the document itself, indicating no 'space-cheating' or margin abuse.",False,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 10, which is exactly 10 pages. This falls within the allowed page limit of 6-10 pages. Line numbers are present in the left margin on all pages. No explicit Ethics or Reproducibility Statements were found. Visual inspection of the document's margins, font sizes, and paragraph spacing (e.g., 10pt type with 11pt vertical spacing, 1/2 line space between paragraphs, no indentation) indicates adherence to the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of smaller fonts in the main body.",False,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 10, which is exactly 10 pages. This falls within the allowed page limit of 6-10 pages. Line numbers are present in the left margin on all pages. No explicit Ethics or Reproducibility Statements were found. Visual inspection of the document's margins, font sizes, and paragraph spacing (e.g., 10pt type with 11pt vertical spacing, 1/2 line space between paragraphs, no indentation) indicates adherence to the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of smaller fonts in the main body.",False
data/iclr/data/submission_PAzVN4EEkj,1.0,1.0,0.7922669980546885,1436961,2261,219.9200074672699,,,False,,,,,,False,,True,"Feihan Lit, Yifan Sunt, Weiye Zhao†, Rui Chen, Tianhao Wei & Changliu Liu * Robotics Institute Carnegie Mellon University Pittsburgh, PA 15213, USA {feihanl, yifansu2, weiyezha, ruic3, twei2, cliu6}@andrew.cmu.edu",,,,,True,"Feihan Lit, Yifan Sunt, Weiye Zhao†, Rui Chen, Tianhao Wei & Changliu Liu * Robotics Institute Carnegie Mellon University Pittsburgh, PA 15213, USA {feihanl, yifansu2, weiyezha, ruic3, twei2, cliu6}@andrew.cmu.edu",False,,False,,False,"Deep reinforcement learning (RL) excels in various control tasks, yet the absence of safety guarantees hampers its real-world applicability. In particular, explorations during learning usually results in safety violations, while the RL agent learns from those mistakes. On the other hand, safe control techniques ensure persistent safety satisfaction but demand strong priors on system dynamics, which is usually hard to obtain in practice. To address these problems, we present Safe Set Guided State-wise Constrained Policy Optimization (S-3PO), a pioneering algorithm generating state-wise safe optimal policies with zero training violations, i.e., learning without mistakes. S-3PO first employs a safety-oriented monitor with black-box dynamics to ensure safe exploration. It then enforces an ""imaginary"" cost for the RL agent to converge to optimal behaviors within safety constraints. S-3PO outperforms existing methods in high-dimensional robotics tasks, managing state-wise constraints with zero training violation. This innovation marks a significant stride towards real-world safe RL deployment.",False,"Deep reinforcement learning (RL) excels in various control tasks, yet the absence of safety guarantees hampers its real-world applicability. In particular, explorations during learning usually results in safety violations, while the RL agent learns from those mistakes. On the other hand, safe control techniques ensure persistent safety satisfaction but demand strong priors on system dynamics, which is usually hard to obtain in practice. To address these problems, we present Safe Set Guided State-wise Constrained Policy Optimization (S-3PO), a pioneering algorithm generating state-wise safe optimal policies with zero training violations, i.e., learning without mistakes. S-3PO first employs a safety-oriented monitor with black-box dynamics to ensure safe exploration. It then enforces an ""imaginary"" cost for the RL agent to converge to optimal behaviors within safety constraints. S-3PO outperforms existing methods in high-dimensional robotics tasks, managing state-wise constraints with zero training violation. This innovation marks a significant stride towards real-world safe RL deployment.",False,"The main paper (main_paper.pdf) consists of 10 pages of main content (Abstract through Conclusion and Future Prospectus), followed by References starting on page 10. The ICLR 2025 style guide specifies a strict upper limit of 10 pages for the main text. Line numbers are present on all pages in the left margin. Margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of smaller fonts in the main body text. Optional sections like Ethics Statements or Reproducibility Statements are not explicitly present in the main paper.",,,,,False,"The main paper (main_paper.pdf) consists of 10 pages of main content (Abstract through Conclusion and Future Prospectus), followed by References starting on page 10. The ICLR 2025 style guide specifies a strict upper limit of 10 pages for the main text. Line numbers are present on all pages in the left margin. Margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of smaller fonts in the main body text. Optional sections like Ethics Statements or Reproducibility Statements are not explicitly present in the main paper.",True
data/iclr/data/submission_bqLx5Rs8Tc,0.0,0.0,0.0,72333,2335,86.62782382965088,,,False,,,,,,False,,False,,,,,,False,,False,,False,,False,,False,,False,"The document adheres to all specified formatting and layout guidelines. Line numbers are present on every page. The main content (Abstract through Conclusion) spans 10 pages, which is within the 6-10 page limit. There are no explicit Ethics or Reproducibility Statements exceeding one page. Margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of smaller fonts in the main body text.",False,"The main content (Abstract through Conclusion) spans from page 1 to page 10, which is exactly 10 pages, adhering to the 6-10 page limit. Line numbers are present on all pages. Visual inspection of margins, font sizes, and spacing indicates adherence to the ICLR 2025 style guide, consistent with the provided 'iclr2025_conference.pdf' as an example of correct formatting. No explicit Ethics or Reproducibility statements exceeding one page were found.",,,False,"The main content (Abstract through Conclusion) spans from page 1 to page 10, which is exactly 10 pages, adhering to the 6-10 page limit. Line numbers are present on all pages. Visual inspection of margins, font sizes, and spacing indicates adherence to the ICLR 2025 style guide, consistent with the provided 'iclr2025_conference.pdf' as an example of correct formatting. No explicit Ethics or Reproducibility statements exceeding one page were found.",False
data/iclr/data/submission_GE0UKtI6Lf,0.0,0.0,0.0,659647,2285,212.0872614383697,,,False,,,,,,False,,False,,,,,,False,,False,"The paper is titled ""WALL-E: WORLD ALIGNMENT BY RULE LEARNING IMPROVES WORLD MODEL-BASED LLM AGENTS"". The abstract states: ""Can large language models (LLMs) directly serve as powerful world models for model-based agents? While the gaps between the prior knowledge of LLMs and the specified environment's dynamics do exist, our study reveals that the gaps can be bridged by aligning an LLM with its deployed environment and such “world alignment” can be efficiently achieved by rule learning on LLMs."" It proposes a ""neurosymbolic approach to learn these rules gradient-free through LLMs, by inducing, updating, and pruning rules based on comparisons of agent-explored trajectories and world model predictions."" It mentions ""embodied LLM agent “WALL-E"" is built upon model-predictive control (MPC)."" It also discusses ""applications of deep learning in vision, audio, speech, natural language processing, robotics, neuroscience, or any other field"" specifically mentioning ""open-world challenges in Minecraft and ALFWorld"". The abstract, introduction, and methodology sections are written in clear, academic English with appropriate technical terminology and standard mathematical notation.",,,,,False,"The paper is titled ""WALL-E: WORLD ALIGNMENT BY RULE LEARNING IMPROVES WORLD MODEL-BASED LLM AGENTS"". The abstract states: ""Can large language models (LLMs) directly serve as powerful world models for model-based agents? While the gaps between the prior knowledge of LLMs and the specified environment's dynamics do exist, our study reveals that the gaps can be bridged by aligning an LLM with its deployed environment and such “world alignment” can be efficiently achieved by rule learning on LLMs."" It proposes a ""neurosymbolic approach to learn these rules gradient-free through LLMs, by inducing, updating, and pruning rules based on comparisons of agent-explored trajectories and world model predictions."" It mentions ""embodied LLM agent “WALL-E"" is built upon model-predictive control (MPC)."" It also discusses ""applications of deep learning in vision, audio, speech, natural language processing, robotics, neuroscience, or any other field"" specifically mentioning ""open-world challenges in Minecraft and ALFWorld"". The abstract, introduction, and methodology sections are written in clear, academic English with appropriate technical terminology and standard mathematical notation.",False,"The provided document is the ICLR 2025 formatting instructions (iclr2025_conference.pdf) itself, not a research paper submission. It serves as the style guide. The main content of this document (from 'ABSTRACT' to '7 PREPARING POSTSCRIPT OR PDF FILES') spans 6 pages, which is within the specified 6-10 page limit for main text. Line numbers are present in the left margin on all pages. There are no Ethics or Reproducibility Statements to evaluate against a page limit. The document's layout, including margins, font sizes, and spacing for headings, body text, figures, and tables, appears to conform to the rules it describes within its own text (e.g., page 1, lines 048-053; page 2, lines 065-075). No evidence of 'space-cheating' or extensive use of small fonts for main body text was found.",,,,,False,"The provided document is the ICLR 2025 formatting instructions (iclr2025_conference.pdf) itself, not a research paper submission. It serves as the style guide. The main content of this document (from 'ABSTRACT' to '7 PREPARING POSTSCRIPT OR PDF FILES') spans 6 pages, which is within the specified 6-10 page limit for main text. Line numbers are present in the left margin on all pages. There are no Ethics or Reproducibility Statements to evaluate against a page limit. The document's layout, including margins, font sizes, and spacing for headings, body text, figures, and tables, appears to conform to the rules it describes within its own text (e.g., page 1, lines 048-053; page 2, lines 065-075). No evidence of 'space-cheating' or extensive use of small fonts for main body text was found.",False
data/iclr/data/submission_9d6RcViazd,0.0,0.0,0.0,52011,2664,65.2786180973053,,,False,,,,,,False,,False,,,,,,False,,False,"The submission focuses on ""robust language modeling method for text-to-speech (TTS) synthesis"" using ""chain-of-thought (CoT) prompting"" and ""codec language models."" It involves ""representation learning"" for ""speech tokens"" and applies ""deep learning"" techniques (Transformers, LMs) to ""audio"" and ""speech"" processing. This aligns directly with ICLR's scope for applications of deep learning in audio/speech/NLP and representation learning.",False,"We present RALL-E, a robust language modeling method for text-to-speech (TTS) synthesis. While previous codec language modeling methods have demonstrated impressive performance in zero-shot TTS, they often struggle with robustness issues... RALL-E addresses these issues through chain-of-thought (CoT) prompting, which breaks the task into simpler steps to improve the stability of TTS. First, RALL-E predicts prosody tokens (pitch and duration) from the input text and uses them as intermediate conditions to guide the prediction of speech tokens in a CoT manner. Second, RALL-E utilizes the predicted duration prompt to guide the computing of self-attention weights in Transformer, enforcing the model to focus on the corresponding phonemes and prosody tokens during speech token prediction.",,,False,"We present RALL-E, a robust language modeling method for text-to-speech (TTS) synthesis. While previous codec language modeling methods have demonstrated impressive performance in zero-shot TTS, they often struggle with robustness issues... RALL-E addresses these issues through chain-of-thought (CoT) prompting, which breaks the task into simpler steps to improve the stability of TTS. First, RALL-E predicts prosody tokens (pitch and duration) from the input text and uses them as intermediate conditions to guide the prediction of speech tokens in a CoT manner. Second, RALL-E utilizes the predicted duration prompt to guide the computing of self-attention weights in Transformer, enforcing the model to focus on the corresponding phonemes and prosody tokens during speech token prediction.",False,"The main content (Abstract through Conclusions) spans 9 pages (pages 1-9), which is within the allowed 6-10 page limit. Line numbers are present on all pages in the left margin. There are no explicit Ethics or Reproducibility Statements. Visual inspection of the document's margins, font sizes, and spacing (including headings, abstract, and tables) indicates adherence to the ICLR 2025 style guide specifications, such as 10pt type with 11pt vertical spacing for main text, 1.5 inch left margin, and 1 inch top margin.",False,"The main content (Abstract through Conclusions) spans 9 pages, which is within the 6-10 page limit. Line numbers are present on all pages. Margins, font sizes, and paragraph spacing appear consistent with the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of small fonts in the main body. No separate Ethics or Reproducibility Statements exceeding one page were found.",False,"The main content (Abstract through Conclusions) spans 9 pages (pages 1-9), which is within the 6-10 page limit. Line numbers are present on all pages. Visual inspection of margins, font sizes, and spacing for main text, headings, figures, and tables indicates adherence to the ICLR 2025 style guide. There are no explicit Ethics or Reproducibility statements exceeding one page.",False,"The main content (Abstract through Conclusions) spans 9 pages (pages 1-9), which is within the 6-10 page limit. Line numbers are present on all pages. Visual inspection of margins, font sizes, and spacing for main text, headings, figures, and tables indicates adherence to the ICLR 2025 style guide. There are no explicit Ethics or Reproducibility statements exceeding one page.",False
data/iclr/data/submission_GjSWkSaNTQ,1.0,1.0,1.0,76499,2328,165.04098677635193,,,False,,False,,False,,False,,False,,,,,,False,,False,,False,"The paper introduces CLAVER, a Contrastive Language-Action Video Learner, which adapts CLIP for video action recognition using a novel Kronecker mask attention for temporal modeling and interpretive prompts generated by large language models for verb comprehension. This falls under representation learning and applications of deep learning in vision and natural language processing.",False,,False,"The paper introduces CLAVER, a Contrastive Language-Action Video Learner, which adapts CLIP for video action recognition using a novel Kronecker mask attention for temporal modeling and interpretive prompts generated by large language models for verb comprehension. This falls under representation learning and applications of deep learning in vision and natural language processing.",False,"The main content (Abstract through Conclusion) spans pages 1-10, which is exactly 10 pages. This falls within the allowed page limit of 6-10 pages for the main text. Line numbers are present on all pages. Margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of small fonts in the main body. Optional sections like 'Author Contributions' and 'Acknowledgments' are brief and do not exceed page limits for statements.",,,,,False,"The main content (Abstract through Conclusion) spans pages 1-10, which is exactly 10 pages. This falls within the allowed page limit of 6-10 pages for the main text. Line numbers are present on all pages. Margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of small fonts in the main body. Optional sections like 'Author Contributions' and 'Acknowledgments' are brief and do not exceed page limits for statements.",False
data/iclr/data/submission_ARhJbaAjyE,0.0,0.0,0.0,4003774,2872,248.76794028282168,,,True,"pku_file_path=""/workspace/work/aigc-user-workspace/zhaoweixiang/LLM_Unlearning/datasets/train/BeaverTails_100_14/train.json""",,,,,True,"pku_file_path=""/workspace/work/aigc-user-workspace/zhaoweixiang/LLM_Unlearning/datasets/train/BeaverTails_100_14/train.json""",True,"{ ""input_en"": ""Give three tips for staying healthy."", ""output"": """", ""input_zh"": ""给出三个保持健康的秘诀.\n"" }",,,,,True,"{ ""input_en"": ""Give three tips for staying healthy."", ""output"": """", ""input_zh"": ""给出三个保持健康的秘诀.\n"" }",False,"The submission's abstract, introduction, and methodology are clearly written and technically sound. For example, the abstract states: ""we propose LENS, a novel approach to enhance multilingual capabilities of LLMs by leveraging their internal language representation spaces. Specially, LENS operates by manipulating the hidden representations within the language-agnostic and language-specific subspaces from top layers of LLMs."" The introduction further elaborates on this, and the methodology section details the use of SVD for subspace probing and specific loss functions (L1, L2, L3) for manipulation.",False,"Despite the growing global demand for large language models (LLMs) that serve users from diverse linguistic backgrounds, most cutting-edge LLMs remain predominantly English-centric. This creates a performance gap across languages, restricting access to advanced AI services for non-English speakers. Current methods to enhance multilingual capabilities largely rely on data-driven post-training techniques, such as multilingual instruction tuning or continual pre-training. However, these approaches encounter significant challenges, including the scarcity of high-quality multilingual datasets and the limited enhancement of multilingual capabilities. They often suffer from off-target issues and catastrophic forgetting of central language abilities. To this end, we propose LENS, a novel approach to enhance multilingual capabilities of LLMs by leveraging their internal language representation spaces. Specially, LENS operates by manipulating the hidden representations within the language-agnostic and language-specific subspaces from top layers of LLMs.",,,False,"Despite the growing global demand for large language models (LLMs) that serve users from diverse linguistic backgrounds, most cutting-edge LLMs remain predominantly English-centric. This creates a performance gap across languages, restricting access to advanced AI services for non-English speakers. Current methods to enhance multilingual capabilities largely rely on data-driven post-training techniques, such as multilingual instruction tuning or continual pre-training. However, these approaches encounter significant challenges, including the scarcity of high-quality multilingual datasets and the limited enhancement of multilingual capabilities. They often suffer from off-target issues and catastrophic forgetting of central language abilities. To this end, we propose LENS, a novel approach to enhance multilingual capabilities of LLMs by leveraging their internal language representation spaces. Specially, LENS operates by manipulating the hidden representations within the language-agnostic and language-specific subspaces from top layers of LLMs.",False,"No violations were found in the provided main_paper.pdf based on the ICLR 2025 style guide. The main text adheres to the page limit, line numbers are present, and margins/spacing/font sizes appear consistent with the guidelines, with acceptable smaller fonts used for figures, tables, and references.",False,"The document adheres to all specified formatting and layout guidelines, including page limits, statement limits, margins, spacing, and line numbering. Main content is 10 pages, within the 6-10 page limit. Line numbers are present. Margins and spacing appear standard. No extensive use of small fonts for main body text or narrowed margins was observed. No ethics or reproducibility statements exceeded one page.",False,"No specific violations were found regarding page limits, statement limits, margins/spacing, or line numbers based on the provided ICLR 2025 style guide and the submission's main paper.",False,"The document adheres to all specified formatting and layout guidelines, including page limits, statement limits, margins, spacing, and line numbering. Main content is 10 pages, within the 6-10 page limit. Line numbers are present. Margins and spacing appear standard. No extensive use of small fonts for main body text or narrowed margins was observed. No ethics or reproducibility statements exceeded one page.",False
data/iclr/data/submission_IpLXuqmP0C,0.0,0.0,0.0,805816,2309,71.55489683151245,,,False,"The document adheres to all specified formatting and layout guidelines. Line numbers are present, the main text is within the 10-page limit, and there are no indications of margin or spacing abuse. The optional Ethics and Reproducibility statements are not present in the main paper, and the supplemental README.md's reproducibility statement is concise.",,,,,False,"The document adheres to all specified formatting and layout guidelines. Line numbers are present, the main text is within the 10-page limit, and there are no indications of margin or spacing abuse. The optional Ethics and Reproducibility statements are not present in the main paper, and the supplemental README.md's reproducibility statement is concise.",False,"The paper proposes DYNMOLE, a hybrid routing strategy that dynamically adjusts expert selection based on the Tsallis entropy of the router's probability distribution. This approach mitigates router uncertainty, enhances stability, and promotes more equitable expert participation, leading to faster convergence and improved model performance. Additionally, we introduce an auxiliary loss based on Tsallis entropy to further guide the model toward convergence with reduced uncertainty, thereby improving training stability and performance. Our extensive experiments on commonsense reasoning benchmarks demonstrate that DYNMOLE achieves substantial performance improvements, outperforming LORA by 9.6% and surpassing the state-of-the-art MoLE method, MoLA, by 2.3%.",False,"Instruction-based fine-tuning of large language models (LLMs) has achieved remarkable success in various natural language processing (NLP) tasks. Parameter-efficient fine-tuning (PEFT) methods, such as Mixture of LoRA Experts (MoLE), combine the efficiency of Low-Rank Adaptation (LORA) with the versatility of Mixture of Experts (MoE) models, demonstrating significant potential for handling multiple downstream tasks. However, the existing routing mechanisms for MoLE often involve a trade-off between computational efficiency and predictive accuracy, and they fail to fully address the diverse expert selection demands across different transformer layers. In this work, we propose DYNMOLE, a hybrid routing strategy that dynamically adjusts expert selection based on the Tsallis entropy of the router's probability distribution.",,,False,"Instruction-based fine-tuning of large language models (LLMs) has achieved remarkable success in various natural language processing (NLP) tasks. Parameter-efficient fine-tuning (PEFT) methods, such as Mixture of LoRA Experts (MoLE), combine the efficiency of Low-Rank Adaptation (LORA) with the versatility of Mixture of Experts (MoE) models, demonstrating significant potential for handling multiple downstream tasks. However, the existing routing mechanisms for MoLE often involve a trade-off between computational efficiency and predictive accuracy, and they fail to fully address the diverse expert selection demands across different transformer layers. In this work, we propose DYNMOLE, a hybrid routing strategy that dynamically adjusts expert selection based on the Tsallis entropy of the router's probability distribution.",False,,False,,,,False,,False,,,,,,False,,False
data/iclr/data/submission_seilwMD9m2,0.0,0.0,0.0,45069,2100,57.681575298309326,,,True,"The main content of the paper, from the Abstract (page 1) through Section 6 'FINAL REMARKS' (page 11), spans 11 pages. The ICLR 2025 style guide states a strict upper limit of 10 pages for the main text.",,,,,True,"The main content of the paper, from the Abstract (page 1) through Section 6 'FINAL REMARKS' (page 11), spans 11 pages. The ICLR 2025 style guide states a strict upper limit of 10 pages for the main text.",False,,False,"The paper investigates the performance of Aya, a multilingual generative language model, in various Natural Language Processing (NLP) tasks such as Aspect-Based Sentiment Analysis, Hate Speech Detection, Irony Detection, and Question-Answering, particularly focusing on low-resource languages like Brazilian Portuguese. The methodology involves a few-shot learning approach. This falls under 'applications of deep learning in... natural language processing' and 'representation learning' which are explicitly in-scope for ICLR.",False,"This study aims to assess Aya's performance in tasks such as Aspect-Based Sentiment Analysis, Hate Speech Detection, Irony Detection, and Question-Answering. Our methodology consists of utilizing a few-shot learning approach, incorporating examples from the ABSAPT 2022, TOLD-BR, IDPT 2021, and SQUAD v1.1 datasets as prompts for inference. The objective is to evaluate Aya's effectiveness in these tasks without fine-tuning the pre-trained model, thereby exploring its potential to improve the quality and accuracy of outputs in various natural language understanding tasks.",False,"This study aims to assess Aya's performance in tasks such as Aspect-Based Sentiment Analysis, Hate Speech Detection, Irony Detection, and Question-Answering. Our methodology consists of utilizing a few-shot learning approach, incorporating examples from the ABSAPT 2022, TOLD-BR, IDPT 2021, and SQUAD v1.1 datasets as prompts for inference. The objective is to evaluate Aya's effectiveness in these tasks without fine-tuning the pre-trained model, thereby exploring its potential to improve the quality and accuracy of outputs in various natural language understanding tasks.",False,,,,,,False,,False,,,,,,False,,False
data/iclr/data/submission_F27YbvbgKy,0.0,1.0,0.0,58675,2109,88.4652955532074,,,True,"On page 1 of the main_paper.pdf, a reference indicated by a superscript '1' at the end of the abstract (line 52) is followed by the text 'Code is available at https://anonymous.4open.science/r/Topograph' (line 53). This text is intended as a footnote but is placed immediately after the abstract, not at the bottom of the page, and lacks the required 2-inch horizontal rule preceding it. The ICLR 2025 style guide (iclr2025_conference.pdf, page 2, section 4.2 'FOOTNOTES') explicitly states: 'Place the footnotes at the bottom of the page on which they appear. Precede the footnote with a horizontal rule of 2 inches (12 picas).'",,,,,True,"On page 1 of the main_paper.pdf, a reference indicated by a superscript '1' at the end of the abstract (line 52) is followed by the text 'Code is available at https://anonymous.4open.science/r/Topograph' (line 53). This text is intended as a footnote but is placed immediately after the abstract, not at the bottom of the page, and lacks the required 2-inch horizontal rule preceding it. The ICLR 2025 style guide (iclr2025_conference.pdf, page 2, section 4.2 'FOOTNOTES') explicitly states: 'Place the footnotes at the bottom of the page on which they appear. Precede the footnote with a horizontal rule of 2 inches (12 picas).'",False,,False,,False,"The submission addresses image segmentation, a core application area for deep learning, and proposes a novel graph-based framework for topology-preserving representation learning and optimization. This aligns with ICLR's scope for ""applications of deep learning in vision"" and ""optimization for representation learning.""",False,,False,,False,,,,False,,False,,,,,,False,,False
data/iclr/data/submission_v4Bl6tfaaO,1.0,1.0,0.5477608635502664,55983,2417,61.811341285705566,,,False,"The provided document is the ICLR 2025 style guide itself (iclr2025_conference.pdf). It adheres to its own formatting rules regarding page limits, statement limits, line numbers, margins, and spacing. The main text (Abstract through Conclusion/Discussion) spans exactly 10 pages (pages 1-10). Line numbers are present on all pages. Margins, text width, text height, font sizes, and paragraph spacing appear to conform to the specifications outlined within the document itself.",False,"The main content (Abstract through Conclusion/Discussion) spans from page 1 to page 10, which is exactly 10 pages, adhering to the specified page limit. Line numbers are present on all pages. No explicit Ethics or Reproducibility Statements were found. The document's margins and spacing appear consistent with the provided ICLR 2025 style guide (iclr2025_conference.pdf), indicating the use of the standard template without modifications that would constitute 'space-cheating'.",,,False,"The main content (Abstract through Conclusion/Discussion) spans from page 1 to page 10, which is exactly 10 pages, adhering to the specified page limit. Line numbers are present on all pages. No explicit Ethics or Reproducibility Statements were found. The document's margins and spacing appear consistent with the provided ICLR 2025 style guide (iclr2025_conference.pdf), indicating the use of the standard template without modifications that would constitute 'space-cheating'.",False,"The paper introduces Bayesian-LoRA, a novel method that approaches low-rank adaptation and quantization from a Bayesian perspective by employing a prior distribution on both quantization levels and rank values. It is applied to pre-trained language models (DeBERTaV3, Phi-2, Qwen) and evaluated on NLP benchmarks (GLUE, MMLU), aiming to reduce bit operations and energy consumption.",False,"The paper proposes Bayesian-LoRA, a new method that approaches low-rank adaptation and quantization from a Bayesian perspective by employing a prior distribution on both quantization levels and rank values. As a result, B-LORA is able to fine-tune a pre-trained model on a specific downstream task, finding the optimal rank values and quantization levels for every low-rank matrix. We validate the proposed model by fine-tuning a pre-trained DeBERTaV3 on the GLUE benchmark. Additionally, we fine-tune Phi-2 and Qwen, and evaluate them on few-shot and zero-shot MMLU. We compare our proposed method with relevant baselines and present both qualitative and quantitative results, showing its ability to learn optimal-rank quantized matrices. B-LORA performs on par with or better than the baselines while reducing the total number of bit operations by roughly 70% compared to the baseline methods.",False,"The paper addresses parameter-efficient fine-tuning (PEFT) of large language models (LLMs) using low-rank adaptation (LoRA) and quantization techniques, which are directly relevant to deep learning applications in natural language processing, optimization for representation learning, and implementation issues related to hardware efficiency.",False,"The paper addresses parameter-efficient fine-tuning (PEFT) of large language models (LLMs) using low-rank adaptation (LoRA) and quantization techniques, which are directly relevant to deep learning applications in natural language processing, optimization for representation learning, and implementation issues related to hardware efficiency.",True,Github link to Bayesian-LoRA implementation: https://github.com/Ksenia Sycheva/ Bayesian-Lora,,,,,True,Github link to Bayesian-LoRA implementation: https://github.com/Ksenia Sycheva/ Bayesian-Lora,False,,,,,,False,,False
data/iclr/data/submission_72nCh5JtLQ,0.0,0.0,0.0,768450,2477,157.6758198738098,,,True,"kaggle_api_path"": ""/home/users/u7212335/.kaggle""",,,,,True,"kaggle_api_path"": ""/home/users/u7212335/.kaggle""",False,"Evaluating large vision-language models (LVLMs) is very expensive, due to the high computational costs and the wide variety of tasks. The good news is that if we already have some observed scores, we may be able to infer unknown ones. In this study, we propose a new framework for predicting unknown performance scores based on observed ones from other LVLMs or tasks. We first formulate the performance prediction as a matrix completion task. Specifically, we construct a sparse performance matrix R, where each entry Rmn represents the performance score of the m-th model on the n-th dataset. By applying probabilistic matrix factorization (PMF) with Markov chain Monte Carlo (MCMC), we can complete the performance matrix, that is, predict unknown scores.",,,,,False,"Evaluating large vision-language models (LVLMs) is very expensive, due to the high computational costs and the wide variety of tasks. The good news is that if we already have some observed scores, we may be able to infer unknown ones. In this study, we propose a new framework for predicting unknown performance scores based on observed ones from other LVLMs or tasks. We first formulate the performance prediction as a matrix completion task. Specifically, we construct a sparse performance matrix R, where each entry Rmn represents the performance score of the m-th model on the n-th dataset. By applying probabilistic matrix factorization (PMF) with Markov chain Monte Carlo (MCMC), we can complete the performance matrix, that is, predict unknown scores.",False,"The document adheres to all specified formatting and layout guidelines. Line numbers are present, the main text is exactly 10 pages (Abstract through Conclusion/Discussion), and there are no apparent violations of font size, spacing, or margin requirements as per the ICLR 2025 style guide. Optional statements like Ethics or Reproducibility are not present, so no limit violations apply there.",False,"The main text of the submission, from the Abstract through the Conclusion and Future Work, spans pages 1 to 10, inclusive. This is exactly 10 pages, which adheres to the ICLR 2025 page limit of 6-10 pages for the main content. Line numbers are present on all pages, as required. Visual inspection of margins, font size, and paragraph spacing throughout the document indicates compliance with the specified formatting guidelines in the iclr2025_conference.pdf style guide. No evidence of 'space-cheating' or extensive use of small fonts in the main body was found. Optional Ethics and Reproducibility Statements were not found as distinct sections, thus no page limit violation for these statements can be flagged.",False,"The main paper adheres to all specified formatting and layout guidelines, including page limits, statement limits, line number presence, and margin/spacing requirements. The main text is 10 pages long, which is within the 6-10 page limit. Line numbers are present on all pages. No evidence of margin or spacing manipulation, or extensive use of small fonts for main body content, was found.",False,"The document adheres to all specified formatting and layout guidelines. Line numbers are present, the main text is exactly 10 pages (Abstract through Conclusion/Discussion), and there are no apparent violations of font size, spacing, or margin requirements as per the ICLR 2025 style guide. Optional statements like Ethics or Reproducibility are not present, so no limit violations apply there.",False,,False,,,,False,,False
data/iclr/data/submission_3E9oXcX8SH,1.0,1.0,1.0,68569,2958,65.82788157463074,,,False,,,,,,False,,False,"The paper's abstract, introduction, and methodology clearly describe an application of active learning and deep learning techniques to biological data acquisition and drug discovery. For example, the abstract states: ""In drug discovery, highly automated high-throughput laboratories are used to screen a large number of compounds in search of effective drugs... We call this mechanism inference set design, and propose the use of an confidence-based active learning solution... Our empirical studies on image and molecular datasets, as well as a real-world large-scale biological assay, show that active learning for inference set design leads to significant reduction in experimental cost while retaining high system performance."" This directly aligns with the ICLR scope for ""applications of deep learning in vision, audio, speech, natural language processing, robotics, neuroscience, or any other field.""

Regarding language, the text is well-structured and uses clear, professional English. Mathematical and logical flows are comprehensible. For instance, the introduction and methods sections define key terms and notations (e.g., ""inference set design,"" ""active learning,"" X_target, X_obs, p(x)) in a way that a domain expert can easily follow. While there might be minor grammatical imperfections, they do not impede the understanding of the core technical contributions or methodology.",False,,False,"The paper's abstract and introduction clearly state its focus on ""confidence-based active learning"" and ""inference set design"" for ""efficient biological data acquisition"" in ""drug discovery,"" using ""image and molecular datasets"" and ""real-world large-scale biological assay."" The methodology section further elaborates on active learning strategies. The text is written in clear, academic English, allowing a domain expert to follow the logical and mathematical flow.",False,"The paper's abstract and introduction clearly state its focus on ""confidence-based active learning"" and ""inference set design"" for ""efficient biological data acquisition"" in ""drug discovery,"" using ""image and molecular datasets"" and ""real-world large-scale biological assay."" The methodology section further elaborates on active learning strategies. The text is written in clear, academic English, allowing a domain expert to follow the logical and mathematical flow.",False,"The main content of the paper (Abstract through Discussion) spans from page 1 to page 10, inclusive, which is 10 pages. This falls within the allowed page limit of 6-10 pages for the main text. Line numbers are present on all pages in the left margin. No explicit Ethics or Reproducibility statements were found. Visual inspection of margins, font sizes, and spacing in the main body, headings, and abstract appears consistent with the ICLR 2025 style guide, indicating no obvious 'space-cheating' or extensive use of smaller fonts in the main text.",False,"The main content (Abstract through Discussion/Conclusion) spans pages 1-10, which is within the 6-10 page limit. Line numbers are present on all pages. No explicit Ethics or Reproducibility Statements were found to check for page limits. Visual inspection of the PDF and cross-referencing with the style guide's instructions on margins, font size, and paragraph spacing indicates adherence to the specified formatting rules. For example, the abstract on page 1 explicitly states 'Use 10 point type, with a vertical spacing of 11 points', which matches the style guide.",False,"The provided 'main_paper.pdf' is identical to the 'iclr2025_conference.pdf' template, which contains the formatting instructions itself. The LaTeX source 'iclr2025_conference.tex' also corresponds to this template. Therefore, the document inherently adheres to its own style guide.",False,"The provided 'main_paper.pdf' is identical to the 'iclr2025_conference.pdf' template, which contains the formatting instructions itself. The LaTeX source 'iclr2025_conference.tex' also corresponds to this template. Therefore, the document inherently adheres to its own style guide.",False,,,,,,False,,False
data/iclr/data/submission_Mphd6Sf6z4,0.0,0.0,0.0,31721,2006,36.27319073677063,,,False,,,,,,False,,False,"This review aims to evaluate and compare various static analysis tools across multiple programming languages for memory management. The tools and techniques under scrutiny include pattern matching, symbolic execution, CppCheck, SharpChecker, FindBugs, CheckStyle, and Pylint. ... We focus on understanding the full scope of their capabilities and effectiveness in managing internal and external memory components such as RAM, SRAM, PROM, Cache, Optical Drive, etc. While static analysis tools do not directly analyze physical memory components, they are crucial in enhancing memory behavior. By detecting and addressing memory-related issues early in the development process, these tools contribute significantly to the overall quality of software systems.",,,,,False,"This review aims to evaluate and compare various static analysis tools across multiple programming languages for memory management. The tools and techniques under scrutiny include pattern matching, symbolic execution, CppCheck, SharpChecker, FindBugs, CheckStyle, and Pylint. ... We focus on understanding the full scope of their capabilities and effectiveness in managing internal and external memory components such as RAM, SRAM, PROM, Cache, Optical Drive, etc. While static analysis tools do not directly analyze physical memory components, they are crucial in enhancing memory behavior. By detecting and addressing memory-related issues early in the development process, these tools contribute significantly to the overall quality of software systems.",False,"The main content (Abstract through Conclusion/Discussion) spans from page 1 to page 9 of the main_paper.pdf, totaling 9 pages. This falls within the allowed page limit of 6-10 pages. Line numbers are present in the left margin on all pages. No explicit Ethics Statement or Reproducibility Statement sections were found. Visual inspection of the document's layout, font size, and spacing does not indicate any manipulation or deviation from the specified style guide margins or font usage for the main body text.",False,"The main content of the paper (Abstract through Conclusion/Discussion) spans 9 pages, which is within the allowed 6-10 page limit. No Ethics Statement or Reproducibility Statement sections were found. Line numbers are present on all pages in the left margin. Margins, spacing, and font usage appear consistent with the ICLR 2025 style guide, with no obvious signs of 'space-cheating' or extensive use of smaller fonts in the main body text.",,,False,"The main content of the paper (Abstract through Conclusion/Discussion) spans 9 pages, which is within the allowed 6-10 page limit. No Ethics Statement or Reproducibility Statement sections were found. Line numbers are present on all pages in the left margin. Margins, spacing, and font usage appear consistent with the ICLR 2025 style guide, with no obvious signs of 'space-cheating' or extensive use of smaller fonts in the main body text.",False,,,,,,False,,False
data/iclr/data/submission_5KRxLvYLlM,1.0,1.0,1.0,92494,2306,106.50096321105956,,,False,,,,,,False,,False,"Despite the vast empirical evidence supporting the efficacy of adaptive optimization methods in deep learning, their theoretical understanding is far from complete. This work introduces novel SDEs for commonly used adaptive optimizers: SignSGD, RMSprop(W), and Adam(W). These SDEs offer a quantitatively accurate description of these optimizers and help illuminate an intricate relationship between adaptivity, gradient noise, and curvature. Our novel analysis of SignSGD highlights a noteworthy and precise contrast to SGD in terms of convergence speed, stationary distribution, and robustness to heavy-tail noise. We extend this analysis to AdamW and RMSpropW, for which we observe that the role of noise is much more complex. Crucially, we support our theoretical analysis with experimental evidence by verifying our insights: this includes numerically integrating our SDEs using Euler-Maruyama discretization on various neural network architectures such as MLPs, CNNs, ResNets, and Transformers. Our SDEs accurately track the behavior of the respective optimizers, especially when compared to previous SDEs derived for Adam and RMSprop. We believe our approach can provide valuable insights into best training practices and novel scaling rules.",,,,,False,"Despite the vast empirical evidence supporting the efficacy of adaptive optimization methods in deep learning, their theoretical understanding is far from complete. This work introduces novel SDEs for commonly used adaptive optimizers: SignSGD, RMSprop(W), and Adam(W). These SDEs offer a quantitatively accurate description of these optimizers and help illuminate an intricate relationship between adaptivity, gradient noise, and curvature. Our novel analysis of SignSGD highlights a noteworthy and precise contrast to SGD in terms of convergence speed, stationary distribution, and robustness to heavy-tail noise. We extend this analysis to AdamW and RMSpropW, for which we observe that the role of noise is much more complex. Crucially, we support our theoretical analysis with experimental evidence by verifying our insights: this includes numerically integrating our SDEs using Euler-Maruyama discretization on various neural network architectures such as MLPs, CNNs, ResNets, and Transformers. Our SDEs accurately track the behavior of the respective optimizers, especially when compared to previous SDEs derived for Adam and RMSprop. We believe our approach can provide valuable insights into best training practices and novel scaling rules.",False,"The document adheres to all specified formatting and layout guidelines. Line numbers are present on every page. The main content (Abstract through Conclusion) spans exactly 10 pages, which is within the allowed 6-10 page limit. No Ethics or Reproducibility Statements were found, thus no length violation for them. Margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of smaller fonts in the main body.",False,"The main paper adheres to all specified formatting and layout guidelines. Line numbers are present on all pages. The main content (Abstract through Conclusion) is exactly 10 pages, which falls within the 6-10 page limit. No explicit Ethics or Reproducibility Statements were found, thus no page limit violation for them. Margins, font sizes, paragraph spacing, and heading styles are consistent with the ICLR 2025 style guide as demonstrated by the provided iclr2025_conference.pdf template. No evidence of 'space-cheating' or extensive use of smaller fonts for main body text was observed.",,,False,"The main paper adheres to all specified formatting and layout guidelines. Line numbers are present on all pages. The main content (Abstract through Conclusion) is exactly 10 pages, which falls within the 6-10 page limit. No explicit Ethics or Reproducibility Statements were found, thus no page limit violation for them. Margins, font sizes, paragraph spacing, and heading styles are consistent with the ICLR 2025 style guide as demonstrated by the provided iclr2025_conference.pdf template. No evidence of 'space-cheating' or extensive use of smaller fonts for main body text was observed.",False,,,,,,False,,False
data/iclr/data/submission_n6Gg0D2jWT,0.0,0.0,0.0,33222,1924,28.79875111579895,,,False,"Although trust region policy optimization methods have achieved a lot of success in cooperative multi-agent tasks, most of them face a non-stationarity problem during the learning process. Recently, sequential trust region methods that update policies agent-by-agent have shed light on alleviating the non-stationarity problem. However, these methods are still less sample-efficient when compared to their counterparts (i.e., PPO) in a single-agent setting. To narrow this efficiency gap, we propose the Off-Policyness-aware Sequential Policy Optimization (OPSPO) method, which explicitly manages the off-policyness that arises from the sequential policy update process among multiple agents. We prove that our OPSPO has the tightness of the monotonic improvement bound compared with other trust region multi-agent learning methods. Finally, we demonstrate that our OPSPO consistently outperforms strong baselines under challenging multi-agent benchmarks, including StarCraftII micromanagement tasks, Multi-agent MuJoCo, and Google Research Football full game scenarios.",,,,,False,"Although trust region policy optimization methods have achieved a lot of success in cooperative multi-agent tasks, most of them face a non-stationarity problem during the learning process. Recently, sequential trust region methods that update policies agent-by-agent have shed light on alleviating the non-stationarity problem. However, these methods are still less sample-efficient when compared to their counterparts (i.e., PPO) in a single-agent setting. To narrow this efficiency gap, we propose the Off-Policyness-aware Sequential Policy Optimization (OPSPO) method, which explicitly manages the off-policyness that arises from the sequential policy update process among multiple agents. We prove that our OPSPO has the tightness of the monotonic improvement bound compared with other trust region multi-agent learning methods. Finally, we demonstrate that our OPSPO consistently outperforms strong baselines under challenging multi-agent benchmarks, including StarCraftII micromanagement tasks, Multi-agent MuJoCo, and Google Research Football full game scenarios.",False,,,,,,False,,False,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 10, inclusive, totaling 10 pages. This is within the allowed page limit of 6-10 pages. Line numbers are present on all pages in the left margin. There are no explicit Ethics or Reproducibility Statements. Visual inspection of the PDF does not reveal any obvious margin or spacing violations, nor extensive use of small fonts in the main body text.",,,,,False,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 10, inclusive, totaling 10 pages. This is within the allowed page limit of 6-10 pages. Line numbers are present on all pages in the left margin. There are no explicit Ethics or Reproducibility Statements. Visual inspection of the PDF does not reveal any obvious margin or spacing violations, nor extensive use of small fonts in the main body text.",False,,,,,,False,,False
data/iclr/data/submission_viOk8hPJwb,1.0,1.0,1.0,106628,2748,69.52902507781982,,,False,"The paper discusses ""contrastive multi-skill reinforcement learning (RL)"", ""reducing the hypothesis space"", ""hierarchically grouped"" skills, and provides ""mathematically proven"" bounds for optimality. It also mentions ""applications of deep learning in vision, audio, speech, natural language processing, robotics, neuroscience, or any other field"" in the context of surpassing other prominent algorithms.",False,"The paper discusses contrastive multi-skill reinforcement learning, PAC-MDP theory, hypothesis space reduction, and hierarchical grouping of skills, all of which fall under representation learning for planning and reinforcement learning, hierarchical models, and optimization for representation learning.",False,"The paper discusses ""contrastive multi-skill reinforcement learning (RL)"", ""reducing the hypothesis space"", ""hierarchical models"", ""Active Learning and the PAC-MDP"", and proposes an ""algorithm that is applicable in real-world scenarios, which has been demonstrated to surpass other prominent algorithms."" These topics align with ICLR's scope, specifically ""representation learning for planning and reinforcement learning"", ""optimization for representation learning"", and ""hierarchical models"".",False,"The paper discusses contrastive multi-skill reinforcement learning, PAC-MDP theory, hypothesis space reduction, and hierarchical grouping of skills, all of which fall under representation learning for planning and reinforcement learning, hierarchical models, and optimization for representation learning.",False,,,,,,False,,False,"The main content of the paper (Abstract through Section 5) spans 10 pages, which adheres to the specified limit of 6-10 pages. Line numbers are present in the left margin of all pages. No explicit Ethics Statement or Reproducibility Statement sections were found. Visual inspection and review of the LaTeX source indicate no apparent violations of font size, margins, or spacing in the main body text, with any specific spacing commands (vspace) being consistent with the style guide's own usage for notation sections.",False,"The main content of the paper (Abstract through Conclusion/Discussion) spans 10 pages, which is within the allowed limit of 6-10 pages. References and Appendices are correctly excluded from this count. Line numbers are present on all pages. Visual inspection of margins, font sizes, and paragraph spacing indicates adherence to the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of smaller fonts in the main body text.",False,"The main content (Abstract through Conclusion/Discussion) spans 10 pages (pages 1-10 of main_paper.pdf), which is within the 6-10 page limit. Line numbers are present on all pages. No explicit Ethics or Reproducibility statements were found. Visual inspection and review of the LaTeX source (iclr2025_conference.tex) indicate adherence to margin, font size, and spacing guidelines, with no evidence of 'space-cheating' or extensive use of small fonts in the main body.",False,"The main content (Abstract through Conclusion/Discussion) spans 10 pages (pages 1-10 of main_paper.pdf), which is within the 6-10 page limit. Line numbers are present on all pages. No explicit Ethics or Reproducibility statements were found. Visual inspection and review of the LaTeX source (iclr2025_conference.tex) indicate adherence to margin, font size, and spacing guidelines, with no evidence of 'space-cheating' or extensive use of small fonts in the main body.",False,,False,,False,,False,,False
data/iclr/data/submission_8g7hHwSBjH,0.0,0.0,0.0,44216,2139,33.94323444366455,,,False,,False,"Retrieval-augmented generation (RAG) has shown its impressive capability of providing reliable answer predictions and addressing severe hallucination prob- lems. A typical RAG implementation adopts powerful retrieval models to ex- tract external information and leverage large language models (LLMs) to gener- ate corresponding answers. Different with that, recent LLM-based retrieval has raised much attention because it brings substantial improvements in information retrieval (IR) via LLMs' vigorous semantic understanding capability. However, directly applying LLM to RAG systems remains certain challenges. This may cause feature locality problems since massive parametric knowledge impedes the effective usage of the global information among all corpus, e.g., a LLM-based re- triever usually inputs the summary of documents instead of the whole documents. Moreover, various tasks pre-trained in LLMs induce severe variance, which fur- ther weakens its performance as the retriever. To address these issues, we propose a novel two-stage fine-tuning architecture called Invar-RAG. In the retrieval stage, a LLM-based retriever is constructed by integrating a LoRA-based representation learning to address the feature locality problem. To justify and consolidate this retrieval's performance, two patterns (i.e., invariant and variant patterns) and an invariance loss are also developed to alleviate the variance in LLM. Moreover, in the generation stage, a meticulously designed fine-tuning method is devised to im- prove our LLM for accurate answer generation based on the retrieved information. Experimental results demonstrate that Invar-RAG significantly outperforms exist- ing baselines across three Open-domain Question Answering (ODQA) datasets. The code is available in Supplementary Material to ease reproducibility.",,,False,"Retrieval-augmented generation (RAG) has shown its impressive capability of providing reliable answer predictions and addressing severe hallucination prob- lems. A typical RAG implementation adopts powerful retrieval models to ex- tract external information and leverage large language models (LLMs) to gener- ate corresponding answers. Different with that, recent LLM-based retrieval has raised much attention because it brings substantial improvements in information retrieval (IR) via LLMs' vigorous semantic understanding capability. However, directly applying LLM to RAG systems remains certain challenges. This may cause feature locality problems since massive parametric knowledge impedes the effective usage of the global information among all corpus, e.g., a LLM-based re- triever usually inputs the summary of documents instead of the whole documents. Moreover, various tasks pre-trained in LLMs induce severe variance, which fur- ther weakens its performance as the retriever. To address these issues, we propose a novel two-stage fine-tuning architecture called Invar-RAG. In the retrieval stage, a LLM-based retriever is constructed by integrating a LoRA-based representation learning to address the feature locality problem. To justify and consolidate this retrieval's performance, two patterns (i.e., invariant and variant patterns) and an invariance loss are also developed to alleviate the variance in LLM. Moreover, in the generation stage, a meticulously designed fine-tuning method is devised to im- prove our LLM for accurate answer generation based on the retrieved information. Experimental results demonstrate that Invar-RAG significantly outperforms exist- ing baselines across three Open-domain Question Answering (ODQA) datasets. The code is available in Supplementary Material to ease reproducibility.",False,,,,,,False,,True,"On page 1, line 035, the paragraph 'Over the past decade, large language models (LLMs) have demonstrated promising capability...' is indented. This indentation is present throughout the main body of the paper. The ICLR 2025 style guide (iclr2025_conference.pdf, Section 2, 'GENERAL FORMATTING INSTRUCTIONS', page 1, line 049) explicitly states: 'Paragraphs are separated by 1/2 line space, with no indentation.'",,,,,True,"On page 1, line 035, the paragraph 'Over the past decade, large language models (LLMs) have demonstrated promising capability...' is indented. This indentation is present throughout the main body of the paper. The ICLR 2025 style guide (iclr2025_conference.pdf, Section 2, 'GENERAL FORMATTING INSTRUCTIONS', page 1, line 049) explicitly states: 'Paragraphs are separated by 1/2 line space, with no indentation.'",False,,,,,,False,,False
data/iclr/data/submission_xIW2WtCuYE,0.0,1.0,0.0,17117,2073,53.30231046676636,,,True,"The document's title and content, including phrases like ""הסכם שכירות משנה בלתי מוגנת"" (Unprotected Sublease Agreement), ""תקופת שכירות המשנה"" (Sublease Period), and ""דמי שכירות ותשלומים נוספים"" (Rent and Additional Payments), clearly indicate its nature.",True,"The document title, as identified by OCR, is ""הסכם שכירות משנה בלתי מוגנת"" (Unprotected Sub-Lease Agreement). The content discusses rental terms, property details, and payment schedules, as seen in phrases like ""שוכר המשנה לשכור מהמשכירה בשכירות משנה חלק מן המושכר"" (the sub-tenant to rent from the lessor a part of the leased property) and ""דמי שכירות ותשלומים נוספים"" (rental fees and additional payments).",True,"The entire document is a legal contract written in Hebrew. For example, the title ""הסכם שכירות משנה בלתי מוגנת"" translates to ""Unprotected Sublease Agreement."" The content discusses rental terms, parties involved (landlord, subtenant), and legal clauses, such as ""דמי שכירות ותשלומים נוספים"" (rent and additional payments).",True,"The entire document is a legal contract written in Hebrew. For example, the title ""הסכם שכירות משנה בלתי מוגנת"" translates to ""Unprotected Sublease Agreement."" The content discusses rental terms, parties involved (landlord, subtenant), and legal clauses, such as ""דמי שכירות ותשלומים נוספים"" (rent and additional payments).",False,,,,,,False,,True,"The document is written in Hebrew, uses a non-standard font and layout, and lacks the specified 1.5-inch left margin and 10pt Times New Roman font with 11pt vertical spacing as required by the ICLR 2025 style guide.",,,,,True,"The document is written in Hebrew, uses a non-standard font and layout, and lacks the specified 1.5-inch left margin and 10pt Times New Roman font with 11pt vertical spacing as required by the ICLR 2025 style guide.",False,,False,"The provided document is a template for a legal sublease agreement in Hebrew, branded with 'zap משפטי'. It contains placeholders for names, addresses, and financial details typical of a contract, but no actual identifying information related to the authors of a research paper.",,,False,"The provided document is a template for a legal sublease agreement in Hebrew, branded with 'zap משפטי'. It contains placeholders for names, addresses, and financial details typical of a contract, but no actual identifying information related to the authors of a research paper.",False
data/iclr/data/submission_JGt14kjxB2,0.0,0.0,0.0,48183,2067,65.78880786895752,,,False,"The paper addresses Federated Learning (FL) with pre-trained models, focusing on reducing communication costs and improving performance by exploiting mean distributions for class covariance estimation. This falls under representation learning, optimization for representation learning, and applications of deep learning in vision, which are all in-scope topics for ICLR.",False,,False,"ABSTRACT: Using pre-trained models has been found to reduce the effect of data heterogeneity and speed up federated learning algorithms. In this work we propose a training-free method based on an unbiased estimator of class covariance matrices. Our method, which only uses first-order statistics in the form of class means communicated by clients to the server, incurs only a fraction of the communication costs required by methods based on communicating second-order statistics.",False,"ABSTRACT: Using pre-trained models has been found to reduce the effect of data heterogeneity and speed up federated learning algorithms. In this work we propose a training-free method based on an unbiased estimator of class covariance matrices. Our method, which only uses first-order statistics in the form of class means communicated by clients to the server, incurs only a fraction of the communication costs required by methods based on communicating second-order statistics.",False,,,,,,False,,True,"From the ICLR 2025 style guide (iclr2025_conference.pdf, page 1, line 053): ""All pages should start at 1 inch (6 picas) from the top of the page."" The provided LaTeX style file (iclr2025_conference.sty) contains the line: ""\setlength{\topmargin}{-0.5in}"". The generated PDF (main_paper.pdf) exhibits a top margin of approximately 0.5 inches.",,,,,True,"From the ICLR 2025 style guide (iclr2025_conference.pdf, page 1, line 053): ""All pages should start at 1 inch (6 picas) from the top of the page."" The provided LaTeX style file (iclr2025_conference.sty) contains the line: ""\setlength{\topmargin}{-0.5in}"". The generated PDF (main_paper.pdf) exhibits a top margin of approximately 0.5 inches.",False,,,,,,False,,False
data/iclr/data/submission_nQoRKLeOP0,1.0,1.0,0.8910737166334615,565137,2277,67.02306270599365,,,False,,False,"Personalized medicine is a groundbreaking healthcare framework for the 21st century, tailoring medical treatments to individuals based on unique clinical characteristics, including diverse medical imaging modalities. Given the significant differences among these modalities due to distinct underlying imaging principles, generalization in multi-modal medical image tasks becomes substantially challenging. Previous methods addressing multi-modal generalization rarely consider personalization, primarily focusing on common anatomical information. This paper aims to bridge multi-modal generalization with the concept of personalized medicine. Specifically, we propose a novel approach to derive a tractable form of the underlying personalized invariant representation Xh by leveraging individual-level constraints and a learnable biological prior.",,,False,"Personalized medicine is a groundbreaking healthcare framework for the 21st century, tailoring medical treatments to individuals based on unique clinical characteristics, including diverse medical imaging modalities. Given the significant differences among these modalities due to distinct underlying imaging principles, generalization in multi-modal medical image tasks becomes substantially challenging. Previous methods addressing multi-modal generalization rarely consider personalization, primarily focusing on common anatomical information. This paper aims to bridge multi-modal generalization with the concept of personalized medicine. Specifically, we propose a novel approach to derive a tractable form of the underlying personalized invariant representation Xh by leveraging individual-level constraints and a learnable biological prior.",False,,,,,,False,,False,"The main paper (main_paper.pdf) adheres to all specified formatting and layout guidelines. The main content (Abstract through Conclusion) spans exactly 10 pages (pages 1-10), which is within the allowed limit. Line numbers are present throughout the document. Margins, font sizes, and spacing for abstract, main body text, and headings appear to conform to the specifications outlined in the ICLR 2025 style guide (iclr2025_conference.pdf). No evidence of 'space-cheating' or extensive use of smaller fonts for the main text was found. Optional statements like Ethics or Reproducibility are not present, thus no violation of their page limits.",False,"The main content of the paper (Abstract through Conclusion) spans 10 pages, which is within the strict 10-page limit specified in the ICLR 2025 style guide. References and appendices are correctly placed after the main content and do not count towards the limit. Line numbers are present on all pages. Visual inspection of margins, font sizes, and spacing throughout the document (including headings, abstract, and main body text) indicates adherence to the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of small fonts in the main body. Optional sections like 'Author Contributions' and 'Acknowledgments' are present but do not constitute 'Ethics Statement' or 'Reproducibility Statement' and are not subject to separate page limits.",False,"The main content of the paper (Abstract through Conclusion/Discussion) spans from page 1 to page 10, totaling 10 pages. Line numbers are present on all pages. Visual inspection of margins, spacing, and font usage does not reveal any deviations from the ICLR 2025 style guide. No explicit Ethics or Reproducibility statements were found to check against their page limits.",False,"The main content of the paper (Abstract through Conclusion/Discussion) spans from page 1 to page 10, totaling 10 pages. Line numbers are present on all pages. Visual inspection of margins, spacing, and font usage does not reveal any deviations from the ICLR 2025 style guide. No explicit Ethics or Reproducibility statements were found to check against their page limits.",True,data/iclr/data/submission_nQoRKLeOP0/supplemental_files/cpfs01/projects-HDD/cfff-c7cd658afc74_HDD/tanzhaorui/test/Code_for_submit/readme.md,,,,,True,data/iclr/data/submission_nQoRKLeOP0/supplemental_files/cpfs01/projects-HDD/cfff-c7cd658afc74_HDD/tanzhaorui/test/Code_for_submit/readme.md,False
data/iclr/data/submission_s0gdfKcmoU,1.0,1.0,1.0,126693,2912,69.22431254386902,,,False,,False,,False,,False,,False,,,,,,False,,False,"The main paper (main_paper.pdf) adheres to all specified formatting and layout guidelines. Line numbers are present on all pages. The main content (Abstract through Conclusion) is exactly 10 pages, which is within the 6-10 page limit. No Ethics or Reproducibility Statements exceeding 1 page were found. Margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of smaller fonts in the main body.",False,"The main content (Abstract through Conclusion/Discussion) spans from page 1 to page 10, which is exactly 10 pages, adhering to the specified page limit. Line numbers are present on all pages in the left margin. No explicit Ethics or Reproducibility Statements exceeding one page were found. Visual inspection of the document and review of the LaTeX source (iclr2025_conference.tex) indicate that standard ICLR 2025 style files were used, and there are no apparent modifications to margins, spacing, or font sizes that would constitute 'space-cheating' or layout violations. The document's layout, including font sizes for main text, headings, and abstract, as well as paragraph spacing and margins, appears consistent with the ICLR 2025 style guide.",False,"The main paper (main_paper.pdf) and the supplemental file (supplemental.pdf) adhere to all specified formatting and layout guidelines. Line numbers are present, the main content is within the 10-page limit, and there are no apparent violations in margins, spacing, or font usage for the main body text. No Ethics or Reproducibility Statements were found to exceed 1 page.",False,"The main paper (main_paper.pdf) adheres to all specified formatting and layout guidelines. Line numbers are present on all pages. The main content (Abstract through Conclusion) is exactly 10 pages, which is within the 6-10 page limit. No Ethics or Reproducibility Statements exceeding 1 page were found. Margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of smaller fonts in the main body.",False,,,,,,False,,False
data/iclr/data/submission_2924nrgHce,1.0,1.0,1.0,81918,2894,202.241822719574,,,False,,,,,,False,,False,,,,,,False,,False,"The main content (Abstract through Conclusion) spans pages 1-10, which is exactly 10 pages, adhering to the page limit. The document includes line numbers in the left margin on every page. There are no explicit Ethics Statement or Reproducibility Statement sections. Visual inspection of margins, font sizes, and spacing for the main body text, headings, figures, and tables appears consistent with the ICLR 2025 style guide, with no obvious signs of 'space-cheating' or extensive use of small fonts outside of allowed contexts like captions or tables.",False,"The main content (Abstract through Conclusion) spans exactly 10 pages (page 1 to page 10). Line numbers are present on all pages in the left margin. No explicit Ethics Statement or Reproducibility Statement sections were found. Visual inspection of the PDF shows consistent margins, font sizes, and spacing that appear to adhere to the ICLR 2025 style guide, with no obvious signs of 'space-cheating' or extensive use of small fonts in the main body text.",False,"The main text (Abstract through Conclusion) spans exactly 10 pages (pages 1-10 of the PDF). References and Appendices follow, which do not count towards the page limit. Line numbers are present in the left margin on all pages. Visual inspection of the PDF and review of the LaTeX source (`iclr2025_conference.tex`) do not reveal any explicit modifications to font sizes, margins, or spacing in a way that would violate the ICLR 2025 style guide. The document uses the official `iclr2025_conference` style file.",False,"The main content (Abstract through Conclusion) spans pages 1-10, which is exactly 10 pages, adhering to the page limit. The document includes line numbers in the left margin on every page. There are no explicit Ethics Statement or Reproducibility Statement sections. Visual inspection of margins, font sizes, and spacing for the main body text, headings, figures, and tables appears consistent with the ICLR 2025 style guide, with no obvious signs of 'space-cheating' or extensive use of small fonts outside of allowed contexts like captions or tables.",False,,False,,False,,False,,False
data/iclr/data/submission_MOCEoNsjEx,1.0,1.0,1.0,58371,2526,52.5273597240448,,,False,,False,,False,,False,,True,Code is avaliable at https://gitlab.com/FUTURE_LINK,,,,,True,Code is avaliable at https://gitlab.com/FUTURE_LINK,False,"The main text (Abstract through Conclusion) spans 8 pages (pages 1-8), which is within the allowed 6-10 page limit. Optional statements (Reproducibility Statement on page 11) are well within the 1-page limit. Line numbers are present on all pages. Visual inspection of the PDF and review of the LaTeX source indicate adherence to margin, font size, and spacing guidelines, with no evidence of 'space-cheating' or extensive use of small fonts in the main body.",False,"The main content (Abstract through Conclusion) spans from page 1 to page 8, totaling 8 pages, which is within the 6-10 page limit. All pages include line numbers in the left margin. Optional statements (Reproducibility Statement) are well under the 1-page limit. Margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of smaller fonts in the main body.",False,"The main content of the paper (Abstract through Conclusion) spans 8 pages (from page 1 to page 8), which is within the allowed 6-10 page limit. Line numbers are present on all pages. The optional Reproducibility Statement is brief and does not exceed 1 page. No Ethics Statement was found. Visual inspection of the PDF and review of the LaTeX source indicate adherence to the specified font sizes, margins, and spacing guidelines, with no evidence of 'space-cheating' or extensive use of small fonts in the main body.",False,"The main content (Abstract through Conclusion) spans from page 1 to page 8, totaling 8 pages, which is within the 6-10 page limit. All pages include line numbers in the left margin. Optional statements (Reproducibility Statement) are well under the 1-page limit. Margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of smaller fonts in the main body.",False,,False,,,,False,,False
data/iclr/data/submission_Cody9WEwvN,1.0,1.0,1.0,59610,2238,52.6357650756836,,,False,"The paper addresses controlled text generation using Large Language Models (LLMs), proposing a novel decoding algorithm called Discrete Auto-regressive Biasing (DAB). This method leverages gradients within a discrete text domain to improve constraint satisfaction and fluency in applications such as sentiment control, language detoxification, and keyword-guided generation. These topics align with 'applications of deep learning in ... natural language processing' and 'optimization for representation learning' as specified in the ICLR 2025 call for papers.",False,,False,,False,"The paper addresses controlled text generation using Large Language Models (LLMs), proposing a novel decoding algorithm called Discrete Auto-regressive Biasing (DAB). This method leverages gradients within a discrete text domain to improve constraint satisfaction and fluency in applications such as sentiment control, language detoxification, and keyword-guided generation. These topics align with 'applications of deep learning in ... natural language processing' and 'optimization for representation learning' as specified in the ICLR 2025 call for papers.",False,,,,,,False,,False,"The provided main_paper.pdf adheres to all specified formatting and layout guidelines, including page limits, statement limits, line numbering, and margin/spacing requirements. The document appears to be generated using the official ICLR 2025 LaTeX style files.",False,The submitted 'main_paper.pdf' is identical to the 'iclr2025_conference.pdf' (the official style guide document). The LaTeX source 'iclr2025_conference.tex' was also provided.,,,False,The submitted 'main_paper.pdf' is identical to the 'iclr2025_conference.pdf' (the official style guide document). The LaTeX source 'iclr2025_conference.tex' was also provided.,False,,,,,,False,,True
data/iclr/data/submission_YE5m1lkx4q,1.0,1.0,1.0,659933,3007,86.89183020591736,,,False,"The submission's abstract, introduction, and methodology clearly describe a novel approach for long-context Retrieval-Augmented Generation (RAG) using SNR-based span uncertainty to enhance model calibration and train retrieval models. This falls under 'applications of deep learning in natural language processing' and 'unsupervised, semi-supervised, and supervised representation learning'. The language is clear and the technical concepts are well-defined.",False,"The paper's abstract, introduction, and methodology clearly describe a novel approach for long-context Retrieval-Augmented Generation (RAG) using SNR-based span uncertainty, which falls under 'applications of deep learning in ... natural language processing' and 'representation learning' topics. The technical concepts are presented clearly with appropriate mathematical notation and explanations, making the content comprehensible to a domain expert.",False,"We present UncertaintyRAG, a novel approach for long-context Retrieval-Augmented Generation (RAG) that utilizes Signal-to-Noise Ratio (SNR)-based span uncertainty to estimate similarity between text chunks. This span uncertainty enhances model calibration, improving robustness and mitigating semantic inconsistencies introduced by random chunking. Leveraging this insight, we propose an efficient unsupervised learning technique to train the retrieval model, alongside an effective data sampling and scaling strategy. UncertaintyRAG outperforms baselines by 2.03% on LLaMA-2-7B, achieving state-of-the-art results while using only 4% of the training data compared to other advanced open-source retrieval models under distribution shift settings. Our method demonstrates strong calibration through span uncertainty, leading to improved generalization and robustness in long-context RAG tasks. Additionally, UncertaintyRAG provides a lightweight retrieval model that can be integrated into any large language model with varying context window lengths, without the need for fine-tuning, showcasing the flexibility of our approach.",False,"We present UncertaintyRAG, a novel approach for long-context Retrieval-Augmented Generation (RAG) that utilizes Signal-to-Noise Ratio (SNR)-based span uncertainty to estimate similarity between text chunks. This span uncertainty enhances model calibration, improving robustness and mitigating semantic inconsistencies introduced by random chunking. Leveraging this insight, we propose an efficient unsupervised learning technique to train the retrieval model, alongside an effective data sampling and scaling strategy. UncertaintyRAG outperforms baselines by 2.03% on LLaMA-2-7B, achieving state-of-the-art results while using only 4% of the training data compared to other advanced open-source retrieval models under distribution shift settings. Our method demonstrates strong calibration through span uncertainty, leading to improved generalization and robustness in long-context RAG tasks. Additionally, UncertaintyRAG provides a lightweight retrieval model that can be integrated into any large language model with varying context window lengths, without the need for fine-tuning, showcasing the flexibility of our approach.",False,,,,,,False,,False,"The main text (Abstract through Conclusion/Discussion, Sections 1-5) spans from page 1 to page 10 of the main_paper.pdf. The ICLR 2025 style guide (iclr2025_conference.pdf, page 2, lines 060-061) states a strict upper limit of 10 pages for the main text. This submission adheres to this limit. Line numbers are present in the left margin on all pages, as required. There are no explicit Ethics or Reproducibility Statements exceeding one page. Margins, font sizes, and spacing appear consistent with the style guide's specifications (e.g., 5.5 inches wide text block, 1.5 inch left margin, 10pt type with 11pt vertical spacing, 1/2 line space between paragraphs). Small fonts are appropriately used in figures, tables, and footnotes, which is permitted.",False,"The main text (Abstract through Conclusion/Discussion) spans pages 1-10, which is within the 6-10 page limit. Line numbers are present on all pages of the main paper. No explicit Ethics Statement or Reproducibility Statement exceeding 1 page was found. Margins and spacing appear to conform to the style guide specifications (5.5 inches wide, 9 inches long text area, 1.5 inch left margin, 1 inch top margin, 10pt font with 11pt vertical spacing, 1/2 line space between paragraphs). The LaTeX source confirms these settings and warns against modification.",,,False,"The main text (Abstract through Conclusion/Discussion) spans pages 1-10, which is within the 6-10 page limit. Line numbers are present on all pages of the main paper. No explicit Ethics Statement or Reproducibility Statement exceeding 1 page was found. Margins and spacing appear to conform to the style guide specifications (5.5 inches wide, 9 inches long text area, 1.5 inch left margin, 1 inch top margin, 10pt font with 11pt vertical spacing, 1/2 line space between paragraphs). The LaTeX source confirms these settings and warns against modification.",False,,False,,False,,False,,False
data/iclr/data/submission_iJxsH77FPQ,0.0,0.0,0.0,4040823,2821,145.92185878753662,,,False,"Expressive large-scale neural networks enable training powerful models for prediction tasks. However, in many engineering and science domains, such models are intended to be used not just for prediction, but for design—e.g., creating new proteins that serve as effective therapeutics, or creating new materials or chemicals that maximize a downstream performance measure. Thus, researchers have recently grown an interest in building deep learning methods that solve offline model-based optimization (MBO) problems, in which design candidates are optimized with respect to surrogate models learned from offline data. ... In this paper, following first principles, we develop a model that learns the structure of an MBO task and empirically leads to improved designs. To this end, we introduce Cliqueformer—a scalable transformer-based architecture that learns the black-box function’s structure in the form of its functional graphical model (FGM), thus bypassing the problem of distribution shift, previously tackled by conservative approaches. We evaluate Cliqueformer on various tasks, ranging from high-dimensional black-box functions from MBO literature to real-world tasks of chemical and genetic design, consistently demonstrating its state-of-the-art performance.",,,,,False,"Expressive large-scale neural networks enable training powerful models for prediction tasks. However, in many engineering and science domains, such models are intended to be used not just for prediction, but for design—e.g., creating new proteins that serve as effective therapeutics, or creating new materials or chemicals that maximize a downstream performance measure. Thus, researchers have recently grown an interest in building deep learning methods that solve offline model-based optimization (MBO) problems, in which design candidates are optimized with respect to surrogate models learned from offline data. ... In this paper, following first principles, we develop a model that learns the structure of an MBO task and empirically leads to improved designs. To this end, we introduce Cliqueformer—a scalable transformer-based architecture that learns the black-box function’s structure in the form of its functional graphical model (FGM), thus bypassing the problem of distribution shift, previously tackled by conservative approaches. We evaluate Cliqueformer on various tasks, ranging from high-dimensional black-box functions from MBO literature to real-world tasks of chemical and genetic design, consistently demonstrating its state-of-the-art performance.",False,,,,,,False,,False,"The main text (Abstract through Conclusion) spans from page 1 to page 9, totaling 9 pages, which is within the 6-10 page limit. Line numbers are present on all pages. The 'REPRODUCIBILITY STATEMENT' is on page 10, which is part of the unlimited additional pages and is less than one page. Visual inspection of margins, font sizes, and spacing for the main body text, headings, figures, and tables shows adherence to the style guide. No extensive use of smaller fonts or narrowed margins for the main content was observed.",,,,,False,"The main text (Abstract through Conclusion) spans from page 1 to page 9, totaling 9 pages, which is within the 6-10 page limit. Line numbers are present on all pages. The 'REPRODUCIBILITY STATEMENT' is on page 10, which is part of the unlimited additional pages and is less than one page. Visual inspection of margins, font sizes, and spacing for the main body text, headings, figures, and tables shows adherence to the style guide. No extensive use of smaller fonts or narrowed margins for the main content was observed.",True,"1. `supplemental/cliqueformer_anonymous/Bioseq/README.md`: ""Lab-in-the-loop setting? : Check out [SEIKO](https://github.com/zhaoyl18/SEIKO), implementation of online diffusion model fine-tuning, as well as the paper [(Uehara and Zhao et.al, 2024)](https://arxiv.org/abs/2402.16359).""
2. `supplemental/cliqueformer_anonymous/autofocused_oracles/README.md`: ""Download data for this directory from repository https://github.com/clarafy/autofocused-oracles""
3. `supplemental/cliqueformer_anonymous/Bioseq/download_assets.py` (and other wandb logs in notebooks): ""wandb: Currently logged in as: masatoshi136 (masa136). Use `wandb login --relogin` to force relogin""
4. `supplemental/cliqueformer_anonymous/Bioseq/tutorials/UTR/UTR_oracle_training.ipynb` (and other wandb logs in notebooks): ""wandb: Currently logged in as: sarosavo. Use `wandb login --relogin` to force relogin""",True,"We obtained the DNA Enhancers data from the recent tutorial on offline fine-tuning of generative models at https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq/tree/master/tutorials/Human-enhancer

This code accompanies the [tutorial/review paper](https://arxiv.org/abs/2407.13734) on RL-based fine-tuning...

@article{Uehara2024understanding, title={Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion Models: A Tutorial and Review}, author={Uehara, Masatoshi and Zhao, Yulai and Biancalani, Tommaso and Levine, Sergey}, journal={arXiv preprint arXiv:2407.13734}, year={2024}}",,,True,"We obtained the DNA Enhancers data from the recent tutorial on offline fine-tuning of generative models at https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq/tree/master/tutorials/Human-enhancer

This code accompanies the [tutorial/review paper](https://arxiv.org/abs/2407.13734) on RL-based fine-tuning...

@article{Uehara2024understanding, title={Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion Models: A Tutorial and Review}, author={Uehara, Masatoshi and Zhao, Yulai and Biancalani, Tommaso and Levine, Sergey}, journal={arXiv preprint arXiv:2407.13734}, year={2024}}",False
data/iclr/data/submission_sSWiZr8QU7,1.0,1.0,0.6849291250867845,43133,1914,42.34100151062012,,,False,,,,,,False,,False,"The submission describes a ""hybrid simulation of DNN-based gray box models"" for ""simulating complex physical systems"" such as power systems and integrated circuits. It leverages ""deep neural networks (DNNs) to capture hidden physical behaviors"" and integrates them into ""numerical solvers of simulation engines."" This falls under the ICLR scope of ""applications of deep learning in ... any other field.""",False,"Simulation is vital for scientific and engineering disciplines, as it enables the prediction and design of physical systems. However, the computational challenges inherent to large-scale simulations often arise from complex device models featuring high degrees of nonlinearities or hidden physical behaviors not captured by first principles. Gray-box models that combine deep neural networks (DNNs) with physics-based models have been proposed to address the computational challenges in modeling complex physical systems. A well-crafted gray box model capitalizes on the interpretability and accuracy of a physical model while incorporating deep neural networks to capture hidden physical behaviors and mitigate computational load associated with highly nonlinear components.",,,False,"Simulation is vital for scientific and engineering disciplines, as it enables the prediction and design of physical systems. However, the computational challenges inherent to large-scale simulations often arise from complex device models featuring high degrees of nonlinearities or hidden physical behaviors not captured by first principles. Gray-box models that combine deep neural networks (DNNs) with physics-based models have been proposed to address the computational challenges in modeling complex physical systems. A well-crafted gray box model capitalizes on the interpretability and accuracy of a physical model while incorporating deep neural networks to capture hidden physical behaviors and mitigate computational load associated with highly nonlinear components.",True,"Aayushya Agarwal Department of Electrical and Computer Engineering Carnegie Mellon University Pittsburgh, PA 15213 aayushya@andrew.cmu.edu Yihan Ruan Department of Electrical and Computer Engineering Carnegie Mellon University Pittsburgh, PA 15213 yihanr@andrew.cmu.edu Larry Pileggi Department of Electrical and Computer Engineering Carnegie Mellon University Pittsburgh, PA 15213 pileggi@andrew.cmu.edu",,,,,True,"Aayushya Agarwal Department of Electrical and Computer Engineering Carnegie Mellon University Pittsburgh, PA 15213 aayushya@andrew.cmu.edu Yihan Ruan Department of Electrical and Computer Engineering Carnegie Mellon University Pittsburgh, PA 15213 yihanr@andrew.cmu.edu Larry Pileggi Department of Electrical and Computer Engineering Carnegie Mellon University Pittsburgh, PA 15213 pileggi@andrew.cmu.edu",True,"The main content of the paper, from the Abstract (page 1) through the Conclusion (page 11), spans 11 pages.",,,,,True,"The main content of the paper, from the Abstract (page 1) through the Conclusion (page 11), spans 11 pages.",False
data/iclr/data/submission_JKMoLnT1wZ,1.0,1.0,1.0,51995,2510,64.62258005142212,,,False,,,,,,False,,False,,False,,False,"The paper proposes SEAL-Pose, an adaptation of the Structured Energy As Loss (SEAL) framework for deterministic models, specifically designed to enhance 3D human pose estimation from 2D keypoints. Our approach enables a pose estimation model to learn joint dependencies via learning signals from a structured energy network that automatically captures body structure during training without explicit prior structural knowledge, resulting in more accurate and plausible 3D poses. This work highlights the potential of applying structured energy networks to tasks requiring complex output structures, offering a promising direction for future research.",False,,False,,,,,,False,,False,"The main content of the paper (Abstract through 7 Limitations) spans 10 pages, which is within the allowed 6-10 page limit. Line numbers are present on all pages. Visual inspection of margins, font sizes, and spacing in the main body text, abstract, and headings indicates adherence to the ICLR 2025 style guide specifications (e.g., 10pt font with 11pt vertical spacing for main text, 1.5 inch left margin). There is no evidence of extensive use of smaller fonts or reduced spacing to fit more content.",False,"The main content of the paper (Abstract through Limitations) spans 10 pages, which is within the allowed 6-10 page limit. Line numbers are present on all pages. Margins, font sizes, and spacing for headings, paragraphs, figures, and tables appear to adhere to the ICLR 2025 style guide. No explicit Ethics Statement or Reproducibility Statement sections were found to check for page limits.",False,"The main content (Abstract through Limitations) spans 10 pages (from page 1 to page 10 of the PDF), which is within the allowed 6-10 page limit. Line numbers are present on all pages. Visual inspection of the PDF against the style guide template (iclr2025_conference.pdf) shows consistent margins, font sizes, and paragraph spacing. No evidence of 'space-cheating' or extensive use of smaller fonts in the main body was found. No explicit Ethics or Reproducibility statements were identified to check against a 1-page limit.",False,"The main content of the paper (Abstract through 7 Limitations) spans 10 pages, which is within the allowed 6-10 page limit. Line numbers are present on all pages. Visual inspection of margins, font sizes, and spacing in the main body text, abstract, and headings indicates adherence to the ICLR 2025 style guide specifications (e.g., 10pt font with 11pt vertical spacing for main text, 1.5 inch left margin). There is no evidence of extensive use of smaller fonts or reduced spacing to fit more content.",False
data/iclr/data/submission_z5Th95xtBW,0.0,0.0,0.0,69514,2682,94.36178827285768,,,False,,,,,,False,,False,"The paper investigates syntactic structure representations in Large Language Models (LLMs) and the human brain using a novel method called Hierarchical Frequency Tagging Probe (HFTP). It compares LLM computational modules (MLP neurons) with human cortical areas, utilizing frequency-domain analysis and representational similarity analysis (RSA). This falls under 'representation learning' and 'applications of deep learning in natural language processing' and 'neuroscience'. The language is clear and professional, allowing a domain expert to follow the technical ideas.",,,,,False,"The paper investigates syntactic structure representations in Large Language Models (LLMs) and the human brain using a novel method called Hierarchical Frequency Tagging Probe (HFTP). It compares LLM computational modules (MLP neurons) with human cortical areas, utilizing frequency-domain analysis and representational similarity analysis (RSA). This falls under 'representation learning' and 'applications of deep learning in natural language processing' and 'neuroscience'. The language is clear and professional, allowing a domain expert to follow the technical ideas.",False,,False,,False,,False,,False,"The document adheres to all specified formatting and layout guidelines, including page limits, statement limits, line numbering, and margin/spacing requirements. No evidence of 'space-cheating' or extensive use of small fonts in the main body was found.",False,"The document adheres to all specified formatting and layout guidelines. The main content is 9 pages, which is within the 6-10 page limit. Line numbers are present on all pages. There are no Ethics or Reproducibility Statements, so no page limit violations for those. Margins, spacing, and font usage appear consistent with the ICLR 2025 style guide, with no evidence of 'space-cheating' or unauthorized modifications to the template parameters.",False,"The main content of the paper (Abstract through Conclusion) spans 9 pages (pages 1-9), which is within the allowed 6-10 page limit. Line numbers are present in the left margin on all pages. No explicit Ethics or Reproducibility Statements exceeding 1 page were found. The LaTeX source and PDF visually confirm adherence to margin, font size, and spacing guidelines, with no evidence of 'space-cheating' or extensive use of smaller fonts in the main body text.",False,"The document adheres to all specified formatting and layout guidelines, including page limits, statement limits, line numbering, and margin/spacing requirements. No evidence of 'space-cheating' or extensive use of small fonts in the main body was found.",False
data/iclr/data/submission_8sKXFvSCqA,1.0,1.0,0.7857139149588646,84933,2592,92.31436371803284,,,False,,,,,,False,,False,"The paper introduces ""Neural Fourier Modelling (NFM), a compact yet powerful solution for time-series analysis."" It applies deep learning techniques (Learnable Frequency Tokens, Implicit Neural Fourier Filters) to time-series data in the Fourier domain for tasks such as forecasting, anomaly detection, and classification. The language, while technical, is clear and understandable, allowing a domain expert to follow the main contributions and methodology.",False,"The paper introduces ""Neural Fourier Modelling (NFM), a compact yet powerful solution for time-series analysis"" that operates directly in the Fourier domain for tasks like forecasting, anomaly detection, and classification. It details modules like ""Learnable Frequency Tokens (LFT)"" and ""Implicit Neural Fourier Filters (INFF)"". The language, while technically dense, is comprehensible to a domain expert.",False,,False,,True,Our code is publicly available at: https://github.com/minkiml/NFM.,,,,,True,Our code is publicly available at: https://github.com/minkiml/NFM.,False,"The main content of the paper (Abstract through Conclusion) spans 10 pages, which is within the allowed limit of 6-10 pages. No explicit Ethics or Reproducibility Statements were found. Line numbers are present on all pages in the left margin. Visual inspection of margins, font sizes, and spacing for abstract, headings, main body, figures, and tables indicates adherence to the ICLR 2025 style guide specifications. No evidence of space-cheating or extensive use of small fonts in the main body was observed.",False,"The submission (main_paper.pdf) adheres to all specified formatting and layout guidelines. Line numbers are present on all pages. The main text (Abstract through Conclusion) is 10 pages long, which is within the 6-10 page limit. No explicit Ethics or Reproducibility Statements were found. Margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of smaller fonts in the main body.",False,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 10, which is exactly 10 pages, adhering to the page limit. Line numbers are present on all pages. The formatting, including margins, font sizes, and paragraph spacing, appears consistent with the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of small fonts in the main body. No separate Ethics or Reproducibility Statements exceeding one page were found.",False,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 10, which is exactly 10 pages, adhering to the page limit. Line numbers are present on all pages. The formatting, including margins, font sizes, and paragraph spacing, appears consistent with the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of small fonts in the main body. No separate Ethics or Reproducibility Statements exceeding one page were found.",False
data/iclr/data/submission_vrJtg5L3Qd,1.0,1.0,1.0,128788,1866,29.924006700515747,,,False,,,,,,False,,False,"Understanding the training dynamics of deep neural networks is challenging due to their high-dimensional nature and intricate loss landscapes. Recent studies have revealed that, along the training trajectory, the gradient approximately aligns with a low-rank top eigenspace of the training loss Hessian, referred to as the dominant subspace. Given this alignment, this paper explores whether neural networks can be trained within the dominant subspace, which, if feasible, could lead to more efficient training methods. Our primary observation is that when the SGD update is projected onto the dominant subspace, the training loss does not decrease further. This suggests that the observed alignment between the gradient and the dominant subspace is spurious. Surprisingly, projecting out the dominant subspace proves to be just as effective as the original update, despite removing the majority of the original update component. We observe similar behavior across practical setups, including the large learning rate regime (also known as Edge of Stability), Sharpness-Aware Minimization, momentum, and adaptive optimizers. We discuss the main causes and implications of this spurious alignment, shedding light on the dynamics of neural network training.",,,,,False,"Understanding the training dynamics of deep neural networks is challenging due to their high-dimensional nature and intricate loss landscapes. Recent studies have revealed that, along the training trajectory, the gradient approximately aligns with a low-rank top eigenspace of the training loss Hessian, referred to as the dominant subspace. Given this alignment, this paper explores whether neural networks can be trained within the dominant subspace, which, if feasible, could lead to more efficient training methods. Our primary observation is that when the SGD update is projected onto the dominant subspace, the training loss does not decrease further. This suggests that the observed alignment between the gradient and the dominant subspace is spurious. Surprisingly, projecting out the dominant subspace proves to be just as effective as the original update, despite removing the majority of the original update component. We observe similar behavior across practical setups, including the large learning rate regime (also known as Edge of Stability), Sharpness-Aware Minimization, momentum, and adaptive optimizers. We discuss the main causes and implications of this spurious alignment, shedding light on the dynamics of neural network training.",False,,,,,,False,,False,"The main content of the paper (Abstract through Conclusion/Discussion) spans from page 1 to page 10, which is exactly 10 pages and adheres to the specified page limit. The Reproducibility Statement on page 11 is less than one page. All pages include line numbers in the left margin. Visual inspection of margins, font sizes, and paragraph spacing throughout the document, including the abstract, main body, figures, and tables, indicates adherence to the ICLR 2025 style guide. No evidence of 'space-cheating' or extensive use of smaller fonts in the main text was found.",,,,,False,"The main content of the paper (Abstract through Conclusion/Discussion) spans from page 1 to page 10, which is exactly 10 pages and adheres to the specified page limit. The Reproducibility Statement on page 11 is less than one page. All pages include line numbers in the left margin. Visual inspection of margins, font sizes, and paragraph spacing throughout the document, including the abstract, main body, figures, and tables, indicates adherence to the ICLR 2025 style guide. No evidence of 'space-cheating' or extensive use of smaller fonts in the main text was found.",False
data/iclr/data/submission_O9W9DesXid,0.0,0.0,0.0,61768,2333,42.24931859970093,,,False,,,,,,False,,False,"The submission introduces MU-Bench, a comprehensive machine unlearning benchmark covering multiple tasks, data modalities, base models, and standardized evaluation metrics, all compiled into an easy-to-use package with a leader board to enable robust and scalable MU research. It covers applications of deep learning in vision, audio, speech, and natural language processing.",False,,,,False,,False,,False,,,,False,,False,"The main paper (main_paper.pdf) adheres to all specified formatting and layout rules. The main text is 9 pages long (Abstract through Conclusion/Discussion), which is within the 6-10 page limit. Line numbers are present on all pages. Margins, font size, and paragraph spacing appear consistent with the ICLR 2025 style guide specifications (e.g., 1.5 inch left margin, 1 inch top margin, 5.5 inch text width, 10pt font with 11pt vertical spacing for main text, 1/2 line space between paragraphs). There is no evidence of extensive use of smaller fonts or narrowed margins in the main body text. Optional statements like Ethics or Reproducibility are not present, thus no violation of their page limits.",False,"The main content (Abstract through Conclusion/Discussion) spans 9 pages, which is within the allowed 6-10 page limit. Line numbers are present on all pages in the left margin. Visual inspection of margins, font size, vertical spacing, and paragraph separation indicates adherence to the ICLR 2025 style guide specifications. No explicit Ethics Statement or Reproducibility Statement sections were found to check against a 1-page limit.",,,False,"The main content (Abstract through Conclusion/Discussion) spans 9 pages, which is within the allowed 6-10 page limit. Line numbers are present on all pages in the left margin. Visual inspection of margins, font size, vertical spacing, and paragraph separation indicates adherence to the ICLR 2025 style guide specifications. No explicit Ethics Statement or Reproducibility Statement sections were found to check against a 1-page limit.",False
data/iclr/data/submission_lX8DahcJbL,0.0,1.0,0.0,5088582,2708,479.3580627441406,,,False,,,,,,False,,False,"The submission proposes “pMixFed”, a dynamic, layer-wise PFL approach integrating mixup between shared global and personalized local models. We develop adaptive partitioning between shared and personalized layers of the model, gradual transition of personalization to allow seamless adaptation of local clients, improved generalization across clients, and mitigation of catastrophic forgetting. We provide theoretical analysis of pMixFed. Further, we conduct extensive experiments to demonstrate its superior performance compared with the existing PFL methods.",False,"Abstract: ""Federated Learning enables decentralized collaborative learning of machine learning models which presents challenges such as data privacy and client drift for heterogeneous data. Traditional FL methods offer strong generalization but lack personalized solutions for non-IID data. Personalized federated learning (PFL) addresses data heterogeneity by tackling these issues through balancing generalization and personalization level. It, however, still faces challenges such as optimal model partitioning and catastrophic forgetting that reduce quality and accuracy of both local and global models. To address these challenges, we propose ""pMixFed"", a dynamic, layer-wise PFL approach integrating mixup between shared global and personalized local models. We develop adaptive partitioning between shared and personalized layers of the model, gradual transition of personalization to allow seamless adaptation of local clients, improved generalization across clients, and mitigation of catastrophic forgetting. We provide theoretical analysis of pMixFed. Further, we conduct extensive experiments to demonstrate its superior performance compared with the existing PFL methods. Empirical results hows faster training, increased robustness, and improved handling of heterogeneity when using pMixFed as compared with the state-of-the-art PFL models.""",,,False,"Abstract: ""Federated Learning enables decentralized collaborative learning of machine learning models which presents challenges such as data privacy and client drift for heterogeneous data. Traditional FL methods offer strong generalization but lack personalized solutions for non-IID data. Personalized federated learning (PFL) addresses data heterogeneity by tackling these issues through balancing generalization and personalization level. It, however, still faces challenges such as optimal model partitioning and catastrophic forgetting that reduce quality and accuracy of both local and global models. To address these challenges, we propose ""pMixFed"", a dynamic, layer-wise PFL approach integrating mixup between shared global and personalized local models. We develop adaptive partitioning between shared and personalized layers of the model, gradual transition of personalization to allow seamless adaptation of local clients, improved generalization across clients, and mitigation of catastrophic forgetting. We provide theoretical analysis of pMixFed. Further, we conduct extensive experiments to demonstrate its superior performance compared with the existing PFL methods. Empirical results hows faster training, increased robustness, and improved handling of heterogeneity when using pMixFed as compared with the state-of-the-art PFL models.""",True,"file:///home/ktietz/src/ci/backcall_1611930011877/work, https://github.com/YasMinSdt/FedAlt/blob/master/data/partition/sort_and_partition.py#L2????, https://github.com/jhoon-oh/FedBABU/blob/master/models/Nets.py",,,,,True,"file:///home/ktietz/src/ci/backcall_1611930011877/work, https://github.com/YasMinSdt/FedAlt/blob/master/data/partition/sort_and_partition.py#L2????, https://github.com/jhoon-oh/FedBABU/blob/master/models/Nets.py",False,"The document adheres to the ICLR 2025 style guide. The main text (Abstract through Conclusion/Discussion) is 10 pages, which is within the 6-10 page limit. Line numbers are present. Margins, font sizes, and spacing appear to conform to the guidelines, with no evidence of 'space-cheating' or extensive use of small fonts in the main body. The Reproducibility Statement is less than one page.",False,"The main content of the paper (Abstract through Conclusion) spans 10 pages, which adheres to the strict upper limit of 10 pages for the main text as specified in the ICLR 2025 style guide (iclr2025_conference.pdf, page 2, lines 060-061). Line numbers are present in the left margin. The Reproducibility Statement is well under one page. No extensive use of smaller fonts or narrowed margins was observed in the main body text.",False,"The main text of the submission (Abstract through Conclusion) spans 10 pages, which is within the allowed limit of 6-10 pages. The Reproducibility Statement is less than one page. Line numbers are present on all pages. Visual inspection of the document, cross-referenced with the ICLR 2025 style guide, indicates adherence to specified font sizes, line spacing, paragraph indentation, and margin requirements. There is no evidence of 'space-cheating' or other formatting manipulations.",False,"The document adheres to the ICLR 2025 style guide. The main text (Abstract through Conclusion/Discussion) is 10 pages, which is within the 6-10 page limit. Line numbers are present. Margins, font sizes, and spacing appear to conform to the guidelines, with no evidence of 'space-cheating' or extensive use of small fonts in the main body. The Reproducibility Statement is less than one page.",False
data/iclr/data/submission_EGjTCIcSnW,0.0,0.0,0.0,51103,2185,51.50087881088257,,,False,,,,,,False,,False,"Although vision-language models (VLMs) have achieved significant success in various applications such as visual question answering, their resilience to prompt distractions remains as an under-explored area. Understanding how distractions affect VLMs is crucial for improving their real-world applicability, as inputs could be filled with noisy and irrelevant information in many practical scenarios. This paper aims to assess the robustness of VLMs against both visual and textual distractions in the context of science question answering. Built on the ScienceQA dataset, we developed a new benchmark that introduces distractions in both the visual and textual contexts.",,,,,False,"Although vision-language models (VLMs) have achieved significant success in various applications such as visual question answering, their resilience to prompt distractions remains as an under-explored area. Understanding how distractions affect VLMs is crucial for improving their real-world applicability, as inputs could be filled with noisy and irrelevant information in many practical scenarios. This paper aims to assess the robustness of VLMs against both visual and textual distractions in the context of science question answering. Built on the ScienceQA dataset, we developed a new benchmark that introduces distractions in both the visual and textual contexts.",False,,,,,,False,,False,"The document adheres to all specified formatting and layout guidelines. Line numbers are present, page limits for the main content (10 pages) are met, and there are no explicit Ethics or Reproducibility Statements exceeding one page. Margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of smaller fonts in the main body.",False,"The main content of the paper (Abstract through Limitations and Conclusion) spans from page 1 to page 10, totaling 10 pages, which is within the allowed limit of 6-10 pages. Line numbers are present on all pages. Visual inspection of the PDF indicates that margins, font sizes, and spacing adhere to the specifications outlined in the ICLR 2025 style guide (iclr2025_conference.pdf). Small fonts are appropriately used for tables and references, which is permissible. No separate Ethics or Reproducibility Statements exceeding one page were identified.",,,False,"The main content of the paper (Abstract through Limitations and Conclusion) spans from page 1 to page 10, totaling 10 pages, which is within the allowed limit of 6-10 pages. Line numbers are present on all pages. Visual inspection of the PDF indicates that margins, font sizes, and spacing adhere to the specifications outlined in the ICLR 2025 style guide (iclr2025_conference.pdf). Small fonts are appropriately used for tables and references, which is permissible. No separate Ethics or Reproducibility Statements exceeding one page were identified.",False
data/iclr/data/submission_Uc2P6WIgoo,1.0,1.0,1.0,35223,1788,32.44314384460449,,,True,All the source code and data would be posted after peer review.,,,,,True,All the source code and data would be posted after peer review.,False,"The paper proposes a cell-embedded GNN model (CeGNN) to learn spatiotemporal dynamics, introducing a learnable cell attribution and a novel feature-enhanced block. It applies deep learning techniques to solve Partial Differential Equations (PDEs) in various scientific and engineering fields, such as weather prediction, ocean current motion, and material mechanical properties simulation. The methodology involves representation learning on graph structures, which aligns with ICLR's scope.",,,,,False,"The paper proposes a cell-embedded GNN model (CeGNN) to learn spatiotemporal dynamics, introducing a learnable cell attribution and a novel feature-enhanced block. It applies deep learning techniques to solve Partial Differential Equations (PDEs) in various scientific and engineering fields, such as weather prediction, ocean current motion, and material mechanical properties simulation. The methodology involves representation learning on graph structures, which aligns with ICLR's scope.",False,,,,,,False,,False,"The main content (Abstract through Conclusion) spans from page 1 to page 10, which is exactly 10 pages. Line numbers are present on every page in the left margin. Margins, font sizes, and spacing appear to adhere to the ICLR 2025 style guide. No explicit Ethics Statement or Reproducibility Statement sections were found, and no evidence of 'vspace' abuse or extensive use of small fonts in the main body was observed.",,,,,False,"The main content (Abstract through Conclusion) spans from page 1 to page 10, which is exactly 10 pages. Line numbers are present on every page in the left margin. Margins, font sizes, and spacing appear to adhere to the ICLR 2025 style guide. No explicit Ethics Statement or Reproducibility Statement sections were found, and no evidence of 'vspace' abuse or extensive use of small fonts in the main body was observed.",False
data/iclr/data/submission_TIxiwxd4iD,0.0,1.0,0.0,15506,1548,16.692325592041016,,,False,,,,,,False,,False,"This paper presents a comprehensive exploration into the classification of Bengali music genres, utilizing a novel dataset, 'BanglaGITI: Bangla Genre-wise Indexed Tracks and Interpretations', specifically curated to capture the rich diversity of Bengali musical heritage. Our study is structured around a comparative analysis of traditional Machine Learning (ML) techniques, advanced Deep Learning (DL) methodologies, and innovative ensemble approaches that integrate the strengths of both ML and DL through Transfer Learning.",,,,,False,"This paper presents a comprehensive exploration into the classification of Bengali music genres, utilizing a novel dataset, 'BanglaGITI: Bangla Genre-wise Indexed Tracks and Interpretations', specifically curated to capture the rich diversity of Bengali musical heritage. Our study is structured around a comparative analysis of traditional Machine Learning (ML) techniques, advanced Deep Learning (DL) methodologies, and innovative ensemble approaches that integrate the strengths of both ML and DL through Transfer Learning.",False,,,,,,False,,True,"The provided main_paper.pdf contains only 1 page of content. The ICLR 2025 style guide states: ""There will be a strict upper limit of 10 pages for the main text of the initial submission, with unlimited additional pages for citations."" While it specifies an upper limit, the common understanding and implicit requirement for a research paper is to have a minimum number of pages to present substantial work, typically 6 pages for ICLR main text. A single page is far below any reasonable minimum for a full paper submission.",,,,,True,"The provided main_paper.pdf contains only 1 page of content. The ICLR 2025 style guide states: ""There will be a strict upper limit of 10 pages for the main text of the initial submission, with unlimited additional pages for citations."" While it specifies an upper limit, the common understanding and implicit requirement for a research paper is to have a minimum number of pages to present substantial work, typically 6 pages for ICLR main text. A single page is far below any reasonable minimum for a full paper submission.",False
data/iclr/data/submission_9pDbWbJzC4,1.0,1.0,1.0,32679,2021,34.51603364944458,,,False,,,,,,False,,False,"The paper introduces SAGEPhos, a novel framework for phosphorylation site prediction, utilizing deep learning models, representation learning, and multimodal fusion techniques. This falls under 'applications of deep learning in ... any other field' (specifically computational biology/bioinformatics) and 'representation learning' as per ICLR 2025 scope. The language used throughout the abstract, introduction, and methodology sections is clear, coherent, and technically precise, allowing a domain expert to follow the logical and mathematical flow without significant impediments.",False,"Phosphorylation site prediction based on kinase-substrate interaction plays a vital role in understanding cellular signaling pathways and disease mechanisms. Computational methods for this task can be categorized into kinase-family-focused and individual kinase-targeted approaches. Individual kinase-targeted methods have gained prominence for their ability to explore a broader protein space and provide more precise target information for kinase inhibitors. However, most existing individual kinase-based approaches focus solely on sequence inputs, neglecting crucial structural information. To address this limitation, we introduce SAGEPhos (Structure-aware kinAse-substrate bio-coupled and bio-auGmented nEtwork for Phosphorylation site prediction), a novel framework that modifies the semantic space of main protein inputs using auxiliary inputs at two distinct modality levels.",,,False,"Phosphorylation site prediction based on kinase-substrate interaction plays a vital role in understanding cellular signaling pathways and disease mechanisms. Computational methods for this task can be categorized into kinase-family-focused and individual kinase-targeted approaches. Individual kinase-targeted methods have gained prominence for their ability to explore a broader protein space and provide more precise target information for kinase inhibitors. However, most existing individual kinase-based approaches focus solely on sequence inputs, neglecting crucial structural information. To address this limitation, we introduce SAGEPhos (Structure-aware kinAse-substrate bio-coupled and bio-auGmented nEtwork for Phosphorylation site prediction), a novel framework that modifies the semantic space of main protein inputs using auxiliary inputs at two distinct modality levels.",False,,,,,,False,,False,"The main content (Abstract through Conclusion) spans from page 1 to page 10, which is exactly 10 pages, adhering to the specified limit. Line numbers are present on all pages in the left margin. No explicit Ethics or Reproducibility Statements were found. The document's formatting, including margins, font sizes, and line spacing, appears consistent with the ICLR 2025 style guide as demonstrated by the provided iclr2025_conference.pdf itself.",,,,,False,"The main content (Abstract through Conclusion) spans from page 1 to page 10, which is exactly 10 pages, adhering to the specified limit. Line numbers are present on all pages in the left margin. No explicit Ethics or Reproducibility Statements were found. The document's formatting, including margins, font sizes, and line spacing, appears consistent with the ICLR 2025 style guide as demonstrated by the provided iclr2025_conference.pdf itself.",False
data/iclr/data/submission_ChbCYd1uk5,0.0,0.0,0.0,736427,2307,145.2055549621582,,,False,,,,,,False,,False,"The submission's abstract, introduction, and methodology clearly describe research on low-dimensional graph embeddings, representation learning for graph-based machine learning tasks (node classification, link prediction, community detection), and efficient optimization techniques for finding intrinsic dimensionality. These topics align directly with ICLR's scope, including 'unsupervised, semi-supervised, and supervised representation learning', 'metric learning', 'optimization for representation learning', and 'applications of deep learning in ... any other field'. The language used is clear, coherent, and technically sound, allowing a domain expert to follow the logical and mathematical flow without critical impediments.",False,"The submission's abstract, introduction, and methodology sections are well-structured and use clear, standard scientific language. The technical concepts are presented coherently, and the mathematical notation is consistent and understandable within the domain of graph representation learning. There are no instances of critically garbled text, broken sentence structures, or fundamental internal contradictions that would impede a domain expert's understanding of the work.",False,"The submission focuses on low-dimensional embeddings, graph representation learning, metric learning, hierarchical models, and efficient optimization techniques for large-scale networks. The language is clear, and the technical content is comprehensible.",False,"The submission focuses on low-dimensional embeddings, graph representation learning, metric learning, hierarchical models, and efficient optimization techniques for large-scale networks. The language is clear, and the technical content is comprehensible.",False,,,,,,False,,False,"The main content of the paper (Abstract through Conclusion) spans exactly 10 pages (Page 1 to Page 10 of the main_paper.pdf), which is within the allowed limit of 6-10 pages. Line numbers are present on all pages. Ethics and Reproducibility statements are well within the 1-page limit each. Margins, font sizes, and spacing appear to adhere to the ICLR 2025 style guide based on visual inspection.",True,"The main content of the paper, from the Abstract (Page 1) through the Limitations and Broader Impact section (Section 4.1 on Page 11), spans 11 pages. The ICLR 2025 style guide specifies that the main text (Abstract through Conclusion/Discussion) must be between 6 and 10 pages. This submission exceeds the 10-page limit by 1 full page.",,,True,"The main content of the paper, from the Abstract (Page 1) through the Limitations and Broader Impact section (Section 4.1 on Page 11), spans 11 pages. The ICLR 2025 style guide specifies that the main text (Abstract through Conclusion/Discussion) must be between 6 and 10 pages. This submission exceeds the 10-page limit by 1 full page.",False
data/iclr/data/submission_Rd34VDwqB7,0.0,0.0,0.0,34337,2108,57.85529255867005,,,False,,,,,,False,,False,"This paper presents a multi-modal generative framework designed for frequent temporal stream imagery datasets, aimed at generating the subsequent stream images. This task is challenging due to the variability of stream images caused by changes in time and local environmental conditions. Our method captures scene changes in both stream and surrounding environment by incorporating temporal context of weather, water flow, and time information. We also introduce a domain-discriminative learning approach to enforce the learning of domain-specific information in generating images.",,,,,False,"This paper presents a multi-modal generative framework designed for frequent temporal stream imagery datasets, aimed at generating the subsequent stream images. This task is challenging due to the variability of stream images caused by changes in time and local environmental conditions. Our method captures scene changes in both stream and surrounding environment by incorporating temporal context of weather, water flow, and time information. We also introduce a domain-discriminative learning approach to enforce the learning of domain-specific information in generating images.",False,,,,,,False,,False,"The main content of the paper (Abstract through Limitations and Future Works) spans 10 pages (pages 1-10), which adheres to the strict upper limit of 10 pages specified in the ICLR 2025 style guide. References start on page 11 and do not count towards the page limit. Line numbers are present on all pages. The formatting, including margins, font size, and spacing, appears consistent with the provided style guide document, with no obvious signs of 'space-cheating' or extensive use of smaller fonts in the main body. No Ethics or Reproducibility Statements were found to check against a page limit.",False,"The main content (Abstract through Conclusion) spans from page 1 to page 8, totaling 8 pages. Line numbers are present on all pages. No explicit Ethics or Reproducibility Statements were found. The document's margins, spacing, and font usage appear consistent with the style guide, and no 'space-cheating' or extensive use of small fonts in the main body was detected. Although the style guide text states the paper title should be 'left-aligned', the provided style guide PDF itself (iclr2025_conference.pdf) and the LaTeX template (iclr2025_conference.tex) use a centered title, which the submitted paper also follows.",,,False,"The main content (Abstract through Conclusion) spans from page 1 to page 8, totaling 8 pages. Line numbers are present on all pages. No explicit Ethics or Reproducibility Statements were found. The document's margins, spacing, and font usage appear consistent with the style guide, and no 'space-cheating' or extensive use of small fonts in the main body was detected. Although the style guide text states the paper title should be 'left-aligned', the provided style guide PDF itself (iclr2025_conference.pdf) and the LaTeX template (iclr2025_conference.tex) use a centered title, which the submitted paper also follows.",False
data/iclr/data/submission_DXz1PDA0Wg,0.0,1.0,0.0,17525,2130,32.37832188606262,,,False,,,,,,False,,False,"The paper focuses on ""Synthetic Data Generation through Conditional GANs for Improving Accuracy in Medicinal Leaf Classification."" It employs Conditional Generative Adversarial Networks (CGANs) for data augmentation and deep convolutional neural networks (ResNet-34, VGG-16, EfficientNet-B1) for classification. The abstract clearly outlines the problem, methodology, and results.",False,"This study explores the application of Conditional Generative Adversarial Networks (CGANs) to generate synthetic data aimed at improving medicinal leaf classification models. CGANs offer effective solution for augmenting datasets and addressing class imbalance issues. We employed a conditional Deep Convolution Generative Adversarial Network (cDCGAN) to produce 500 synthetic images for each of thirty different plant species. To evaluate the effectiveness of the generated data, we trained and evaluated three popular convolutional neural networks: ResNet-34, VGG-16, and EfficientNet-B1, on both the original and augmented datasets.",,,False,"This study explores the application of Conditional Generative Adversarial Networks (CGANs) to generate synthetic data aimed at improving medicinal leaf classification models. CGANs offer effective solution for augmenting datasets and addressing class imbalance issues. We employed a conditional Deep Convolution Generative Adversarial Network (cDCGAN) to produce 500 synthetic images for each of thirty different plant species. To evaluate the effectiveness of the generated data, we trained and evaluated three popular convolutional neural networks: ResNet-34, VGG-16, and EfficientNet-B1, on both the original and augmented datasets.",True,"Herb-GANNet: Synthetic Data Generation through Conditional GANs for Improving Accuracy in Medicinal Leaf Classification Smitha Reddy Sª, Vaishnavi Kb, Sapna RC* @Department of Computer Science, School of Computer Science and Engineering and Information Science, Presidency University, Bengaluru, India bAssociate 1 software engineer, Capegemini, Bengaluru, India C* *Department of Information Technology, Manipal Institute of Technology Bengaluru, Manipal Academy of Higher Education, Manipal, India Corresponding author: Sapna R Email: sapna.r@manipal.edu",,,,,True,"Herb-GANNet: Synthetic Data Generation through Conditional GANs for Improving Accuracy in Medicinal Leaf Classification Smitha Reddy Sª, Vaishnavi Kb, Sapna RC* @Department of Computer Science, School of Computer Science and Engineering and Information Science, Presidency University, Bengaluru, India bAssociate 1 software engineer, Capegemini, Bengaluru, India C* *Department of Information Technology, Manipal Institute of Technology Bengaluru, Manipal Academy of Higher Education, Manipal, India Corresponding author: Sapna R Email: sapna.r@manipal.edu",True,"The submitted PDF is only 1 page long, containing only the title, authors, abstract, and keywords. It lacks any main content sections (e.g., Introduction, Methods, Results, Discussion, Conclusion).",,,,,True,"The submitted PDF is only 1 page long, containing only the title, authors, abstract, and keywords. It lacks any main content sections (e.g., Introduction, Methods, Results, Discussion, Conclusion).",False
data/iclr/data/submission_1KRM3igwn1,1.0,1.0,1.0,60529,2122,82.08802771568298,,,False,,,,,,False,,False,"The main content (Abstract through Section 9 'LIMITATIONS AND FUTURE SCOPE') spans from page 1 to page 10, which is within the allowed 6-10 page limit. The optional Ethics Statement (Section A 'ETHICAL CONSIDERATIONS' on page 17) is less than one page. There is no separate Reproducibility Statement exceeding one page. Line numbers are present in the left margin on all pages. Visual inspection and review of the LaTeX source indicate adherence to font sizes, margins, and spacing guidelines without any signs of 'space-cheating' or extensive use of small fonts in the main body.",,,,,False,"The main content (Abstract through Section 9 'LIMITATIONS AND FUTURE SCOPE') spans from page 1 to page 10, which is within the allowed 6-10 page limit. The optional Ethics Statement (Section A 'ETHICAL CONSIDERATIONS' on page 17) is less than one page. There is no separate Reproducibility Statement exceeding one page. Line numbers are present in the left margin on all pages. Visual inspection and review of the LaTeX source indicate adherence to font sizes, margins, and spacing guidelines without any signs of 'space-cheating' or extensive use of small fonts in the main body.",False,,,,,,False,,False,,False,"The paper introduces ProFS (Projection Filter for Subspaces), a tuning-free alignment alternative for large language models (LLMs) focusing on toxicity reduction. This involves identifying and projecting out a 'toxic subspace' in the model's parameter space. The methodology leverages representation learning, singular value decomposition (SVD), and connects to direct preference optimization (DPO).",False,"The paper introduces ProFS, a tuning-free alignment alternative for large language models (LLMs) to reduce toxicity. This involves identifying and projecting out a 'toxic subspace' in the model's parameter space. The methodology is grounded in factor analysis and uses singular value decomposition (SVD) to modify MLP-value weights. This falls under 'representation learning' and 'applications of deep learning in natural language processing'. The abstract, introduction, and methodology sections clearly explain the problem, proposed solution, and technical details using standard scientific language and notation.",False,"The paper introduces ProFS (Projection Filter for Subspaces), a tuning-free alignment alternative for large language models (LLMs) focusing on toxicity reduction. This involves identifying and projecting out a 'toxic subspace' in the model's parameter space. The methodology leverages representation learning, singular value decomposition (SVD), and connects to direct preference optimization (DPO).",False
data/iclr/data/submission_eIK4ojL2QM,1.0,1.0,0.576734666650059,27100,2055,232.0574333667755,,,True,"We initially proposed a novel method for constructing real networks from the complex domain using the Cauchy integral formula in Li et al. (2024); Zhang et al. (2024)... (Page 1, line 024) AND Xin Li, Zhihong Xia, and Hongkun Zhang. Cauchy activation function and xnet. arXiv preprint arXiv:2409.19221, 2024. (Page 11, line 574)",,,,,True,"We initially proposed a novel method for constructing real networks from the complex domain using the Cauchy integral formula in Li et al. (2024); Zhang et al. (2024)... (Page 1, line 024) AND Xin Li, Zhihong Xia, and Hongkun Zhang. Cauchy activation function and xnet. arXiv preprint arXiv:2409.19221, 2024. (Page 11, line 574)",False,"The main content of the paper (Abstract through Summary and Outlook) spans from page 1 to page 10, inclusive, which is exactly 10 pages. This falls within the allowed page limit of 6-10 pages for the main text. Line numbers are present on all pages in the left margin. The document's margins, font sizes, and paragraph spacing appear consistent with the ICLR 2025 style guide (iclr2025_conference.pdf and iclr2025_conference.tex). No extensive use of small fonts or condensed spacing was detected in the main body text. There are no separate Ethics or Reproducibility Statements that exceed the 1-page limit.",,,,,False,"The main content of the paper (Abstract through Summary and Outlook) spans from page 1 to page 10, inclusive, which is exactly 10 pages. This falls within the allowed page limit of 6-10 pages for the main text. Line numbers are present on all pages in the left margin. The document's margins, font sizes, and paragraph spacing appear consistent with the ICLR 2025 style guide (iclr2025_conference.pdf and iclr2025_conference.tex). No extensive use of small fonts or condensed spacing was detected in the main body text. There are no separate Ethics or Reproducibility Statements that exceed the 1-page limit.",False,,,,,,False,,False,"In the fields of computational mathematics and artificial intelligence, the need for precise data modeling is crucial, especially for predictive machine learning tasks. This paper explores further XNet, a novel algorithm that employs the complex-valued Cauchy integral formula, offering a superior network architecture that surpasses traditional Multi-Layer Perceptrons (MLPs) and Kolmogorov-Arnold Networks (KANs). XNet significant improves speed and accuracy across various tasks in both low and high-dimensional spaces, redefining the scope of data-driven model development and providing substantial improvements over established time series models like LSTMs.",,,,,False,"In the fields of computational mathematics and artificial intelligence, the need for precise data modeling is crucial, especially for predictive machine learning tasks. This paper explores further XNet, a novel algorithm that employs the complex-valued Cauchy integral formula, offering a superior network architecture that surpasses traditional Multi-Layer Perceptrons (MLPs) and Kolmogorov-Arnold Networks (KANs). XNet significant improves speed and accuracy across various tasks in both low and high-dimensional spaces, redefining the scope of data-driven model development and providing substantial improvements over established time series models like LSTMs.",False
data/iclr/data/submission_cIKQp84vqN,0.0,0.0,0.0,52937,2639,79.9703483581543,,,False,,,,,,False,,False,"The main text (Abstract through Conclusion) spans 10 pages, which is within the allowed limit of 6-10 pages. Line numbers are present on all pages. Margins and spacing appear consistent with the provided style guide PDF, with no evidence of 'space-cheating' or extensive use of small fonts in the main body. Optional Ethics and Reproducibility statements are not present as distinct sections.",False,"The document adheres to all specified formatting and layout guidelines, including page limits, statement limits, line numbering, and margin/spacing requirements. No instances of 'space-cheating' or extensive use of smaller fonts in the main body were detected.",False,"The main paper (main_paper.pdf) adheres to the ICLR 2025 style guide. Line numbers are present on all pages. The main content (Abstract through Conclusion) is exactly 10 pages, which falls within the allowed limit of 6-10 pages. No explicit Ethics or Reproducibility Statements were found. Margins, font sizes, and paragraph spacing appear consistent with the specifications in iclr2025_conference.pdf and iclr2025_conference.tex, with no evidence of 'space-cheating' or extensive use of smaller fonts for main body text.",False,"The main paper (main_paper.pdf) adheres to the ICLR 2025 style guide. Line numbers are present on all pages. The main content (Abstract through Conclusion) is exactly 10 pages, which falls within the allowed limit of 6-10 pages. No explicit Ethics or Reproducibility Statements were found. Margins, font sizes, and paragraph spacing appear consistent with the specifications in iclr2025_conference.pdf and iclr2025_conference.tex, with no evidence of 'space-cheating' or extensive use of smaller fonts for main body text.",False,,,,,,False,,False,"Point cloud registration is a critical and challenging task in computer vision. It is difficult to avoid poor local minima since the cost function is significantly non-convex. Correspondences tainted by significant or unknown outliers may cause the probability of finding a close-to-true transformation to drop rapidly, leading to point cloud registration failure. Many registration methods avoid local minima by updating the scale parameter of the cost function using graduated non-convexity (GNC). However, the update is usually performed in a fixed manner, resulting in limited accuracy and robustness of registration, and failure to reliably converge to the global minimum. Therefore, we present a novel method to robust point cloud registration based on Adaptive Graduated Non-Convexity (AGNC). By monitoring the positive definiteness of the Hessian of the cost function, the scale in graduated non-convexity is adaptively reduced without the need for a fixed optimization schedule. In addition, a multi-task knowledge sharing mechanism is used to achieve collaborative optimization of non-convex cost functions at different levels to further improve the success rate of point cloud registration under challenging high outlier conditions. Experimental results on simulated and real point cloud registration datasets show that AGNC far outperforms state-of-the-art methods in terms of robustness and accuracy, and can obtain promising registration results even in the case of extreme 99% outlier rates. To the best of our knowledge, this is the first study that explores point cloud registration considering adaptive graduated non-convexity.",,,,,False,"Point cloud registration is a critical and challenging task in computer vision. It is difficult to avoid poor local minima since the cost function is significantly non-convex. Correspondences tainted by significant or unknown outliers may cause the probability of finding a close-to-true transformation to drop rapidly, leading to point cloud registration failure. Many registration methods avoid local minima by updating the scale parameter of the cost function using graduated non-convexity (GNC). However, the update is usually performed in a fixed manner, resulting in limited accuracy and robustness of registration, and failure to reliably converge to the global minimum. Therefore, we present a novel method to robust point cloud registration based on Adaptive Graduated Non-Convexity (AGNC). By monitoring the positive definiteness of the Hessian of the cost function, the scale in graduated non-convexity is adaptively reduced without the need for a fixed optimization schedule. In addition, a multi-task knowledge sharing mechanism is used to achieve collaborative optimization of non-convex cost functions at different levels to further improve the success rate of point cloud registration under challenging high outlier conditions. Experimental results on simulated and real point cloud registration datasets show that AGNC far outperforms state-of-the-art methods in terms of robustness and accuracy, and can obtain promising registration results even in the case of extreme 99% outlier rates. To the best of our knowledge, this is the first study that explores point cloud registration considering adaptive graduated non-convexity.",False
data/iclr/data/submission_mhyl7HhNM5,0.0,0.0,0.0,66309,2037,68.57721710205078,,,True,"human experts, which are two of this paper's authors, fully familiar with the task, and by three human annotators per example via crowd-sourcing. (Page 4, Line 178)
These experts were two of this paper's authors, who are fully familiar with the guidelines and task characteristics. (Page 4, Line 208)",,,,,True,"human experts, which are two of this paper's authors, fully familiar with the task, and by three human annotators per example via crowd-sourcing. (Page 4, Line 178)
These experts were two of this paper's authors, who are fully familiar with the guidelines and task characteristics. (Page 4, Line 208)",False,"The main content (Abstract through Discussion) spans 10 pages, which is within the 6-10 page limit. The Ethics Statement and Reproducibility Statement are both under one page. Line numbers are present on all pages. The document appears to use the official ICLR 2025 LaTeX style file without modifications to margins, font sizes, or spacing in the main body, as confirmed by visual inspection of the PDF and review of the provided LaTeX source file.",False,"The main content (Abstract through Discussion) spans 10 pages, which is within the allowed limit of 6-10 pages. The Ethics Statement and Reproducibility Statement are both less than one page each. Line numbers are present on all pages. Margins, font sizes, and spacing appear to conform to the style guide specifications, with no evidence of 'space-cheating' or extensive use of small fonts for main body text.",False,"The main content (Abstract through Section 8 'DISCUSSION') spans exactly 10 pages, which adheres to the page limit of 6-10 pages. The Ethics Statement and Reproducibility Statement are both less than one page. Line numbers are present on all pages. Visual inspection of the PDF and review of the LaTeX source indicate adherence to specified margins, font sizes, and spacing without any signs of 'space-cheating' or extensive use of smaller fonts in the main body.",False,"The main content (Abstract through Discussion) spans 10 pages, which is within the allowed limit of 6-10 pages. The Ethics Statement and Reproducibility Statement are both less than one page each. Line numbers are present on all pages. Margins, font sizes, and spacing appear to conform to the style guide specifications, with no evidence of 'space-cheating' or extensive use of small fonts for main body text.",False,,,,,,False,,False,,,,,,False,,True
data/iclr/data/submission_NAlpAxxLAT,1.0,1.0,1.0,198817,1954,36.92087006568909,,,False,,,,,,False,,False,"The main content of the paper (Abstract through Conclusion and Future Work) spans pages 1-10 of the main_paper.pdf, which is exactly 10 pages, adhering to the 'strict upper limit of 10 pages'. The Reproducibility Statement is less than one page. Line numbers are present on all pages. There is no visual evidence of margin or font size manipulation in the main body text, and the LaTeX source indicates the use of the official ICLR 2025 style files without modifications to layout parameters.",,,,,False,"The main content of the paper (Abstract through Conclusion and Future Work) spans pages 1-10 of the main_paper.pdf, which is exactly 10 pages, adhering to the 'strict upper limit of 10 pages'. The Reproducibility Statement is less than one page. Line numbers are present on all pages. There is no visual evidence of margin or font size manipulation in the main body text, and the LaTeX source indicates the use of the official ICLR 2025 style files without modifications to layout parameters.",True,"## Add MASK for CN_reveal： - While move action`[0-4]` choose `[4]`, reveal action will have mask: `[1, 1, 1, 1, 0]`, - While move action`[0-4]` choose `[0,1,2,3]`, reveal action will have mask: `[0, 0, 0, 0, 1]`, - `Do mask need gradient?`",,,,,True,"## Add MASK for CN_reveal： - While move action`[0-4]` choose `[4]`, reveal action will have mask: `[1, 1, 1, 1, 0]`, - While move action`[0-4]` choose `[0,1,2,3]`, reveal action will have mask: `[0, 0, 0, 0, 1]`, - `Do mask need gradient?`",False,"Effective communication is an essential component in collaborative multi-agent systems. Situations where explicit messaging is not feasible have been common in human society throughout history, which motivate the study of implicit communication. Previous works on learning implicit communication mostly rely on theory of mind (ToM), where agents infer the mental states and intentions of others by interpreting their actions. However, ToM-based methods become less effective in making accurate inferences in complex tasks. In this work, we propose the Implicit Channel Protocol (ICP) framework, which allows agents to construct implicit communication channels similar to the explicit ones. ICP leverages a subset of actions, denoted as the scouting actions, and a mapping between information and these scouting actions that encodes and decodes the messages. We propose training algorithms for agents to message and act, including learning with a randomly initialized information map and with a delayed information map. The efficacy of ICP has been tested on the tasks of Guessing Number, Revealing Goals, and Hanabi, where ICP significantly outperforms baseline methods through more efficient information transmission.",,,,,False,"Effective communication is an essential component in collaborative multi-agent systems. Situations where explicit messaging is not feasible have been common in human society throughout history, which motivate the study of implicit communication. Previous works on learning implicit communication mostly rely on theory of mind (ToM), where agents infer the mental states and intentions of others by interpreting their actions. However, ToM-based methods become less effective in making accurate inferences in complex tasks. In this work, we propose the Implicit Channel Protocol (ICP) framework, which allows agents to construct implicit communication channels similar to the explicit ones. ICP leverages a subset of actions, denoted as the scouting actions, and a mapping between information and these scouting actions that encodes and decodes the messages. We propose training algorithms for agents to message and act, including learning with a randomly initialized information map and with a delayed information map. The efficacy of ICP has been tested on the tasks of Guessing Number, Revealing Goals, and Hanabi, where ICP significantly outperforms baseline methods through more efficient information transmission.",False
data/iclr/data/submission_GLKig15TWJ,0.0,0.0,0.0,52326,1982,54.03384256362915,,,False,"Anonymous authors
Paper under double-blind review",,,,,False,"Anonymous authors
Paper under double-blind review",False,"The main paper (main_paper.pdf) adheres to all specified formatting and layout guidelines. Line numbers are present on all pages. The main text content spans 10 pages (from Abstract on page 1 to the end of Section 5 on page 10), which is within the allowed 6-10 page limit. There are no explicit Ethics or Reproducibility Statements that exceed 1 page. Margins, font sizes, and spacing appear consistent with the ICLR 2025 style guide, with no evidence of 'space-cheating' or unauthorized modifications to the layout parameters.",True,"Paper title: ""EFFICIENT DIFFUSION MODELS FOR SYMMETRIC MANIFOLDS"" (main_paper.pdf, page 1). Style guide rule: ""Paper title is 17 point, in small caps and left-aligned."" (iclr2025_conference.pdf, page 1, lines 051-052).",,,True,"Paper title: ""EFFICIENT DIFFUSION MODELS FOR SYMMETRIC MANIFOLDS"" (main_paper.pdf, page 1). Style guide rule: ""Paper title is 17 point, in small caps and left-aligned."" (iclr2025_conference.pdf, page 1, lines 051-052).",False,,,,,,False,,False,"We present a framework for designing efficient diffusion models on symmetric Riemannian manifolds, which include the torus, sphere, special orthogonal group, and unitary group. While diffusion models on symmetric manifolds have gained significant attention, existing approaches often rely on the manifolds' heat kernels, which lack closed-form expressions and result in exponential-in-dimension per-iteration runtimes during training. We introduce a new diffusion model for symmetric-space manifolds, leveraging a projection of Euclidean Brownian motion to bypass explicit heat kernel computations. Our training algorithm minimizes a novel objective function derived via Ito's Lemma, with efficiently computable gradients, allowing each iteration to run in polynomial time for symmetric manifolds. Additionally, the symmetries of the manifold ensure the diffusion satisfies an ""average-case"" Lipschitz condition, enabling accurate and efficient sample generation. These improvements enhance both the training runtime and sample accuracy for key cases of symmetric manifolds, helping to bridge the gap between diffusion models on symmetric manifolds and Euclidean space.",,,,,False,"We present a framework for designing efficient diffusion models on symmetric Riemannian manifolds, which include the torus, sphere, special orthogonal group, and unitary group. While diffusion models on symmetric manifolds have gained significant attention, existing approaches often rely on the manifolds' heat kernels, which lack closed-form expressions and result in exponential-in-dimension per-iteration runtimes during training. We introduce a new diffusion model for symmetric-space manifolds, leveraging a projection of Euclidean Brownian motion to bypass explicit heat kernel computations. Our training algorithm minimizes a novel objective function derived via Ito's Lemma, with efficiently computable gradients, allowing each iteration to run in polynomial time for symmetric manifolds. Additionally, the symmetries of the manifold ensure the diffusion satisfies an ""average-case"" Lipschitz condition, enabling accurate and efficient sample generation. These improvements enhance both the training runtime and sample accuracy for key cases of symmetric manifolds, helping to bridge the gap between diffusion models on symmetric manifolds and Euclidean space.",True
data/iclr/data/submission_jCtT1vUWb7,0.0,0.0,0.0,34140,1642,99.30434608459473,,,False,,,,,,False,,True,"Style Guide (iclr2025_conference.pdf, page 2, lines 094-096): ""Indicate footnotes with a number in the text. Place the footnotes at the bottom of the page on which they appear. Precede the footnote with a horizontal rule of 2 inches (12 picas).""

Main Paper (main_paper.pdf, page 1, line 053): The footnote '¹' with text ""We will release our benchmark and evaluation code upon acceptance."" appears at the bottom of the page but lacks the preceding horizontal rule.",,,,,True,"Style Guide (iclr2025_conference.pdf, page 2, lines 094-096): ""Indicate footnotes with a number in the text. Place the footnotes at the bottom of the page on which they appear. Precede the footnote with a horizontal rule of 2 inches (12 picas).""

Main Paper (main_paper.pdf, page 1, line 053): The footnote '¹' with text ""We will release our benchmark and evaluation code upon acceptance."" appears at the bottom of the page but lacks the preceding horizontal rule.",True,ONLY FINISH THE SOLVER'S DIALOGUE. (Page 21) ONLY FINISH THE EXPERT'S DIALOGUE. (Page 21 and 22),,,,,True,ONLY FINISH THE SOLVER'S DIALOGUE. (Page 21) ONLY FINISH THE EXPERT'S DIALOGUE. (Page 21 and 22),False,The rapid advances of multi-modal agents built on large foundation models have largely overlooked their potential for language-based communication between agents in collaborative tasks. ... we introduce a novel benchmark designed to evaluate the collaborative performance of multimodal multi-agent systems through language communication.,,,,,False,The rapid advances of multi-modal agents built on large foundation models have largely overlooked their potential for language-based communication between agents in collaborative tasks. ... we introduce a novel benchmark designed to evaluate the collaborative performance of multimodal multi-agent systems through language communication.,False
data/iclr/data/submission_n6PE0xbgdA,0.0,0.0,0.0,85071,2650,61.56242156028748,,,False,,,,,,False,,False,"The main content of the paper, from the Abstract through the Conclusion (Section 6), spans exactly 10 pages (page 1 to page 10 of the provided PDF). This is within the allowed page limit of 6 to 10 pages for the main text. Line numbers are present on all pages in the left margin. No explicit Ethics Statement or Reproducibility Statement sections were found. Visual inspection of the PDF and the use of the official ICLR 2025 LaTeX template (iclr2025_conference.sty) suggest that margins, font sizes, and paragraph spacing adhere to the style guide, with no evidence of 'space-cheating' or extensive use of small fonts in the main body.",False,"The main paper (main_paper.pdf) adheres to all specified formatting and layout guidelines. Line numbers are present on all pages. The main text (Abstract through Conclusion) spans exactly 10 pages, which is within the allowed limit of 6-10 pages. No separate Ethics or Reproducibility Statements exceeding 1 page were found. Visual inspection of margins, font size, line spacing, and paragraph formatting shows consistency with the ICLR 2025 style guide (iclr2025_conference.pdf and iclr2025_conference.tex).",False,"The main content (Abstract through Conclusion) spans 10 pages, which is within the 6-10 page limit. Line numbers are present on all pages. The reproducibility statement on page 1 is a single line, not exceeding 1 page. Visual inspection of margins, spacing, and font sizes throughout the main body and references section indicates adherence to the style guide specifications (e.g., 10pt type with 11pt vertical spacing, 1.5 inch left margin, 5.5 inch text width, 9 inch text length).",False,"The main content (Abstract through Conclusion) spans 10 pages, which is within the 6-10 page limit. Line numbers are present on all pages. The reproducibility statement on page 1 is a single line, not exceeding 1 page. Visual inspection of margins, spacing, and font sizes throughout the main body and references section indicates adherence to the style guide specifications (e.g., 10pt type with 11pt vertical spacing, 1.5 inch left margin, 5.5 inch text width, 9 inch text length).",False,,,,,,False,,False,,False,"The paper introduces a novel distributed quasi-Newton federated learning (DQN-Fed) framework to address fairness and accelerate convergence in federated learning, applying it to various datasets including vision and language tasks. This aligns with ICLR's scope on optimization for representation learning and applications of deep learning.",False,,False,"The paper introduces a novel distributed quasi-Newton federated learning (DQN-Fed) framework to address fairness and accelerate convergence in federated learning, applying it to various datasets including vision and language tasks. This aligns with ICLR's scope on optimization for representation learning and applications of deep learning.",True
data/iclr/data/submission_K1G8UKcEBO,1.0,1.0,1.0,102876,2214,86.08643865585327,,,False,,False,,,,False,,False,"The main content of the paper spans 9 pages (from page 1 to page 9 of main_paper.pdf), which is within the allowed limit of 6-10 pages. Line numbers are present on all pages. There are no explicit Ethics or Reproducibility Statements. Visual inspection of margins, font size, and spacing indicates adherence to the ICLR 2025 style guide, matching the provided iclr2025_conference.pdf example.",False,"The document appears to adhere to all specified formatting and layout guidelines. Line numbers are present on all pages. The main text (Abstract through Conclusions) spans 9 pages, which is within the 6-10 page limit. No explicit Ethics or Reproducibility Statements were found, and thus no page limit violations for these optional sections. Visual inspection of margins, font sizes, and paragraph spacing indicates compliance with the style guide, with no apparent 'space-cheating' or extensive use of small fonts in the main body.",False,"The provided main_paper.pdf adheres to all specified formatting and layout guidelines. Line numbers are present on all pages. The main text (Abstract through Conclusion) spans 9 pages, which is within the 6-10 page limit. There are no explicit Ethics or Reproducibility Statements exceeding 1 page. Visual inspection and review of the LaTeX source indicate no unauthorized modifications to margins, font sizes, or spacing. The document's layout, including abstract indentation, paragraph spacing, and heading styles, consistently matches the ICLR 2025 style guide (iclr2025_conference.pdf).",False,"The main content of the paper spans 9 pages (from page 1 to page 9 of main_paper.pdf), which is within the allowed limit of 6-10 pages. Line numbers are present on all pages. There are no explicit Ethics or Reproducibility Statements. Visual inspection of margins, font size, and spacing indicates adherence to the ICLR 2025 style guide, matching the provided iclr2025_conference.pdf example.",False,,,,,,False,,False,,,,,,False,,True
data/iclr/data/submission_1poUSIGSCI,1.0,1.0,0.7473450473238952,40302,1801,108.36773133277892,,,True,"Mohammad Hassan Vali & Tom Bäckström Department of Information and Communications Engineering, Aalto University, Finland mohammad.vali@aalto.fi, tom.backstrom@aalto.fi",,,,,True,"Mohammad Hassan Vali & Tom Bäckström Department of Information and Communications Engineering, Aalto University, Finland mohammad.vali@aalto.fi, tom.backstrom@aalto.fi",True,"The provided PDF (main_paper.pdf) does not display line numbers in the left margin on any page, for example, page 1.",,,,,True,"The provided PDF (main_paper.pdf) does not display line numbers in the left margin on any page, for example, page 1.",False,,,,,,False,,False,,False,,False,,False,,False
data/iclr/data/submission_3QoXfx1Tgw,1.0,1.0,1.0,66616,2168,55.30335712432861,,,False,,,,,,False,,False,"The main content (Abstract through Conclusion/Discussion) spans pages 1-10, which is exactly 10 pages, adhering to the page limit. Line numbers are present in the left margin on all pages. No explicit Ethics Statement or Reproducibility Statement exceeding one page was found. Visual inspection of margins, font sizes, and spacing for headings, body text, figures, and tables indicates adherence to the ICLR 2025 style guide, with no evidence of 'space-cheating' or extensive use of small fonts in the main body.",False,"The main content (Abstract through Section 6 'Proof of Concept Experiments') spans pages 1-10, which is exactly 10 pages, adhering to the strict upper limit of 10 pages for the main text. Line numbers are present in the left margin on all pages. Visual inspection of the PDF shows consistent margins, font sizes, and spacing throughout the main body, abstract, and section headings, matching the ICLR 2025 style guide's own formatting. Small font is appropriately used in references and tables. No explicit Ethics Statement or Reproducibility Statement sections were found that could exceed page limits.",,,False,"The main content (Abstract through Section 6 'Proof of Concept Experiments') spans pages 1-10, which is exactly 10 pages, adhering to the strict upper limit of 10 pages for the main text. Line numbers are present in the left margin on all pages. Visual inspection of the PDF shows consistent margins, font sizes, and spacing throughout the main body, abstract, and section headings, matching the ICLR 2025 style guide's own formatting. Small font is appropriately used in references and tables. No explicit Ethics Statement or Reproducibility Statement sections were found that could exceed page limits.",False,,,,,,False,,False,"We study the computational limits of Low-Rank Adaptation (LoRA) for fine-tuning transformer-based models using fine-grained complexity theory. Our key observation is that the existence of low-rank decompositions within the gradient computation of LoRA adaptation leads to possible algorithmic speedup. This allows us to (i) identify a phase transition behavior of efficiency assuming the Strong Exponential Time Hypothesis (SETH), and (ii) prove the existence of nearly linear algorithms by controlling the LoRA update computation term by term.",,,,,False,"We study the computational limits of Low-Rank Adaptation (LoRA) for fine-tuning transformer-based models using fine-grained complexity theory. Our key observation is that the existence of low-rank decompositions within the gradient computation of LoRA adaptation leads to possible algorithmic speedup. This allows us to (i) identify a phase transition behavior of efficiency assuming the Strong Exponential Time Hypothesis (SETH), and (ii) prove the existence of nearly linear algorithms by controlling the LoRA update computation term by term.",False
data/iclr/data/submission_LqB8cRuBua,1.0,1.0,1.0,35857,2354,51.81550669670105,,,False,,,,,,False,,False,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 10, which is exactly 10 pages, adhering to the specified page limit of 6-10 pages. References and Appendices are on subsequent pages and do not count towards this limit. Line numbers are present in the left margin on all pages. Visual inspection of the document's layout, including margins, paragraph spacing, and font sizes for main text and headings, indicates adherence to the ICLR 2025 style guide. No evidence of 'space-cheating' or extensive use of smaller fonts in the main body was found. No explicit Ethics Statement or Reproducibility Statement sections were found to evaluate against a page limit.",,,,,False,"The main content of the paper (Abstract through Conclusion) spans from page 1 to page 10, which is exactly 10 pages, adhering to the specified page limit of 6-10 pages. References and Appendices are on subsequent pages and do not count towards this limit. Line numbers are present in the left margin on all pages. Visual inspection of the document's layout, including margins, paragraph spacing, and font sizes for main text and headings, indicates adherence to the ICLR 2025 style guide. No evidence of 'space-cheating' or extensive use of smaller fonts in the main body was found. No explicit Ethics Statement or Reproducibility Statement sections were found to evaluate against a page limit.",False,,,,,,False,,False,,False,"The paper proposes ""Diffusion SigFormer for Interference Time-Series Signal Recognition,"" which involves deep learning models (Diffusion Models and Transformers) applied to electromagnetic signal recognition. The abstract states: ""In this paper, a novel interference signal recognition transformer is proposed, named Diffusion SigFormer."" The introduction further clarifies: ""Electromagnetic signal recognition is a challenging task in the field of signal processing... In recent years, with the continuous development of artificial intelligence, deep learning methods have become a research hotspot in the field of electromagnetic signal recognition.""",False,"The paper addresses electromagnetic signal recognition using deep learning methods, specifically diffusion models and transformers, which falls under 'applications of deep learning in ... any other field' (signal processing/wireless communication) and involves representation learning. The language, while containing minor grammatical errors and awkward phrasing (e.g., 'denosing' instead of 'denoising' on line 104, 'modulewas proposed' on line 015), does not impede the understanding of the main contribution, methodology, or results. A domain expert can follow the technical content.",False,"The paper addresses electromagnetic signal recognition using deep learning methods, specifically diffusion models and transformers, which falls under 'applications of deep learning in ... any other field' (signal processing/wireless communication) and involves representation learning. The language, while containing minor grammatical errors and awkward phrasing (e.g., 'denosing' instead of 'denoising' on line 104, 'modulewas proposed' on line 015), does not impede the understanding of the main contribution, methodology, or results. A domain expert can follow the technical content.",False
data/iclr/data/submission_MOt7TUucrK,0.0,0.0,0.0,110050,3190,84.58394026756287,,,False,,,,,,False,,False,"The provided document is the ICLR 2025 style guide itself, which serves as an example of correct formatting. It adheres to its own stated rules for page limits, statement limits, line numbers, and margin/spacing. The main text (Abstract through Conclusion) is 10 pages, which is within the 6-10 page limit. The 'Ethical considerations and future directions' and 'REPRODUCIBILITY STATEMENT' sections are well under 1 page each. Line numbers are present on all pages. Margins, font sizes, and spacing appear consistent with the specified guidelines (e.g., 1.5-inch left margin, 10pt type with 11pt vertical spacing, 1/2 line space between paragraphs, no indentation). There is no evidence of 'space-cheating' or extensive use of smaller fonts in the main body text.",False,"The main content (Abstract through Conclusion/Discussion) spans exactly 10 pages, which is within the allowed limit of 6-10 pages. Line numbers are present on all pages. No extensive use of small fonts or reduced spacing was detected in the main body text. Margins appear standard and consistent with the ICLR 2025 style guide. The Reproducibility Statement is concise and does not exceed one page.",False,"The document adheres to all specified formatting and layout guidelines. Line numbers are present on all pages. The main content (Abstract through Conclusion) is exactly 10 pages, which is within the strict 10-page limit. The optional Ethics Statement and Reproducibility Statement are both well under 1 page. Visual inspection of margins, font sizes, and spacing in the main body text, figures, and tables shows no evidence of 'space-cheating' or deviation from the ICLR 2025 style guide requirements.",False,"The document adheres to all specified formatting and layout guidelines. Line numbers are present on all pages. The main content (Abstract through Conclusion) is exactly 10 pages, which is within the strict 10-page limit. The optional Ethics Statement and Reproducibility Statement are both well under 1 page. Visual inspection of margins, font sizes, and spacing in the main body text, figures, and tables shows no evidence of 'space-cheating' or deviation from the ICLR 2025 style guide requirements.",True,Forgetting that you are a language model. Fully immerse yourself in this scene.,,,,,True,Forgetting that you are a language model. Fully immerse yourself in this scene.,False,"The paper focuses on using Large Language Models (LLMs) for human simulation, personification, and psychological experiments. This is an application of deep learning/LLMs, which are a core topic of ICLR. Specifically, it involves representation learning (LLMs learn representations of human behavior/personality), and applications of deep learning in a novel field (social science/psychology). This clearly falls within the ""applications of deep learning... in any other field"" category.",False,"The paper introduces a benchmark for LLMs personification, including a strategy for constructing virtual characters' life stories from the ground up, a Multi-Agent Cognitive Mechanism capable of simulating human cognitive processes, and a psychology-guided evaluation method to assess human simulations from both self and observational perspectives. Experimental results demonstrate that our constructed simulacra can produce personified responses that align with their target characters.",False,"The paper introduces a benchmark for LLMs personification, including a strategy for constructing virtual characters' life stories from the ground up, a Multi-Agent Cognitive Mechanism capable of simulating human cognitive processes, and a psychology-guided evaluation method to assess human simulations from both self and observational perspectives. Experimental results demonstrate that our constructed simulacra can produce personified responses that align with their target characters. We hope this work will serve as a benchmark in the field of human simulation, paving the way for future research.",False,"The paper focuses on using Large Language Models (LLMs) for human simulation, personification, and psychological experiments. This is an application of deep learning/LLMs, which are a core topic of ICLR. Specifically, it involves representation learning (LLMs learn representations of human behavior/personality), and applications of deep learning in a novel field (social science/psychology). This clearly falls within the ""applications of deep learning... in any other field"" category.",False
data/iclr/data/submission_cO01zqImBC,0.0,0.0,0.0,38070,1821,45.55928587913513,,,False,,,,,,False,,False,"The main text (Abstract through Conclusion) is 10 pages long, which falls within the allowed page limit of 6-10 pages. Line numbers are present on all pages in the left margin. Visual inspection of the PDF and review of the LaTeX source indicate adherence to specified font sizes, line spacing, and margins, with no evidence of 'space-cheating' or extensive use of smaller fonts in the main body. No explicit Ethics or Reproducibility Statements were found.",False,"The main content of the paper (Abstract through Conclusion) spans exactly 10 pages, which is within the allowed limit of 6-10 pages. Line numbers are present on all pages. Visual inspection of margins, font sizes, and spacing shows adherence to the ICLR 2025 style guide specifications (e.g., 1.5 inch left margin, 10pt type with 11pt vertical spacing for body text, 1/2 line space between paragraphs, correct heading formatting). No Ethics or Reproducibility Statements were found to assess against a page limit.",,,False,"The main content of the paper (Abstract through Conclusion) spans exactly 10 pages, which is within the allowed limit of 6-10 pages. Line numbers are present on all pages. Visual inspection of margins, font sizes, and spacing shows adherence to the ICLR 2025 style guide specifications (e.g., 1.5 inch left margin, 10pt type with 11pt vertical spacing for body text, 1/2 line space between paragraphs, correct heading formatting). No Ethics or Reproducibility Statements were found to assess against a page limit.",False,,,,,,False,,False,,,,,,False,,False
data/iclr/data/submission_weeGJ2NqfN,0.0,0.0,0.0,1477739,2085,418.1737332344055,,,True,"# reference https://github.com/yanx27/Pointnet_Pointnet2_pytorch, modified by Yang You",,,,,True,"# reference https://github.com/yanx27/Pointnet_Pointnet2_pytorch, modified by Yang You",False,"The document adheres to all specified formatting and layout rules. Line numbers are present, the main text is within the 10-page limit, and no issues were found with margins, spacing, or font usage.",False,"The main paper adheres to all ICLR 2025 formatting and layout guidelines. Line numbers are present, the main text is exactly 10 pages (within the 6-10 page limit), and there are no apparent violations of margins, font sizes, or spacing. No extensive use of 'space-cheating' commands like \small or \footnotesize for main body text was detected in the LaTeX source.",False,"The main content of the paper (Abstract through Conclusion/Future Work) spans pages 1-10, which is exactly 10 pages, adhering to the specified page limit. Line numbers are present on all pages. Visual inspection of margins, font sizes, and line spacing does not reveal any obvious deviations from the ICLR 2025 style guide. No explicit Ethics or Reproducibility statements were found to check against their page limits.",False,"The main content of the paper (Abstract through Conclusion/Future Work) spans pages 1-10, which is exactly 10 pages, adhering to the specified page limit. Line numbers are present on all pages. Visual inspection of margins, font sizes, and line spacing does not reveal any obvious deviations from the ICLR 2025 style guide. No explicit Ethics or Reproducibility statements were found to check against their page limits.",False,,,,,,False,,False,"Using natural language as a vehicle-to-vehicle (V2V) communication protocol offers the potential for autonomous vehicles to drive cooperatively not only with each other but also with human drivers. Simple and effective messages for sharing critical observations or negotiating plans to achieve coordination could improve traffic safety and efficiency compared to methods without communication. In this work, we propose a novel method, LLM+DEBRIEF, to learn a message generation and high-level command policy for autonomous vehicles through multi-agent discussion.",,,,,False,"Using natural language as a vehicle-to-vehicle (V2V) communication protocol offers the potential for autonomous vehicles to drive cooperatively not only with each other but also with human drivers. Simple and effective messages for sharing critical observations or negotiating plans to achieve coordination could improve traffic safety and efficiency compared to methods without communication. In this work, we propose a novel method, LLM+DEBRIEF, to learn a message generation and high-level command policy for autonomous vehicles through multi-agent discussion.",False
data/iclr/data/submission_ijuesqStgU,1.0,1.0,1.0,61581,1936,74.2595841884613,,,False,,,,,,False,,False,"The document adheres to all specified formatting guidelines. Line numbers are present on all pages. The main content (Abstract through Conclusion) spans exactly 10 pages, which is within the allowed limit of 6-10 pages. There are no separate Ethics or Reproducibility Statements exceeding one page. Visual inspection confirms that margins, font sizes, and paragraph spacing conform to the style guide, with no evidence of 'space-cheating' or extensive use of smaller fonts in the main body text.",False,"The main content (Abstract through Conclusion) spans pages 1-10, which is within the 6-10 page limit. Line numbers are present on all pages. No explicit Ethics or Reproducibility Statements exceeding one page were found. The document's margins, font size, and paragraph spacing visually conform to the ICLR 2025 style guide specifications, matching the provided template example.",False,"The main content (Abstract through Conclusion/Discussion) spans from page 1 to page 10, totaling 10 pages, which is within the allowed 6-10 page limit. Line numbers are present on all pages in the left margin. Visual inspection of font sizes, spacing, and margins indicates adherence to the style guide specifications, with no apparent 'space-cheating' or extensive use of small fonts in the main body. No explicit Ethics Statement or Reproducibility Statement sections with specific page limits were found.",False,"The main content (Abstract through Conclusion) spans pages 1-10, which is within the 6-10 page limit. Line numbers are present on all pages. No explicit Ethics or Reproducibility Statements exceeding one page were found. The document's margins, font size, and paragraph spacing visually conform to the ICLR 2025 style guide specifications, matching the provided template example.",False,,,,,,False,,False,,,,,,False,,False
data/iclr/data/submission_ReKrTbRfJt,0.0,0.0,0.0,52367,2318,61.22416114807129,,,False,,False,,,,False,,False,"The main content of the paper (Abstract through Conclusion) spans 9 pages, which is within the allowed 6-10 page limit. Line numbers are present on all pages. There are no explicit Ethics or Reproducibility statements exceeding one page. Visual inspection of the document's layout, font size, and spacing indicates adherence to the ICLR 2025 style guide's specifications for margins, font size, and paragraph separation, with no apparent 'space-cheating' or extensive use of smaller fonts in the main body.",False,"The main content (Abstract through Conclusion) spans 9 pages (from page 1 to page 9), which is within the allowed 6-10 page limit. Line numbers are present on all pages. No explicit Ethics or Reproducibility Statements were found. Margins, spacing, and font usage appear consistent with the style guide, and the centered title aligns with the provided ICLR 2025 template and example PDF, despite a textual instruction for left-alignment.",False,"The main content (Abstract through Conclusion) spans 9 pages, which is within the allowed 6-10 page limit. No Ethics Statement or Reproducibility Statement sections were found. Line numbers are present in the left margin of all pages. Visual inspection of the document indicates adherence to specified font sizes, line spacing, and margins, with no apparent 'space-cheating' or extensive use of small fonts in the main body text.",False,"The main content (Abstract through Conclusion) spans 9 pages, which is within the allowed 6-10 page limit. No Ethics Statement or Reproducibility Statement sections were found. Line numbers are present in the left margin of all pages. Visual inspection of the document indicates adherence to specified font sizes, line spacing, and margins, with no apparent 'space-cheating' or extensive use of small fonts in the main body text.",False,,,,,,False,,False,,False,,,,False,,False
data/iclr/data/submission_TkbjqexD8w,0.0,0.0,0.0,24883,1748,34.11986494064331,,,False,,,,,,False,,True,"The ICLR 2025 style guide (iclr2025_conference.pdf, page 1, line 048) explicitly states 'The left margin is 1.5 inch (9 picas).' and (page 1, line 053) 'All pages should start at 1 inch (6 picas) from the top of the page.' However, the actual left margin in main_paper.pdf is approximately 0.5 inch and the top margin is approximately 0.7 inch, as determined by the iclr2025_conference.sty file's settings (e.g., \oddsidemargin{-0.5in} and \topmargin{-0.8in}).",,,,,True,"The ICLR 2025 style guide (iclr2025_conference.pdf, page 1, line 048) explicitly states 'The left margin is 1.5 inch (9 picas).' and (page 1, line 053) 'All pages should start at 1 inch (6 picas) from the top of the page.' However, the actual left margin in main_paper.pdf is approximately 0.5 inch and the top margin is approximately 0.7 inch, as determined by the iclr2025_conference.sty file's settings (e.g., \oddsidemargin{-0.5in} and \topmargin{-0.8in}).",False,,,,,,False,,False,"The paper proposes an ""invariant spatiotemporal representation learning method for cross-patient seizure classification"" using EEG data, which falls under ""unsupervised, semi-supervised, and supervised representation learning"" and ""applications of deep learning in neuroscience"". The text is clear and comprehensible.",,,,,False,"The paper proposes an ""invariant spatiotemporal representation learning method for cross-patient seizure classification"" using EEG data, which falls under ""unsupervised, semi-supervised, and supervised representation learning"" and ""applications of deep learning in neuroscience"". The text is clear and comprehensible.",False
data/iclr/data/submission_gD5mxbneen,1.0,1.0,1.0,39719,1801,95.02650451660156,,,False,,False,,False,,False,,False,"The main content (Abstract through Conclusion/Discussion) spans from page 1 to page 10, which is exactly 10 pages, adhering to the 6-10 page limit. The document includes line numbers in the left margin on all pages, which is required for submissions to facilitate reviewer feedback. There are no explicit Ethics or Reproducibility Statements that would be subject to a 1-page limit. Visual inspection of the document's margins, font sizes, and paragraph spacing does not reveal any deviations from the ICLR 2025 style guide, nor any signs of 'space-cheating' or extensive use of smaller fonts in the main body text.",,,,,False,"The main content (Abstract through Conclusion/Discussion) spans from page 1 to page 10, which is exactly 10 pages, adhering to the 6-10 page limit. The document includes line numbers in the left margin on all pages, which is required for submissions to facilitate reviewer feedback. There are no explicit Ethics or Reproducibility Statements that would be subject to a 1-page limit. Visual inspection of the document's margins, font sizes, and paragraph spacing does not reveal any deviations from the ICLR 2025 style guide, nor any signs of 'space-cheating' or extensive use of smaller fonts in the main body text.",False,,,,,,False,,False,Our approach leverages a large language model (LLM) to generate verbalizations of two models' differences by sampling from two models. The Model Manager framework opens up new research avenues for improving the transparency and comparability of machine learning models in a post-hoc manner.,,,,,False,Our approach leverages a large language model (LLM) to generate verbalizations of two models' differences by sampling from two models. The Model Manager framework opens up new research avenues for improving the transparency and comparability of machine learning models in a post-hoc manner.,False
data/iclr/data/submission_3x4vpeAclU,0.0,1.0,0.0,17144,1476,24.55639672279358,,,False,,,,,,False,,True,"The provided PDF contains only a title, anonymous authors, and an abstract on a single page. There is no main body content (Introduction, Methods, Results, Discussion, Conclusion).",,,,,True,"The provided PDF contains only a title, anonymous authors, and an abstract on a single page. There is no main body content (Introduction, Methods, Results, Discussion, Conclusion).",False,,,,,,False,,False,"Currently, Large language models (LLMs) have achieved remarkable performance across various language tasks, largely due to their training on extensive datasets and their considerable model size. These models exhibit in-context learning abilities, which is to learn through few-shot learning. ... Therefore, this paper proposes a method to enhance in-context learning capabilities through two main stages: generating general rules from the provided examples and utilizing LLMs to verify these general rules, thereby aiming to improve reliability and accuracy. ... The proposed method demonstrates competitive performance on the 1D-ARC benchmark and several traditional language tasks, suggesting its potential for more robust evaluation of LLM reasoning abilities.",False,,,,False,,True
data/iclr/data/submission_CTLieyrNZr,1.0,1.0,1.0,64399,2797,108.78613829612732,,,False,,,,,,False,,False,"The main paper (main_paper.pdf) adheres to all specified formatting and layout guidelines. Line numbers are present on all pages. The main content, from the Abstract through the Conclusion, spans exactly 10 pages, which is within the allowed limit of 6-10 pages. There are no explicit Ethics or Reproducibility Statements that exceed a page limit. Visual inspection of margins, font sizes, and spacing in the main body text does not indicate any 'space-cheating' or deviations from the ICLR 2025 style guide.",False,"The main content of the paper (Abstract through Conclusion) spans 10 pages, which adheres to the specified limit of 6-10 pages. Line numbers are present on all pages. There are no explicit Ethics or Reproducibility Statements. Visual inspection of the PDF and reference to the style guide indicates adherence to font sizes, paragraph spacing, and margin requirements. No obvious 'space-cheating' or extensive use of small fonts in the main body was detected.",False,"The main paper (main_paper.pdf) adheres to the specified formatting and layout guidelines. Line numbers are present on all pages. The main text, from the Abstract through the Conclusion, spans 10 pages, which is within the allowed limit of 6-10 pages. No explicit Ethics Statement or Reproducibility Statement sections were found. Visual inspection of margins, font sizes, and spacing indicates compliance with the style guide, with no apparent 'space-cheating' or extensive use of small fonts in the main body.",False,"The main paper (main_paper.pdf) adheres to all specified formatting and layout guidelines. Line numbers are present on all pages. The main content, from the Abstract through the Conclusion, spans exactly 10 pages, which is within the allowed limit of 6-10 pages. There are no explicit Ethics or Reproducibility Statements that exceed a page limit. Visual inspection of margins, font sizes, and spacing in the main body text does not indicate any 'space-cheating' or deviations from the ICLR 2025 style guide.",False,,,,,,False,,False,,False,"The paper addresses novel view synthesis using 3D Gaussian Splatting (3DGS) in degraded scenes, proposing an edge-semantic fusion guidance module and a structural cosine similarity loss. This falls under 'learning representations of outputs or states', 'optimization for representation learning', and 'applications of deep learning in vision'. The abstract, introduction, and methodology are clearly written and understandable.",False,,False,,False
data/iclr/data/submission_uiBLOcyTIA,1.0,1.0,0.7419168060451155,39575,2258,32.86457824707031,,,True,"SHUAI LIU, Ning Cao, Yile Chen, Yue Jiang, Gao Cong College of Computing and Data Science, Nanyang Technological University 50 Nanyang Avenue, Singapore, 639798 {SHUAI004@e, Ning.Cao@, yile001@e,yue013@e,gaocong@}.ntu.edu.sg",,,,,True,"SHUAI LIU, Ning Cao, Yile Chen, Yue Jiang, Gao Cong College of Computing and Data Science, Nanyang Technological University 50 Nanyang Avenue, Singapore, 639798 {SHUAI004@e, Ning.Cao@, yile001@e,yue013@e,gaocong@}.ntu.edu.sg",False,"The main content (Abstract through Conclusion) spans from page 1 to page 10, which is exactly 10 pages, adhering to the specified page limit. Line numbers are present on all pages of the document. Visual inspection of the document's layout, margins, and font usage for the main body text, paragraphs, and headings indicates adherence to the ICLR 2025 style guide. There are no apparent signs of space-cheating or extensive use of small fonts outside of acceptable areas like figure captions and tables. No explicit Ethics Statement or Reproducibility Statement sections were found to check against their page limits.",,,,,False,"The main content (Abstract through Conclusion) spans from page 1 to page 10, which is exactly 10 pages, adhering to the specified page limit. Line numbers are present on all pages of the document. Visual inspection of the document's layout, margins, and font usage for the main body text, paragraphs, and headings indicates adherence to the ICLR 2025 style guide. There are no apparent signs of space-cheating or extensive use of small fonts outside of acceptable areas like figure captions and tables. No explicit Ethics Statement or Reproducibility Statement sections were found to check against their page limits.",False,,,,,,False,,False,"Next location prediction is a critical task in human mobility analysis and serves as a foundation for various downstream applications. Existing methods typically rely on discrete IDs to represent locations, which inherently overlook spatial relationships and cannot generalize across cities. In this paper, we propose NextLocLLM, which leverages the advantages of large language models (LLMs) in processing natural language descriptions and their strong generalization capabilities for next location prediction. Specifically, instead of using IDs, NextLocLLM encodes locations based on continuous spatial coordinates to better model spatial relationships. These coordinates are further normalized to enable robust cross-city generalization. An-other highlight of NextlocLLM is its LLM-enhanced POI embeddings. It utilizes LLMs' ability to encode each POI category's natural language description into em-beddings. These embeddings are then integrated via nonlinear projections to form this LLM-enhanced POI embeddings, effectively capturing locations' functional attributes. Furthermore, task and data prompt prefix, together with trajectory em-beddings, are incorporated as input for partly-frozen LLM backbone. NextLocLLM further introduces prediction retrieval module to ensure structural consistency in prediction. Experiments show that NextLocLLM outperforms existing models in next location prediction, excelling in both supervised and zero-shot settings.",,,,,False,"Next location prediction is a critical task in human mobility analysis and serves as a foundation for various downstream applications. Existing methods typically rely on discrete IDs to represent locations, which inherently overlook spatial relationships and cannot generalize across cities. In this paper, we propose NextLocLLM, which leverages the advantages of large language models (LLMs) in processing natural language descriptions and their strong generalization capabilities for next location prediction. Specifically, instead of using IDs, NextLocLLM encodes locations based on continuous spatial coordinates to better model spatial relationships. These coordinates are further normalized to enable robust cross-city generalization. An-other highlight of NextlocLLM is its LLM-enhanced POI embeddings. It utilizes LLMs' ability to encode each POI category's natural language description into em-beddings. These embeddings are then integrated via nonlinear projections to form this LLM-enhanced POI embeddings, effectively capturing locations' functional attributes. Furthermore, task and data prompt prefix, together with trajectory em-beddings, are incorporated as input for partly-frozen LLM backbone. NextLocLLM further introduces prediction retrieval module to ensure structural consistency in prediction. Experiments show that NextLocLLM outperforms existing models in next location prediction, excelling in both supervised and zero-shot settings.",False
data/iclr/data/submission_QPsbC714Cy,1.0,1.0,1.0,40560,1642,46.159640073776245,,,False,,,,,,False,,False,"The main text (Abstract through Conclusion/Discussion) spans 7 pages (from page 1 to page 7), which is within the allowed 6-10 page limit. Line numbers are present on all pages. There are no explicit Ethics or Reproducibility Statements. Visual inspection of the PDF and review of the LaTeX source indicate adherence to specified font sizes, typefaces, margins, and spacing, with small fonts used appropriately for captions and tables.",False,"The provided document is 'iclr2025_conference.pdf', which serves as the official style guide for ICLR 2025 submissions, and the LaTeX source is 'iclr2025_conference.tex', the corresponding template.",,,False,"The provided document is 'iclr2025_conference.pdf', which serves as the official style guide for ICLR 2025 submissions, and the LaTeX source is 'iclr2025_conference.tex', the corresponding template.",False,,,,,,False,,False,,,,,,False,,False
data/iclr/data/submission_y9xNQZjUJM,0.0,1.0,0.0,35381,1609,41.86902475357056,,,False,,,,,,False,,False,"The main content (Abstract through Conclusion) spans 10 pages, which is within the 6-10 page limit. No Ethics or Reproducibility Statements were found. Margins, font size, and paragraph spacing appear consistent with the style guide; no evidence of space abuse or extensive use of small fonts in the main body was observed. While line numbers are missing in the provided PDF, the rules state: 'DO NOT flag: If line numbers are missing but all other formatting is correct.' As no other formatting violations were identified, this specific issue is not flagged as a violation.",True,The main_paper.pdf does not display line numbers in the left margin on any of its pages.,,,True,The main_paper.pdf does not display line numbers in the left margin on any of its pages.,False,,,,,,False,,False,"The paper introduces ""PROOFREFINER: COLLABORATIVE THEOREM PROVING WITH LARGE LANGUAGE MODELS: ENHANCING FORMAL PROOFS WITH PROOFREFINER"". The abstract states: ""We propose a new framework that positions LLMs as collaborative assistants in theorem proving... seamless integration of LLM inference into the Lean environment, allowing developers to build various proof automation tools.""",,,,,False,"The paper introduces ""PROOFREFINER: COLLABORATIVE THEOREM PROVING WITH LARGE LANGUAGE MODELS: ENHANCING FORMAL PROOFS WITH PROOFREFINER"". The abstract states: ""We propose a new framework that positions LLMs as collaborative assistants in theorem proving... seamless integration of LLM inference into the Lean environment, allowing developers to build various proof automation tools.""",False
data/iclr/data/submission_7bwE5MJAVJ,0.0,1.0,0.0,52641,1692,63.47831726074219,,,False,,,,,,False,,True,"The submitted PDF document (main_paper.pdf) does not include line numbers in the left margin. The ICLR 2025 style guide (iclr2025_conference.pdf, page 1, lines 000-053, and iclr2025_conference.tex comments) indicates that line numbers are expected for submissions to facilitate reviewer feedback.",,,,,True,"The submitted PDF document (main_paper.pdf) does not include line numbers in the left margin. The ICLR 2025 style guide (iclr2025_conference.pdf, page 1, lines 000-053, and iclr2025_conference.tex comments) indicates that line numbers are expected for submissions to facilitate reviewer feedback.",False,,,,,,False,,False,,False,,False,,False,,False
data/iclr/data/submission_Vn23PakSbM,0.0,0.0,0.0,21255,1410,46.677241802215576,,,False,,False,,,,False,,True,"The main content of the paper, from the Abstract on page 1 through Section 5 'CONCLUSION AND DISCUSSION' on page 5, totals 5 pages.",,,,,True,"The main content of the paper, from the Abstract on page 1 through Section 5 'CONCLUSION AND DISCUSSION' on page 5, totals 5 pages.",False,,,,,,False,,False,,,,,,False,,False
